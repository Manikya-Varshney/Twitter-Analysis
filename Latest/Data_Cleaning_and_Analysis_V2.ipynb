{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JHBSrnVsmQT1"
   },
   "source": [
    "\n",
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "iN58fBbFo48-"
   },
   "source": [
    "# Imports for GDrive\n",
    "!pip install -U -q PyDrive\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from google.colab import auth\n",
    "from oauth2client.client import GoogleCredentials"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "XssQCdN5pE03"
   },
   "source": [
    "# Mounting the drive\n",
    "auth.authenticate_user()\n",
    "gauth = GoogleAuth()\n",
    "gauth.credentials = GoogleCredentials.get_application_default()\n",
    "drive = GoogleDrive(gauth)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "3zJK_ICLpTmX"
   },
   "source": [
    "#downloaded = drive.CreateFile({'id':'https://drive.google.com/file/d/17dnH3TDdLmxos83OTHEtXBCD5Tmu-L_k/view?usp=sharing'}) # replace the id with id of file you want to access\n",
    "#downloaded.GetContentFile('h01-20201001-20201008.zip') "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QOdpxl2-w7sR",
    "outputId": "94499167-2ecf-416c-c6ce-b153f5acbd91"
   },
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1xtF___3xLV3",
    "outputId": "152e3328-6a9b-4d40-b40b-959df2d60dfc"
   },
   "source": [
    "import os\n",
    "os.listdir('/content/gdrive/Shared drives/') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XiUQenL10eNC",
    "outputId": "56b52020-09be-4791-e2ca-7fcc010aae99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: emoji in /opt/anaconda3/envs/twitter/lib/python3.8/site-packages (1.4.2)\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/priyanshkedia/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/priyanshkedia/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/priyanshkedia/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/priyanshkedia/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "#!pip install pandas\n",
    "#!pip install numpy\n",
    "#!pip install nltk\n",
    "#!pip install multiprocess\n",
    "!pip install emoji\n",
    "\n",
    "# Import statements\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import csv\n",
    "import re\n",
    "import string\n",
    "import time\n",
    "from datetime import datetime, timedelta, date\n",
    "\n",
    "\n",
    "import multiprocess\n",
    "from multiprocess import Pool, Process\n",
    "import multiprocessing as mp\n",
    "\n",
    "\n",
    "\n",
    "import emoji\n",
    "import nltk\n",
    "from nltk import pos_tag, pos_tag_sents\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "stop_words = set(stopwords.words('english')) - set(['at', 'do', 'your', 'from', 'to', 'out', 'no', 'the'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "G7-qNi6n_pMM"
   },
   "outputs": [],
   "source": [
    "# Extract zip file\n",
    "\n",
    "def extract_zips(filename, temp_folder):\n",
    "  zip_ref = zipfile.ZipFile(filename, 'r')\n",
    "  zip_ref.extractall(temp_folder)\n",
    "  zip_ref.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "OHS5daIFGbLI"
   },
   "outputs": [],
   "source": [
    "def create_filename(FILENAME, TEMP_FOLDER):\n",
    "  FILES_NAMES = FILENAME.split(\".\")[0].split(\"/\")[-1]\n",
    "  FILES_NAMES = FILES_NAMES.rsplit(\"-\", 1)[0]\n",
    "  # FILES_NAMES = os.path.join(TEMP_FOLDER, FILES_NAMES)\n",
    "  return FILES_NAMES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "xvgg28WxGj1R"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-10-01\n",
      "2020-10-02\n",
      "2020-10-03\n",
      "2020-10-04\n",
      "2020-10-05\n",
      "2020-10-06\n",
      "2020-10-07\n"
     ]
    }
   ],
   "source": [
    "def get_date_string(FILESNAMES):\n",
    "  date_string = FILESNAMES.split(\"-\")[-1]\n",
    "  return date_string\n",
    "\n",
    "def strip_date(FILESNAMES):\n",
    "  date_string = get_date_string(FILESNAMES)\n",
    "  ob = datetime.strptime(date_string, \"%Y%m%d\")\n",
    "  date_to_analyse = ob.strftime(\"%a %b %d\")\n",
    "  year = ob.strftime(\"%Y\")\n",
    "  return date_to_analyse, year\n",
    "\n",
    "def get_date_numbers(date_string):\n",
    "    ob = datetime.strptime(date_string, \"%a %b %d\")\n",
    "    return ob.strftime(\"%d\"), ob.strftime(\"%m\")\n",
    "    \n",
    "def get_next_date(current_date,year):\n",
    "    day, month = get_date_numbers(current_date)\n",
    "    date = datetime(int(year), int(month), int(day))\n",
    "    date += timedelta(days = 1)\n",
    "    return date.strftime(\"%Y%m%d\")\n",
    "    \n",
    "def daterange(start_date, end_date):\n",
    "    for n in range(int((end_date - start_date).days)):\n",
    "        yield start_date + timedelta(n)\n",
    "        \n",
    "# h01-20201001-20201008\n",
    "def get_date_range(FILESNAMES):\n",
    "    filesnames = FILESNAMES.split('.')[0]\n",
    "    date_string = filesnames.split(\"-\")[-2:]\n",
    "    start_date = strip_date(date_string[0])\n",
    "    end_date = strip_date(date_string[1])\n",
    "    start_date, start_year, end_date, end_year = start_date[0], start_date[1], end_date[0], end_date[1]\n",
    "    start_day, start_month = get_date_numbers(start_date)\n",
    "    end_day, end_month = get_date_numbers(end_date)\n",
    "    \n",
    "    start_date = date(int(start_year), int(start_month), int(start_day))\n",
    "    end_date = date(int(end_year), int(end_month), int(end_day))\n",
    "    return start_date, end_date \n",
    "\n",
    "\n",
    "\n",
    "start_date, end_date = get_date_range('h01-20201001-20201008.zip')\n",
    "\n",
    "for single_date in daterange(start_date, end_date):\n",
    "    print(single_date.strftime(\"%Y-%m-%d\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_directory(directory):\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Axem73RZAAkA"
   },
   "outputs": [],
   "source": [
    "# Constants\n",
    "FILENAME = \"h01-20200818-10files.zip\"\n",
    "# FILENAME = \"/home/manikya_varshney/Documents/Python/Yale/h01-20200818-10files.zip\"\n",
    "TEMP_FOLDER = \"/tmp\"\n",
    "FILES_NAMES = create_filename(FILENAME, TEMP_FOLDER)\n",
    "date_to_analyse = strip_date(FILES_NAMES)\n",
    "num_cores = mp.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "added_ids = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "ZuK9Y_DSL0cL"
   },
   "outputs": [],
   "source": [
    "# Keep data in english only\n",
    "def remove_other_langs(data):\n",
    "  data = data[data['lang'] == 'en'].reset_index(drop=True)\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_other_date(data, old_data, current_date):\n",
    "    if data.equals(old_data):\n",
    "        return\n",
    "    date_to_save = get_next_date(current_date, year)\n",
    "    print(date_to_save)\n",
    "    directory = \"data\"\n",
    "    create_directory(directory)\n",
    "    combined = pd.concat([data, old_data]).drop_duplicates(keep=False)\n",
    "    \n",
    "    combined.to_csv(\"{}/{}.csv\".format(directory, date_to_save))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Xwsew-o1MzvI"
   },
   "outputs": [],
   "source": [
    "# Keep specific date\n",
    "def remove_other_dates(data, date_to_analyse):\n",
    "  new_data = data[data['created_at'].str[:10] == date_to_analyse].reset_index(drop=True)\n",
    "  save_other_date(new_data, data, date_to_analyse)\n",
    "  return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "fnpiNeSksPHD"
   },
   "outputs": [],
   "source": [
    "# Creating the RT Column\n",
    "def create_rt_column(data):\n",
    "  data['RT'] = data['text'].str[:2]=='RT'\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "jDie85V6J4IJ"
   },
   "outputs": [],
   "source": [
    "# Parse CSV data\n",
    "def parse_data_from_file(filename, date_to_analyse):\n",
    "  data = pd.read_csv(filename, index_col = None, header=0, engine = 'python')\n",
    "  data = remove_other_langs(data)\n",
    "  data = remove_other_dates(data, date_to_analyse)\n",
    "  data = create_rt_column(data)\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "oksZ2gyHLTYM"
   },
   "outputs": [],
   "source": [
    "# Parse all files of the same date\n",
    "def parse_all_files(TEMP_FOLDER, FILES_NAMES, date_to_analyse):\n",
    "  files = os.listdir(TEMP_FOLDER)\n",
    "  new_data = []\n",
    "  for file in files:\n",
    "    if file.startswith(FILES_NAMES):\n",
    "      parsed_data = parse_data_from_file(os.path.join(TEMP_FOLDER, file), date_to_analyse)\n",
    "      new_data.append(parsed_data)\n",
    "  return pd.concat(new_data, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "d6dvi4iZuO7z"
   },
   "outputs": [],
   "source": [
    "def get_text_or_extended_text(data, added_ids):\n",
    "  data[\"FINAL_TEXT\"] = np.where(data['extended_tweet_full_text'].notnull(), data[\"extended_tweet_full_text\"], data[\"text\"])\n",
    "  id = data[\"id\"].to_dict()\n",
    "  added_ids.update(id)\n",
    "  return data[\"FINAL_TEXT\"]\n",
    "\n",
    "def get_quoted_text(data, added_ids):\n",
    "  data[\"FINAL_TEXT\"] = np.where(data['QT_full_text'].notnull(), data[\"QT_full_text\"], data[\"QT_text\"])\n",
    "  added_ids.update(data[\"id\"].to_dict())\n",
    "  added_ids.update(data[\"QT_id\"].to_dict())\n",
    "  return data[\"FINAL_TEXT\"]\n",
    "\n",
    "def get_text_or_full_text_rt(data, added_ids):\n",
    "  data[\"FINAL_TEXT\"] = np.where(data['RT_full_text'].notnull(), data[\"RT_full_text\"], data[\"RT_text\"])\n",
    "  added_ids.update(data[\"id\"].to_dict())\n",
    "  added_ids.update(data[\"RT_id\"].to_dict())\n",
    "  return data[\"FINAL_TEXT\"]\n",
    "\n",
    "def get_quote_rt_full(data, added_ids):\n",
    "  data[\"FINAL_TEXT\"] = get_text_or_full_text_rt(data, added_ids) + get_quoted_text(data, added_ids)\n",
    "  return data[\"FINAL_TEXT\"]\n",
    "\n",
    "\n",
    "\n",
    "def get_quote_rt(data, is_quote, is_rt):\n",
    "  data = data.loc[(data['is_quote_tweet'] == is_quote) & (data['RT'] == is_rt)]\n",
    "\n",
    "  if not is_quote and not is_rt:\n",
    "    added_ids.update(dict(zip(data[\"id\"], data[\"is_quote_tweet\"])))\n",
    "    data[\"FINAL_TEXT\"] = np.where(data['extended_tweet_full_text'].notnull(), data[\"extended_tweet_full_text\"], data[\"text\"])\n",
    "\n",
    "  if is_quote and not is_rt:\n",
    "    data[\"FINAL_TEXT\"] = np.where(data[\"QT_id\"].isin(added_ids.keys()), get_text_or_extended_text(data, added_ids), get_quoted_text(data, added_ids))\n",
    "\n",
    "  if not is_quote and is_rt:\n",
    "    data[\"FINAL_TEXT\"] = np.where(data[\"RT_id\"].isin(added_ids.keys()), added_ids.update(data['id'].to_dict()), get_text_or_full_text_rt(data, added_ids))\n",
    "    data = data[data[\"FINAL_TEXT\"].notna()]\n",
    "\n",
    "  if is_quote and is_rt:\n",
    "    data[\"FINAL_TEXT\"] = np.where((data[\"RT_id\"].isin(added_ids.keys())) & (data[\"QT_id\"].isin(added_ids.keys())), added_ids.update(data['id'].to_dict()), None)\n",
    "    data[\"FINAL_TEXT\"] = np.where((data[\"RT_id\"].isin(added_ids.keys())) & (~data[\"QT_id\"].isin(added_ids.keys())), get_quoted_text(data, added_ids), None)\n",
    "    data[\"FINAL_TEXT\"] = np.where((~data[\"RT_id\"].isin(added_ids.keys())) & (data[\"QT_id\"].isin(added_ids.keys())), get_text_or_full_text_rt(data, added_ids), None)\n",
    "    data[\"FINAL_TEXT\"] = np.where((~data[\"RT_id\"].isin(added_ids.keys())) & (~data[\"QT_id\"].isin(added_ids.keys())), get_quote_rt_full(data, added_ids), None)\n",
    "    data = data[data[\"FINAL_TEXT\"].notna()]\n",
    "\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_df(filename, TEMP_FOLDER, FILES_NAMES, date_to_analyse):\n",
    "    extract_zips(filename, TEMP_FOLDER)\n",
    "    data = parse_all_files(TEMP_FOLDER, FILES_NAMES, date_to_analyse)\n",
    "\n",
    "    non_quote_non_rt = get_quote_rt(data, False, False)\n",
    "    quote_non_rt = get_quote_rt(data, True, False)\n",
    "    quote_rt = get_quote_rt(data, True, True)\n",
    "    non_quote_rt = get_quote_rt(data, False, True)\n",
    "    \n",
    "    return non_quote_non_rt, quote_non_rt, quote_rt, non_quote_rt\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HCpVf_aD1lV-"
   },
   "source": [
    "1. Casing (Upper or lower case)\n",
    "2. Noise Removal (Removal of punctuation, white spaces, special characters, HTML tags)\n",
    "3. Tokenization (Tweets to tokens i.e. words seprated by spaces)\n",
    "4. Stopword Removal\n",
    "5. Text Normalization (Stemming and Lemmatization) bold text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "e212hr-q2LKf"
   },
   "outputs": [],
   "source": [
    "#Removing emojis\n",
    "def demoji(text):\n",
    "  emoji_pattern = re.compile(\"[\"\n",
    "  u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                u\"\\U00002702-\\U000027B0\"\n",
    "                u\"\\U000024C2-\\U0001F251\"\n",
    "                u\"\\U0001f926-\\U0001f937\"\n",
    "                u'\\U00010000-\\U0010ffff'\n",
    "                u\"\\u200d\"\n",
    "                u\"\\u2640-\\u2642\"\n",
    "                u\"\\u2600-\\u2B55\"\n",
    "                u\"\\u23cf\"\n",
    "                u\"\\u23e9\"\n",
    "                u\"\\u231a\"\n",
    "                u\"\\u3030\"\n",
    "                u\"\\ufe0f\"\n",
    "                        \"]+\", flags=re.UNICODE)\n",
    "  return(emoji_pattern.sub(r'', text.decode('utf-8')))\n",
    "\n",
    "#POSTags\n",
    "def get_wordnet_pos(word):\n",
    "  \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "  tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "  tag_dict = {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV}\n",
    "  return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "def apply_stemming(row):\n",
    "  row_list = row[\"FINAL_TEXT\"]\n",
    "  stemmed_list = [ps.stem(word) for word in row_list]\n",
    "  return (stemmed_list)\n",
    "\n",
    "#Remove URLs, user@, punctutions\n",
    "def df_cleaning(data_):\n",
    "  punc = string.punctuation\n",
    "  data_[\"FINAL_TEXT\"] = data_[\"FINAL_TEXT\"].astype(str).str.replace(r\"http\\S+| www\\S+| https\\S+| \\S+\\.com\\S+| \\S+\\.com| \\@[\\w]+\", \"\", regex=True)\n",
    "\n",
    "  # ##################-------Punctutions-------##################\n",
    "  data_[\"FINAL_TEXT\"] = data_[\"FINAL_TEXT\"].astype(str).str.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    " \n",
    "  # ##################-------More Cleaning-------##################\n",
    "  data_[\"FINAL_TEXT\"] = data_[\"FINAL_TEXT\"].astype(str).str.replace(\"/[^a-zA-Z0-9 ]/g\", \"\", regex=True).str.replace(\"\\n\",\" \", regex=True).str.replace(\"—\",\" \", regex=True).str.strip(\"“\").str.strip(\"”\").str.strip(\"’\").str.lstrip(\" \").str.rstrip(\" \")\n",
    "  \n",
    "  # ##################-------Emojis-------##################\n",
    "#   data_[\"FINAL_TEXT\"] = data_[\"FINAL_TEXT\"].astype(str).apply(lambda x:demoji(x.encode('utf8')))\n",
    "  data_[\"FINAL_TEXT\"] = data_[\"FINAL_TEXT\"].astype(str).apply(lambda x: x.encode('ascii', 'ignore').decode('ascii'))\n",
    "  # ##################-------Tokenizing-------##################\n",
    "  data_[\"FINAL_TEXT\"] = data_[\"FINAL_TEXT\"].astype(str)\n",
    "  data_[\"FINAL_TEXT\"] = data_[\"FINAL_TEXT\"].apply(nltk.word_tokenize)\n",
    "#   data_[\"FINAL_TEXT\"] = data_.apply(lambda row: nltk.word_tokenize(row[\"FINAL_TEXT\"]), axis=1)\n",
    "\n",
    "  # ##################-------Lower characters---------##################\n",
    "  data_[\"FINAL_TEXT\"] = data_[\"FINAL_TEXT\"].apply(lambda x: [word.lower() for word in x])\n",
    "\n",
    "  # ##################-------Remove punctuations-------##################\n",
    "  data_[\"FINAL_TEXT\"] = data_[\"FINAL_TEXT\"].apply(lambda x: [word for word in x if word not in punc])\n",
    "\n",
    "  # ##################-------Removing stopwords-------##################\n",
    "#   data_[\"FINAL_TEXT\"] = data_[\"FINAL_TEXT\"].astype(str).apply(lambda x: [word for word in x if word not in stop_words])\n",
    "  \n",
    "  # data_[\"FINAL_TEXT\"] = data_[\"FINAL_TEXT\"].astype(str).apply(lambda x: [word for word in x if word not in stop_words])\n",
    "  data_[\"FINAL_TEXT\"] = data_[\"FINAL_TEXT\"].apply(lambda words: ' '.join(word for word in words if word not in stop_words))\n",
    " \n",
    "  \n",
    "  # ##################-------Stemming-------##################\n",
    "  ps = PorterStemmer()\n",
    "#   data_[\"FINAL_TEXT\"] = data_[\"FINAL_TEXT\"].apply(lambda x: [ps.stem(y) for y in x])\n",
    "  data_[\"FINAL_TEXT\"] = data_[\"FINAL_TEXT\"].apply(lambda x: [ps.stem(y) for y in x.split(' ')])\n",
    "#   data_[\"FINAL_TEXT\"] = data_.apply(apply_stemming, axis = 1)\n",
    "#   data_[\"FINAL_TEXT\"] = data_.apply(ps.stem)\n",
    "\n",
    "#   print(data_[\"FINAL_TEXT\"], \"from here\")\n",
    "\n",
    "\n",
    "  # ##################-------Lemmatizing-------##################\n",
    "#   lemmatizer = WordNetLemmatizer()\n",
    "#   data[\"FINAL_TEXT\"] = data[\"FINAL_TEXT\"].astype(str).apply(lambda x: [lemmatizer.lemmatize(y, get_wordnet_pos(y)) for y in x])\n",
    "  \n",
    "  # ##################-------Joining the lemmetized tokens to form string-------##################\n",
    "  data_[\"FINAL_TEXT\"] = data_[\"FINAL_TEXT\"].apply(lambda x: ' '.join(x))\n",
    " \n",
    "  # ##################-------Remove punctuations-------##################\n",
    "#   data_[\"FINAL_TEXT\"] = data_[\"FINAL_TEXT\"].astype(str).str.translate(str.maketrans(\"\", \"\", string.punctuation)).str.replace(\"’\", \" \").str.replace(\"“\", \" \").str.replace(\"”\", \" \")\n",
    "\n",
    "#   final_df[\"FINAL_TEXT\"] = data_[\"FINAL_TEXT\"]\n",
    "#   final_df = data_\n",
    "  return data_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "ROcVKfprPobe"
   },
   "outputs": [],
   "source": [
    "def split_dataframe(df, nums = num_cores): \n",
    "    chunks = list()\n",
    "    num_chunks = nums\n",
    "    chunk_size = len(df) // nums\n",
    "    for i in range(num_chunks):\n",
    "        chunks.append(df[i*chunk_size:(i+1)*chunk_size])\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data_and_save(data_,filename):\n",
    "    print(data_.shape)\n",
    "    start = time.time()\n",
    "    pool = Pool(num_cores)\n",
    "    df = split_dataframe(data_, num_cores)\n",
    "    data1 = pd.concat(pool.map(df_cleaning, df))\n",
    "    data = data1.copy()\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    end = time.time()\n",
    "    directory = \"output_data/{}\".format(date_to_analyse)\n",
    "    create_directory(directory)\n",
    "    data1.to_csv(\"{}/{}.csv\".format(directory, filename))\n",
    "    print(\"time = \", end - start)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_all_data_and_save(data_):\n",
    "    non_quote_non_rt, quote_non_rt, quote_rt, non_quote_rt = data_\n",
    "    non_quote_non_rt = clean_data_and_save(non_quote_non_rt, 'non_quote_non_rt')\n",
    "    quote_non_rt = clean_data_and_save(quote_non_rt, 'quote_non_rt')    \n",
    "    non_quote_rt = clean_data_and_save(non_quote_rt, 'non_quote_rt')\n",
    "    quote_rt = clean_data_and_save(quote_rt, 'quote_rt')\n",
    "    return non_quote_non_rt, quote_non_rt, quote_rt, non_quote_rt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_all_data(data_):\n",
    "    non_quote_non_rt, quote_non_rt, quote_rt, non_quote_rt = data_\n",
    "    combined = pd.concat([non_quote_non_rt, quote_non_rt, quote_rt, non_quote_rt])\n",
    "    directory = \"output_data/{}\".format(date_to_analyse)\n",
    "    create_directory(directory)\n",
    "    combined.to_csv('{}/combined_{}.csv'.format(directory, date_to_analyse), index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILENAME = \"h01-20200818-10files.zip\"\n",
    "# FILENAME = \"/home/manikya_varshney/Documents/Python/Yale/h01-20200818-10files.zip\"\n",
    "TEMP_FOLDER = \"/tmp\"\n",
    "FILES_NAMES = create_filename(FILENAME, TEMP_FOLDER)\n",
    "date_to_analyse, year = strip_date(FILES_NAMES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = separate_df(FILENAME, TEMP_FOLDER, FILES_NAMES, date_to_analyse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28689, 81)\n",
      "time =  4.630657434463501\n",
      "(9171, 81)\n",
      "time =  1.83561372756958\n",
      "(49999, 81)\n",
      "time =  8.074134349822998\n",
      "(17882, 81)\n",
      "time =  5.409012794494629\n"
     ]
    }
   ],
   "source": [
    "non_quote_non_rt, quote_non_rt, quote_rt, non_quote_rt = clean_all_data_and_save(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_all_data((non_quote_non_rt, quote_non_rt, quote_rt, non_quote_rt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'output_data/Tue Aug 18/combined_Tue Aug 18.csv'\n",
    "data = pd.read_csv(path, index_col=None, header=0, engine='python' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>id</th>\n",
       "      <th>source</th>\n",
       "      <th>is_quote_tweet</th>\n",
       "      <th>quoted_tweet_id</th>\n",
       "      <th>in_reply_to_status_id_str</th>\n",
       "      <th>in_reply_to_user_id_str</th>\n",
       "      <th>in_reply_to_screen_name</th>\n",
       "      <th>lang</th>\n",
       "      <th>quote_count</th>\n",
       "      <th>...</th>\n",
       "      <th>QT_verified</th>\n",
       "      <th>QT_place_id</th>\n",
       "      <th>QT_place_full_name</th>\n",
       "      <th>QT_place_country_code</th>\n",
       "      <th>QT_coordinates</th>\n",
       "      <th>QT_text</th>\n",
       "      <th>QT_full_text</th>\n",
       "      <th>TEST_FLAG</th>\n",
       "      <th>RT</th>\n",
       "      <th>FINAL_TEXT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tue Aug 18 19:46:26 +0000 2020</td>\n",
       "      <td>1.2958092994538086e+18</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.2958060139087012e+18</td>\n",
       "      <td>8.649260500354007e+17</td>\n",
       "      <td>redditships</td>\n",
       "      <td>en</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Y</td>\n",
       "      <td>False</td>\n",
       "      <td>redditship sure lost job bc the pandem vet sur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tue Aug 18 19:46:26 +0000 2020</td>\n",
       "      <td>1.29580930000316e+18</td>\n",
       "      <td>&lt;a href=\"https://mobile.twitter.com\" rel=\"nofo...</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Y</td>\n",
       "      <td>False</td>\n",
       "      <td>your parent your told to send your child to sc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tue Aug 18 19:46:26 +0000 2020</td>\n",
       "      <td>1.2958093004854804e+18</td>\n",
       "      <td>&lt;a href=\"https://about.twitter.com/products/tw...</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Y</td>\n",
       "      <td>False</td>\n",
       "      <td>cu boulder oncampu test confirm 6 case covid19...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tue Aug 18 19:46:26 +0000 2020</td>\n",
       "      <td>1.295809300645007e+18</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.2957933158840197e+18</td>\n",
       "      <td>67175426.0</td>\n",
       "      <td>FletchersDogs</td>\n",
       "      <td>en</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Y</td>\n",
       "      <td>False</td>\n",
       "      <td>fletchersdog true look like commun wont the fu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tue Aug 18 19:46:26 +0000 2020</td>\n",
       "      <td>1.2958093004059156e+18</td>\n",
       "      <td>&lt;a href=\"https://mobile.twitter.com\" rel=\"nofo...</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Y</td>\n",
       "      <td>False</td>\n",
       "      <td>arsboni dominik prankl the constitution austri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105732</th>\n",
       "      <td>Tue Aug 18 20:16:15 +0000 2020</td>\n",
       "      <td>1.2958168023254835e+18</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Y</td>\n",
       "      <td>True</td>\n",
       "      <td>600 miami dade school employe posit covid 19 d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105733</th>\n",
       "      <td>Tue Aug 18 20:16:15 +0000 2020</td>\n",
       "      <td>1.2958168031727124e+18</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Y</td>\n",
       "      <td>True</td>\n",
       "      <td>drdavidbul the govern need to lift the pointle...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105734</th>\n",
       "      <td>Tue Aug 18 20:16:15 +0000 2020</td>\n",
       "      <td>1.295816803617321e+18</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Y</td>\n",
       "      <td>True</td>\n",
       "      <td>lt covid19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105735</th>\n",
       "      <td>Tue Aug 18 20:16:15 +0000 2020</td>\n",
       "      <td>1.295816804158169e+18</td>\n",
       "      <td>&lt;a href=\"https://mobile.twitter.com\" rel=\"nofo...</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Y</td>\n",
       "      <td>True</td>\n",
       "      <td>break sampp 500 rise to record close fulli wip...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105736</th>\n",
       "      <td>Tue Aug 18 20:16:15 +0000 2020</td>\n",
       "      <td>1.2958168040115814e+18</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Y</td>\n",
       "      <td>True</td>\n",
       "      <td>the snp must abandon care home to covid19 nati...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>105737 rows × 81 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            created_at                      id  \\\n",
       "0       Tue Aug 18 19:46:26 +0000 2020  1.2958092994538086e+18   \n",
       "1       Tue Aug 18 19:46:26 +0000 2020    1.29580930000316e+18   \n",
       "2       Tue Aug 18 19:46:26 +0000 2020  1.2958093004854804e+18   \n",
       "3       Tue Aug 18 19:46:26 +0000 2020   1.295809300645007e+18   \n",
       "4       Tue Aug 18 19:46:26 +0000 2020  1.2958093004059156e+18   \n",
       "...                                ...                     ...   \n",
       "105732  Tue Aug 18 20:16:15 +0000 2020  1.2958168023254835e+18   \n",
       "105733  Tue Aug 18 20:16:15 +0000 2020  1.2958168031727124e+18   \n",
       "105734  Tue Aug 18 20:16:15 +0000 2020   1.295816803617321e+18   \n",
       "105735  Tue Aug 18 20:16:15 +0000 2020   1.295816804158169e+18   \n",
       "105736  Tue Aug 18 20:16:15 +0000 2020  1.2958168040115814e+18   \n",
       "\n",
       "                                                   source is_quote_tweet  \\\n",
       "0       <a href=\"http://twitter.com/download/android\" ...          False   \n",
       "1       <a href=\"https://mobile.twitter.com\" rel=\"nofo...          False   \n",
       "2       <a href=\"https://about.twitter.com/products/tw...          False   \n",
       "3       <a href=\"http://twitter.com/download/android\" ...          False   \n",
       "4       <a href=\"https://mobile.twitter.com\" rel=\"nofo...          False   \n",
       "...                                                   ...            ...   \n",
       "105732  <a href=\"http://twitter.com/download/iphone\" r...          False   \n",
       "105733  <a href=\"http://twitter.com/download/android\" ...          False   \n",
       "105734  <a href=\"http://twitter.com/download/android\" ...          False   \n",
       "105735  <a href=\"https://mobile.twitter.com\" rel=\"nofo...          False   \n",
       "105736  <a href=\"http://twitter.com/download/android\" ...          False   \n",
       "\n",
       "       quoted_tweet_id in_reply_to_status_id_str in_reply_to_user_id_str  \\\n",
       "0                  NaN    1.2958060139087012e+18   8.649260500354007e+17   \n",
       "1                  NaN                       NaN                     NaN   \n",
       "2                  NaN                       NaN                     NaN   \n",
       "3                  NaN    1.2957933158840197e+18              67175426.0   \n",
       "4                  NaN                       NaN                     NaN   \n",
       "...                ...                       ...                     ...   \n",
       "105732             NaN                       NaN                     NaN   \n",
       "105733             NaN                       NaN                     NaN   \n",
       "105734             NaN                       NaN                     NaN   \n",
       "105735             NaN                       NaN                     NaN   \n",
       "105736             NaN                       NaN                     NaN   \n",
       "\n",
       "       in_reply_to_screen_name lang quote_count  ... QT_verified  QT_place_id  \\\n",
       "0                  redditships   en         0.0  ...         NaN          NaN   \n",
       "1                          NaN   en         0.0  ...         NaN          NaN   \n",
       "2                          NaN   en         0.0  ...         NaN          NaN   \n",
       "3                FletchersDogs   en         0.0  ...         NaN          NaN   \n",
       "4                          NaN   en         0.0  ...         NaN          NaN   \n",
       "...                        ...  ...         ...  ...         ...          ...   \n",
       "105732                     NaN   en         0.0  ...         NaN          NaN   \n",
       "105733                     NaN   en         0.0  ...         NaN          NaN   \n",
       "105734                     NaN   en         0.0  ...         NaN          NaN   \n",
       "105735                     NaN   en         0.0  ...         NaN          NaN   \n",
       "105736                     NaN   en         0.0  ...         NaN          NaN   \n",
       "\n",
       "        QT_place_full_name QT_place_country_code QT_coordinates QT_text  \\\n",
       "0                      NaN                   NaN            NaN     NaN   \n",
       "1                      NaN                   NaN            NaN     NaN   \n",
       "2                      NaN                   NaN            NaN     NaN   \n",
       "3                      NaN                   NaN            NaN     NaN   \n",
       "4                      NaN                   NaN            NaN     NaN   \n",
       "...                    ...                   ...            ...     ...   \n",
       "105732                 NaN                   NaN            NaN     NaN   \n",
       "105733                 NaN                   NaN            NaN     NaN   \n",
       "105734                 NaN                   NaN            NaN     NaN   \n",
       "105735                 NaN                   NaN            NaN     NaN   \n",
       "105736                 NaN                   NaN            NaN     NaN   \n",
       "\n",
       "       QT_full_text TEST_FLAG     RT  \\\n",
       "0               NaN         Y  False   \n",
       "1               NaN         Y  False   \n",
       "2               NaN         Y  False   \n",
       "3               NaN         Y  False   \n",
       "4               NaN         Y  False   \n",
       "...             ...       ...    ...   \n",
       "105732          NaN         Y   True   \n",
       "105733          NaN         Y   True   \n",
       "105734          NaN         Y   True   \n",
       "105735          NaN         Y   True   \n",
       "105736          NaN         Y   True   \n",
       "\n",
       "                                               FINAL_TEXT  \n",
       "0       redditship sure lost job bc the pandem vet sur...  \n",
       "1       your parent your told to send your child to sc...  \n",
       "2       cu boulder oncampu test confirm 6 case covid19...  \n",
       "3       fletchersdog true look like commun wont the fu...  \n",
       "4       arsboni dominik prankl the constitution austri...  \n",
       "...                                                   ...  \n",
       "105732  600 miami dade school employe posit covid 19 d...  \n",
       "105733  drdavidbul the govern need to lift the pointle...  \n",
       "105734                                         lt covid19  \n",
       "105735  break sampp 500 rise to record close fulli wip...  \n",
       "105736  the snp must abandon care home to covid19 nati...  \n",
       "\n",
       "[105737 rows x 81 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Data_Cleaning_and_Analysis_V2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
