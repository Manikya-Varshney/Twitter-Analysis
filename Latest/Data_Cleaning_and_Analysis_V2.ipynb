{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JHBSrnVsmQT1"
   },
   "source": [
    "\n",
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "iN58fBbFo48-"
   },
   "source": [
    "# Imports for GDrive\n",
    "!pip install -U -q PyDrive\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from google.colab import auth\n",
    "from oauth2client.client import GoogleCredentials"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "XssQCdN5pE03"
   },
   "source": [
    "# Mounting the drive\n",
    "auth.authenticate_user()\n",
    "gauth = GoogleAuth()\n",
    "gauth.credentials = GoogleCredentials.get_application_default()\n",
    "drive = GoogleDrive(gauth)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "3zJK_ICLpTmX"
   },
   "source": [
    "#downloaded = drive.CreateFile({'id':'https://drive.google.com/file/d/17dnH3TDdLmxos83OTHEtXBCD5Tmu-L_k/view?usp=sharing'}) # replace the id with id of file you want to access\n",
    "#downloaded.GetContentFile('h01-20201001-20201008.zip') "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QOdpxl2-w7sR",
    "outputId": "94499167-2ecf-416c-c6ce-b153f5acbd91"
   },
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1xtF___3xLV3",
    "outputId": "152e3328-6a9b-4d40-b40b-959df2d60dfc"
   },
   "source": [
    "import os\n",
    "os.listdir('/content/gdrive/Shared drives/') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XiUQenL10eNC",
    "outputId": "56b52020-09be-4791-e2ca-7fcc010aae99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: emoji in /home/manikya_varshney/.local/lib/python3.8/site-packages (1.2.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.3.3; however, version 21.2.4 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/manikya_varshney/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/manikya_varshney/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/manikya_varshney/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/manikya_varshney/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "#!pip install pandas\n",
    "#!pip install numpy\n",
    "#!pip install nltk\n",
    "#!pip install multiprocess\n",
    "!pip install emoji\n",
    "\n",
    "# Import statements\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import csv\n",
    "import re\n",
    "import string\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "\n",
    "import multiprocess\n",
    "from multiprocess import Pool, Process\n",
    "import multiprocessing as mp\n",
    "\n",
    "\n",
    "\n",
    "import emoji\n",
    "import nltk\n",
    "from nltk import pos_tag, pos_tag_sents\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "stop_words = set(stopwords.words('english')) - set(['at', 'do', 'your', 'from', 'to', 'out', 'no', 'the'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "G7-qNi6n_pMM"
   },
   "outputs": [],
   "source": [
    "# Extract zip file\n",
    "\n",
    "def extract_zips(filename, temp_folder):\n",
    "  zip_ref = zipfile.ZipFile(filename, 'r')\n",
    "  zip_ref.extractall(temp_folder)\n",
    "  zip_ref.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "OHS5daIFGbLI"
   },
   "outputs": [],
   "source": [
    "def create_filename(FILENAME, TEMP_FOLDER):\n",
    "  FILES_NAMES = FILENAME.split(\".\")[0].split(\"/\")[-1]\n",
    "  FILES_NAMES = FILES_NAMES.rsplit(\"-\", 1)[0]\n",
    "  # FILES_NAMES = os.path.join(TEMP_FOLDER, FILES_NAMES)\n",
    "  return FILES_NAMES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "xvgg28WxGj1R"
   },
   "outputs": [],
   "source": [
    "def get_date_string(FILESNAMES):\n",
    "  date_string = FILESNAMES.split(\"-\")[-1]\n",
    "  return date_string\n",
    "\n",
    "def strip_date(FILESNAMES):\n",
    "  date_string = get_date_string(FILESNAMES)\n",
    "  ob = datetime.strptime(date_string, \"%Y%m%d\")\n",
    "  date_to_analyse = ob.strftime(\"%a %b %d\")\n",
    "  year = ob.strftime(\"%Y\")\n",
    "  return date_to_analyse, year\n",
    "\n",
    "def get_date_numbers(date_string):\n",
    "    ob = datetime.strptime(date_string, \"%a %b %d\")\n",
    "    return ob.strftime(\"%d\"), ob.strftime(\"%m\")\n",
    "    \n",
    "def get_next_date(current_date,year):\n",
    "    day, month = get_date_numbers(current_date)\n",
    "    date = datetime(int(year), int(month), int(day))\n",
    "    date += timedelta(days = 1)\n",
    "    return date.strftime(\"%Y%m%d\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_directory(directory):\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Axem73RZAAkA"
   },
   "outputs": [],
   "source": [
    "# Constants\n",
    "#FILENAME = \"h01-20200818-10files.zip\"\n",
    "FILENAME = \"/home/manikya_varshney/Documents/Python/Yale/h01-20200818-10files.zip\"\n",
    "TEMP_FOLDER = \"/tmp\"\n",
    "FILES_NAMES = create_filename(FILENAME, TEMP_FOLDER)\n",
    "date_to_analyse = strip_date(FILES_NAMES)\n",
    "num_cores = mp.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "added_ids = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "ZuK9Y_DSL0cL"
   },
   "outputs": [],
   "source": [
    "# Keep data in english only\n",
    "def remove_other_langs(data):\n",
    "  data = data[data['lang'] == 'en'].reset_index(drop=True)\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_other_date(data, old_data, current_date):\n",
    "    if data.equals(old_data):\n",
    "        return\n",
    "    date_to_save = get_next_date(current_date, year)\n",
    "    print(date_to_save)\n",
    "    directory = \"data\"\n",
    "    create_directory(directory)\n",
    "    combined = pd.concat([data, old_data]).drop_duplicates(keep=False)\n",
    "    \n",
    "    combined.to_csv(\"{}/{}.csv\".format(directory, date_to_save))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Xwsew-o1MzvI"
   },
   "outputs": [],
   "source": [
    "# Keep specific date\n",
    "def remove_other_dates(data, date_to_analyse):\n",
    "  new_data = data[data['created_at'].str[:10] == date_to_analyse].reset_index(drop=True)\n",
    "  save_other_date(new_data, data, date_to_analyse)\n",
    "  return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "fnpiNeSksPHD"
   },
   "outputs": [],
   "source": [
    "# Creating the RT Column\n",
    "def create_rt_column(data):\n",
    "  data['RT'] = data['text'].str[:2]=='RT'\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "jDie85V6J4IJ"
   },
   "outputs": [],
   "source": [
    "# Parse CSV data\n",
    "def parse_data_from_file(filename, date_to_analyse):\n",
    "  data = pd.read_csv(filename, index_col = None, header=0, engine = 'python')\n",
    "  data = remove_other_langs(data)\n",
    "  data = remove_other_dates(data, date_to_analyse)\n",
    "  data = create_rt_column(data)\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "oksZ2gyHLTYM"
   },
   "outputs": [],
   "source": [
    "# Parse all files of the same date\n",
    "def parse_all_files(TEMP_FOLDER, FILES_NAMES, date_to_analyse):\n",
    "  files = os.listdir(TEMP_FOLDER)\n",
    "  new_data = []\n",
    "  for file in files:\n",
    "    if file.startswith(FILES_NAMES):\n",
    "      parsed_data = parse_data_from_file(os.path.join(TEMP_FOLDER, file), date_to_analyse)\n",
    "      new_data.append(parsed_data)\n",
    "  return pd.concat(new_data, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "d6dvi4iZuO7z"
   },
   "outputs": [],
   "source": [
    "def get_text_or_extended_text(data, added_ids):\n",
    "  data[\"FINAL_TEXT\"] = np.where(data['extended_tweet_full_text'].notnull(), data[\"extended_tweet_full_text\"], data[\"text\"])\n",
    "  id = data[\"id\"].to_dict()\n",
    "  added_ids.update(id)\n",
    "  return data[\"FINAL_TEXT\"]\n",
    "\n",
    "def get_quoted_text(data, added_ids):\n",
    "  data[\"FINAL_TEXT\"] = np.where(data['QT_full_text'].notnull(), data[\"QT_full_text\"], data[\"QT_text\"])\n",
    "  added_ids.update(data[\"id\"].to_dict())\n",
    "  added_ids.update(data[\"QT_id\"].to_dict())\n",
    "  return data[\"FINAL_TEXT\"]\n",
    "\n",
    "def get_text_or_full_text_rt(data, added_ids):\n",
    "  data[\"FINAL_TEXT\"] = np.where(data['RT_full_text'].notnull(), data[\"RT_full_text\"], data[\"RT_text\"])\n",
    "  added_ids.update(data[\"id\"].to_dict())\n",
    "  added_ids.update(data[\"RT_id\"].to_dict())\n",
    "  return data[\"FINAL_TEXT\"]\n",
    "\n",
    "def get_quote_rt_full(data, added_ids):\n",
    "  data[\"FINAL_TEXT\"] = get_text_or_full_text_rt(data, added_ids) + get_quoted_text(data, added_ids)\n",
    "  return data[\"FINAL_TEXT\"]\n",
    "\n",
    "\n",
    "\n",
    "def get_quote_rt(data, is_quote, is_rt):\n",
    "  data = data.loc[(data['is_quote_tweet'] == is_quote) & (data['RT'] == is_rt)]\n",
    "\n",
    "  if not is_quote and not is_rt:\n",
    "    added_ids.update(dict(zip(data[\"id\"], data[\"is_quote_tweet\"])))\n",
    "    data[\"FINAL_TEXT\"] = np.where(data['extended_tweet_full_text'].notnull(), data[\"extended_tweet_full_text\"], data[\"text\"])\n",
    "\n",
    "  if is_quote and not is_rt:\n",
    "    data[\"FINAL_TEXT\"] = np.where(data[\"QT_id\"].isin(added_ids.keys()), get_text_or_extended_text(data, added_ids), get_quoted_text(data, added_ids))\n",
    "\n",
    "  if not is_quote and is_rt:\n",
    "    data[\"FINAL_TEXT\"] = np.where(data[\"RT_id\"].isin(added_ids.keys()), added_ids.update(data['id'].to_dict()), get_text_or_full_text_rt(data, added_ids))\n",
    "    data = data[data[\"FINAL_TEXT\"].notna()]\n",
    "\n",
    "  if is_quote and is_rt:\n",
    "    data[\"FINAL_TEXT\"] = np.where((data[\"RT_id\"].isin(added_ids.keys())) & (data[\"QT_id\"].isin(added_ids.keys())), added_ids.update(data['id'].to_dict()), None)\n",
    "    data[\"FINAL_TEXT\"] = np.where((data[\"RT_id\"].isin(added_ids.keys())) & (~data[\"QT_id\"].isin(added_ids.keys())), get_quoted_text(data, added_ids), None)\n",
    "    data[\"FINAL_TEXT\"] = np.where((~data[\"RT_id\"].isin(added_ids.keys())) & (data[\"QT_id\"].isin(added_ids.keys())), get_text_or_full_text_rt(data, added_ids), None)\n",
    "    data[\"FINAL_TEXT\"] = np.where((~data[\"RT_id\"].isin(added_ids.keys())) & (~data[\"QT_id\"].isin(added_ids.keys())), get_quote_rt_full(data, added_ids), None)\n",
    "    data = data[data[\"FINAL_TEXT\"].notna()]\n",
    "\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_df(filename, TEMP_FOLDER, FILES_NAMES, date_to_analyse):\n",
    "    extract_zips(filename, TEMP_FOLDER)\n",
    "    data = parse_all_files(TEMP_FOLDER, FILES_NAMES, date_to_analyse)\n",
    "\n",
    "    non_quote_non_rt = get_quote_rt(data, False, False)\n",
    "    quote_non_rt = get_quote_rt(data, True, False)\n",
    "    quote_rt = get_quote_rt(data, True, True)\n",
    "    non_quote_rt = get_quote_rt(data, False, True)\n",
    "    \n",
    "    return non_quote_non_rt, quote_non_rt, quote_rt, non_quote_rt\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HCpVf_aD1lV-"
   },
   "source": [
    "1. Casing (Upper or lower case)\n",
    "2. Noise Removal (Removal of punctuation, white spaces, special characters, HTML tags)\n",
    "3. Tokenization (Tweets to tokens i.e. words seprated by spaces)\n",
    "4. Stopword Removal\n",
    "5. Text Normalization (Stemming and Lemmatization) bold text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "e212hr-q2LKf"
   },
   "outputs": [],
   "source": [
    "#Removing emojis\n",
    "def demoji(text):\n",
    "  emoji_pattern = re.compile(\"[\"\n",
    "  u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                u\"\\U00002702-\\U000027B0\"\n",
    "                u\"\\U000024C2-\\U0001F251\"\n",
    "                u\"\\U0001f926-\\U0001f937\"\n",
    "                u'\\U00010000-\\U0010ffff'\n",
    "                u\"\\u200d\"\n",
    "                u\"\\u2640-\\u2642\"\n",
    "                u\"\\u2600-\\u2B55\"\n",
    "                u\"\\u23cf\"\n",
    "                u\"\\u23e9\"\n",
    "                u\"\\u231a\"\n",
    "                u\"\\u3030\"\n",
    "                u\"\\ufe0f\"\n",
    "                        \"]+\", flags=re.UNICODE)\n",
    "  return(emoji_pattern.sub(r'', text.decode('utf-8')))\n",
    "\n",
    "#POSTags\n",
    "def get_wordnet_pos(word):\n",
    "  \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "  tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "  tag_dict = {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV}\n",
    "  return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "def apply_stemming(row):\n",
    "  row_list = row[\"FINAL_TEXT\"]\n",
    "  stemmed_list = [ps.stem(word) for word in row_list]\n",
    "  return (stemmed_list)\n",
    "\n",
    "#Remove URLs, user@, punctutions\n",
    "def df_cleaning(data_):\n",
    "  punc = string.punctuation\n",
    "  data_[\"FINAL_TEXT\"] = data_[\"FINAL_TEXT\"].astype(str).str.replace(r\"http\\S+| www\\S+| https\\S+| \\S+\\.com\\S+| \\S+\\.com| \\@[\\w]+\", \"\", regex=True)\n",
    "\n",
    "  # ##################-------Punctutions-------##################\n",
    "  data_[\"FINAL_TEXT\"] = data_[\"FINAL_TEXT\"].astype(str).str.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    " \n",
    "  # ##################-------More Cleaning-------##################\n",
    "  data_[\"FINAL_TEXT\"] = data_[\"FINAL_TEXT\"].astype(str).str.replace(\"/[^a-zA-Z0-9 ]/g\", \"\", regex=True).str.replace(\"\\n\",\" \", regex=True).str.replace(\"‚Äî\",\" \", regex=True).str.strip(\"‚Äú\").str.strip(\"‚Äù\").str.strip(\"‚Äô\").str.lstrip(\" \").str.rstrip(\" \")\n",
    "  \n",
    "  # ##################-------Emojis-------##################\n",
    "  data_[\"FINAL_TEXT\"] = data_[\"FINAL_TEXT\"].astype(str).apply(lambda x:demoji(x.encode('utf8')))\n",
    "\n",
    "  # ##################-------Tokenizing-------##################\n",
    "  data_[\"FINAL_TEXT\"] = data_[\"FINAL_TEXT\"].astype(str)\n",
    "  data_[\"FINAL_TEXT\"] = data_[\"FINAL_TEXT\"].apply(nltk.word_tokenize)\n",
    "#   data_[\"FINAL_TEXT\"] = data_.apply(lambda row: nltk.word_tokenize(row[\"FINAL_TEXT\"]), axis=1)\n",
    "\n",
    "  # ##################-------Lower characters---------##################\n",
    "  data_[\"FINAL_TEXT\"] = data_[\"FINAL_TEXT\"].apply(lambda x: [word.lower() for word in x])\n",
    "\n",
    "  # ##################-------Remove punctuations-------##################\n",
    "  data_[\"FINAL_TEXT\"] = data_[\"FINAL_TEXT\"].apply(lambda x: [word for word in x if word not in punc])\n",
    "\n",
    "  # ##################-------Removing stopwords-------##################\n",
    "#   data_[\"FINAL_TEXT\"] = data_[\"FINAL_TEXT\"].astype(str).apply(lambda x: [word for word in x if word not in stop_words])\n",
    "  \n",
    "  # data_[\"FINAL_TEXT\"] = data_[\"FINAL_TEXT\"].astype(str).apply(lambda x: [word for word in x if word not in stop_words])\n",
    "  data_[\"FINAL_TEXT\"] = data_[\"FINAL_TEXT\"].apply(lambda words: ' '.join(word for word in words if word not in stop_words))\n",
    " \n",
    "  \n",
    "  # ##################-------Stemming-------##################\n",
    "  ps = PorterStemmer()\n",
    "#   data_[\"FINAL_TEXT\"] = data_[\"FINAL_TEXT\"].apply(lambda x: [ps.stem(y) for y in x])\n",
    "  data_[\"FINAL_TEXT\"] = data_[\"FINAL_TEXT\"].apply(lambda x: [ps.stem(y) for y in x.split(' ')])\n",
    "#   data_[\"FINAL_TEXT\"] = data_.apply(apply_stemming, axis = 1)\n",
    "#   data_[\"FINAL_TEXT\"] = data_.apply(ps.stem)\n",
    "\n",
    "#   print(data_[\"FINAL_TEXT\"], \"from here\")\n",
    "\n",
    "\n",
    "  # ##################-------Lemmatizing-------##################\n",
    "#   lemmatizer = WordNetLemmatizer()\n",
    "#   data[\"FINAL_TEXT\"] = data[\"FINAL_TEXT\"].astype(str).apply(lambda x: [lemmatizer.lemmatize(y, get_wordnet_pos(y)) for y in x])\n",
    "  \n",
    "  # ##################-------Joining the lemmetized tokens to form string-------##################\n",
    "  data_[\"FINAL_TEXT\"] = data_[\"FINAL_TEXT\"].apply(lambda x: ' '.join(x))\n",
    " \n",
    "  # ##################-------Remove punctuations-------##################\n",
    "#   data_[\"FINAL_TEXT\"] = data_[\"FINAL_TEXT\"].astype(str).str.translate(str.maketrans(\"\", \"\", string.punctuation)).str.replace(\"‚Äô\", \" \").str.replace(\"‚Äú\", \" \").str.replace(\"‚Äù\", \" \")\n",
    "\n",
    "#   final_df[\"FINAL_TEXT\"] = data_[\"FINAL_TEXT\"]\n",
    "#   final_df = data_\n",
    "  return data_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "ROcVKfprPobe"
   },
   "outputs": [],
   "source": [
    "def split_dataframe(df, nums = num_cores): \n",
    "    chunks = list()\n",
    "    num_chunks = nums\n",
    "    chunk_size = len(df) // nums\n",
    "    for i in range(num_chunks):\n",
    "        chunks.append(df[i*chunk_size:(i+1)*chunk_size])\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data_and_save(data_,filename):\n",
    "    print(data_.shape)\n",
    "    start = time.time()\n",
    "    pool = Pool(num_cores)\n",
    "    df = split_dataframe(data_, num_cores)\n",
    "    data1 = pd.concat(pool.map(df_cleaning, df))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    end = time.time()\n",
    "    directory = \"output_data/{}\".format(date_to_analyse)\n",
    "    create_directory(directory)\n",
    "    data1.to_csv(\"{}/{}.csv\".format(directory, filename))\n",
    "    print(\"time = \", end - start)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_all_data_and_save(data_):\n",
    "    non_quote_non_rt, quote_non_rt, quote_rt, non_quote_rt = data_\n",
    "    clean_data_and_save(non_quote_non_rt, 'non_quote_non_rt')\n",
    "    clean_data_and_save(quote_non_rt, 'quote_non_rt')    \n",
    "    clean_data_and_save(non_quote_rt, 'non_quote_rt')\n",
    "    clean_data_and_save(quote_rt, 'quote_rt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_all_data(data_):\n",
    "    non_quote_non_rt, quote_non_rt, quote_rt, non_quote_rt = data_\n",
    "    non_quote_non_rt = non_quote_non_rt.reset_index()\n",
    "    quote_non_rt = quote_non_rt.reset_index()  \n",
    "    quote_rt = quote_rt.reset_index()  \n",
    "    non_quote_rt = non_quote_rt.reset_index()  \n",
    "    combined = pd.concat([non_quote_non_rt, quote_non_rt, quote_rt, non_quote_rt],axis=1 , ignore_index=True)\n",
    "    directory = \"output_data/{}\".format(date_to_analyse)\n",
    "    create_directory(directory)\n",
    "    combined.to_csv('{}/combined_{}.csv'.format(directory, date_to_analyse))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FILENAME = \"h01-20200818-10files.zip\"\n",
    "FILENAME = \"/home/manikya_varshney/Documents/Python/Yale/h01-20200818-10files.zip\"\n",
    "TEMP_FOLDER = \"/tmp\"\n",
    "FILES_NAMES = create_filename(FILENAME, TEMP_FOLDER)\n",
    "date_to_analyse, year = strip_date(FILES_NAMES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = separate_df(FILENAME, TEMP_FOLDER, FILES_NAMES, date_to_analyse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28689, 81)\n",
      "time =  5.719989538192749\n",
      "(9171, 81)\n",
      "time =  2.1625313758850098\n",
      "(49999, 81)\n",
      "time =  9.4677574634552\n",
      "(17882, 81)\n",
      "time =  5.574165344238281\n"
     ]
    }
   ],
   "source": [
    "clean_all_data_and_save(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_all_data(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/manikya_varshney/Documents/Python/Yale/Latest/output_data/Tue Aug 18/combined_Tue Aug 18.csv'\n",
    "data = pd.read_csv(path, index_col=None, header=0, engine='python' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>318</th>\n",
       "      <th>319</th>\n",
       "      <th>320</th>\n",
       "      <th>321</th>\n",
       "      <th>322</th>\n",
       "      <th>323</th>\n",
       "      <th>324</th>\n",
       "      <th>325</th>\n",
       "      <th>326</th>\n",
       "      <th>327</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Tue Aug 18 19:46:26 +0000 2020</td>\n",
       "      <td>1.2958092994538086e+18</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.295806e+18</td>\n",
       "      <td>8.649261e+17</td>\n",
       "      <td>redditships</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Y</td>\n",
       "      <td>True</td>\n",
       "      <td>Most people just want school to resume so they...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>11.0</td>\n",
       "      <td>Tue Aug 18 19:46:26 +0000 2020</td>\n",
       "      <td>1.29580930000316e+18</td>\n",
       "      <td>&lt;a href=\"https://mobile.twitter.com\" rel=\"nofo...</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Y</td>\n",
       "      <td>True</td>\n",
       "      <td>They sent my granddaughter home from school ou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>13.0</td>\n",
       "      <td>Tue Aug 18 19:46:26 +0000 2020</td>\n",
       "      <td>1.2958093004854804e+18</td>\n",
       "      <td>&lt;a href=\"https://about.twitter.com/products/tw...</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Y</td>\n",
       "      <td>True</td>\n",
       "      <td>Millions are going to die before the covid-19 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>17.0</td>\n",
       "      <td>Tue Aug 18 19:46:26 +0000 2020</td>\n",
       "      <td>1.295809300645007e+18</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.295793e+18</td>\n",
       "      <td>6.717543e+07</td>\n",
       "      <td>FletchersDogs</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Y</td>\n",
       "      <td>True</td>\n",
       "      <td>This COVID shit lasting like a Honda Civic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>18.0</td>\n",
       "      <td>Tue Aug 18 19:46:26 +0000 2020</td>\n",
       "      <td>1.2958093004059156e+18</td>\n",
       "      <td>&lt;a href=\"https://mobile.twitter.com\" rel=\"nofo...</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Y</td>\n",
       "      <td>True</td>\n",
       "      <td>future surgeon in ur bio but you‚Äôre going to l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50004</th>\n",
       "      <td>49994</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Y</td>\n",
       "      <td>True</td>\n",
       "      <td>For those who are asking ano yung mga naging s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50005</th>\n",
       "      <td>49995</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Y</td>\n",
       "      <td>True</td>\n",
       "      <td>The @WhiteHouse Coronavirus Task Force remains...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50006</th>\n",
       "      <td>49996</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Y</td>\n",
       "      <td>True</td>\n",
       "      <td>Over 2,000 students in Cherokee County, GA are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50007</th>\n",
       "      <td>49997</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Y</td>\n",
       "      <td>True</td>\n",
       "      <td>‚òÄÔ∏èüëí\\n\\nüçìSemplicemente... \\n\\nüçâBuon #Ferragosto...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50008</th>\n",
       "      <td>49998</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Y</td>\n",
       "      <td>True</td>\n",
       "      <td>I have a Patreon where I do shirtless livestre...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50009 rows √ó 329 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0     0                               1  \\\n",
       "0              0   5.0  Tue Aug 18 19:46:26 +0000 2020   \n",
       "1              1  11.0  Tue Aug 18 19:46:26 +0000 2020   \n",
       "2              2  13.0  Tue Aug 18 19:46:26 +0000 2020   \n",
       "3              3  17.0  Tue Aug 18 19:46:26 +0000 2020   \n",
       "4              4  18.0  Tue Aug 18 19:46:26 +0000 2020   \n",
       "...          ...   ...                             ...   \n",
       "50004      49994   NaN                             NaN   \n",
       "50005      49995   NaN                             NaN   \n",
       "50006      49996   NaN                             NaN   \n",
       "50007      49997   NaN                             NaN   \n",
       "50008      49998   NaN                             NaN   \n",
       "\n",
       "                            2  \\\n",
       "0      1.2958092994538086e+18   \n",
       "1        1.29580930000316e+18   \n",
       "2      1.2958093004854804e+18   \n",
       "3       1.295809300645007e+18   \n",
       "4      1.2958093004059156e+18   \n",
       "...                       ...   \n",
       "50004                     NaN   \n",
       "50005                     NaN   \n",
       "50006                     NaN   \n",
       "50007                     NaN   \n",
       "50008                     NaN   \n",
       "\n",
       "                                                       3      4    5  \\\n",
       "0      <a href=\"http://twitter.com/download/android\" ...  False  NaN   \n",
       "1      <a href=\"https://mobile.twitter.com\" rel=\"nofo...  False  NaN   \n",
       "2      <a href=\"https://about.twitter.com/products/tw...  False  NaN   \n",
       "3      <a href=\"http://twitter.com/download/android\" ...  False  NaN   \n",
       "4      <a href=\"https://mobile.twitter.com\" rel=\"nofo...  False  NaN   \n",
       "...                                                  ...    ...  ...   \n",
       "50004                                                NaN    NaN  NaN   \n",
       "50005                                                NaN    NaN  NaN   \n",
       "50006                                                NaN    NaN  NaN   \n",
       "50007                                                NaN    NaN  NaN   \n",
       "50008                                                NaN    NaN  NaN   \n",
       "\n",
       "                  6             7              8  ... 318  319  320  321  322  \\\n",
       "0      1.295806e+18  8.649261e+17    redditships  ... NaN  NaN  NaN  NaN  NaN   \n",
       "1               NaN           NaN            NaN  ... NaN  NaN  NaN  NaN  NaN   \n",
       "2               NaN           NaN            NaN  ... NaN  NaN  NaN  NaN  NaN   \n",
       "3      1.295793e+18  6.717543e+07  FletchersDogs  ... NaN  NaN  NaN  NaN  NaN   \n",
       "4               NaN           NaN            NaN  ... NaN  NaN  NaN  NaN  NaN   \n",
       "...             ...           ...            ...  ...  ..  ...  ...  ...  ...   \n",
       "50004           NaN           NaN            NaN  ... NaN  NaN  NaN  NaN  NaN   \n",
       "50005           NaN           NaN            NaN  ... NaN  NaN  NaN  NaN  NaN   \n",
       "50006           NaN           NaN            NaN  ... NaN  NaN  NaN  NaN  NaN   \n",
       "50007           NaN           NaN            NaN  ... NaN  NaN  NaN  NaN  NaN   \n",
       "50008           NaN           NaN            NaN  ... NaN  NaN  NaN  NaN  NaN   \n",
       "\n",
       "      323 324 325   326                                                327  \n",
       "0     NaN NaN   Y  True  Most people just want school to resume so they...  \n",
       "1     NaN NaN   Y  True  They sent my granddaughter home from school ou...  \n",
       "2     NaN NaN   Y  True  Millions are going to die before the covid-19 ...  \n",
       "3     NaN NaN   Y  True         This COVID shit lasting like a Honda Civic  \n",
       "4     NaN NaN   Y  True  future surgeon in ur bio but you‚Äôre going to l...  \n",
       "...    ..  ..  ..   ...                                                ...  \n",
       "50004 NaN NaN   Y  True  For those who are asking ano yung mga naging s...  \n",
       "50005 NaN NaN   Y  True  The @WhiteHouse Coronavirus Task Force remains...  \n",
       "50006 NaN NaN   Y  True  Over 2,000 students in Cherokee County, GA are...  \n",
       "50007 NaN NaN   Y  True  ‚òÄÔ∏èüëí\\n\\nüçìSemplicemente... \\n\\nüçâBuon #Ferragosto...  \n",
       "50008 NaN NaN   Y  True  I have a Patreon where I do shirtless livestre...  \n",
       "\n",
       "[50009 rows x 329 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Data_Cleaning_and_Analysis_V2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
