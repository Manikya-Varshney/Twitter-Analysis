{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JHBSrnVsmQT1"
   },
   "source": [
    "\n",
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "iN58fBbFo48-"
   },
   "source": [
    "# Imports for GDrive\n",
    "!pip install -U -q PyDrive\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from google.colab import auth\n",
    "from oauth2client.client import GoogleCredentials"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "XssQCdN5pE03"
   },
   "source": [
    "# Mounting the drive\n",
    "auth.authenticate_user()\n",
    "gauth = GoogleAuth()\n",
    "gauth.credentials = GoogleCredentials.get_application_default()\n",
    "drive = GoogleDrive(gauth)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "3zJK_ICLpTmX"
   },
   "source": [
    "#downloaded = drive.CreateFile({'id':'https://drive.google.com/file/d/17dnH3TDdLmxos83OTHEtXBCD5Tmu-L_k/view?usp=sharing'}) # replace the id with id of file you want to access\n",
    "#downloaded.GetContentFile('h01-20201001-20201008.zip') "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QOdpxl2-w7sR",
    "outputId": "94499167-2ecf-416c-c6ce-b153f5acbd91"
   },
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1xtF___3xLV3",
    "outputId": "152e3328-6a9b-4d40-b40b-959df2d60dfc"
   },
   "source": [
    "import os\n",
    "os.listdir('/content/gdrive/Shared drives/') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XiUQenL10eNC",
    "outputId": "56b52020-09be-4791-e2ca-7fcc010aae99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: emoji in /opt/anaconda3/envs/twitter/lib/python3.8/site-packages (1.4.2)\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement pytables\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for pytables\u001b[0m\n",
      "Collecting tables\n",
      "  Downloading tables-3.6.1-cp38-cp38-macosx_10_9_x86_64.whl (4.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.3 MB 1.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.9.3 in /opt/anaconda3/envs/twitter/lib/python3.8/site-packages (from tables) (1.21.2)\n",
      "Collecting numexpr>=2.6.2\n",
      "  Downloading numexpr-2.7.3-cp38-cp38-macosx_10_9_x86_64.whl (99 kB)\n",
      "\u001b[K     |████████████████████████████████| 99 kB 31.3 MB/s eta 0:00:011\n",
      "\u001b[?25hInstalling collected packages: numexpr, tables\n",
      "Successfully installed numexpr-2.7.3 tables-3.6.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/priyanshkedia/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/priyanshkedia/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/priyanshkedia/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/priyanshkedia/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "#!pip install pandas\n",
    "#!pip install numpy\n",
    "#!pip install nltk\n",
    "#!pip install multiprocess\n",
    "!pip install emoji\n",
    "!pip install pytables\n",
    "!pip install tables\n",
    "\n",
    "# Import statements\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "import shutil\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import csv\n",
    "import re\n",
    "import string\n",
    "import time\n",
    "from datetime import datetime, timedelta, date\n",
    "\n",
    "\n",
    "import multiprocess\n",
    "from multiprocess import Pool, Process\n",
    "import multiprocessing as mp\n",
    "\n",
    "\n",
    "\n",
    "import emoji\n",
    "import nltk\n",
    "from nltk import pos_tag, pos_tag_sents\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "stop_words = set(stopwords.words('english')) - set(['at', 'do', 'your', 'from', 'to', 'out', 'no', 'the'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "G7-qNi6n_pMM"
   },
   "outputs": [],
   "source": [
    "# Extract zip file\n",
    "\n",
    "def extract_zips(filename, temp_folder):\n",
    "  if os.path.isdir(temp_folder):\n",
    "    return\n",
    "  zip_ref = zipfile.ZipFile(filename, 'r')\n",
    "  zip_ref.extractall(temp_folder)\n",
    "  zip_ref.close()\n",
    "  if os.path.isdir(os.path.join(TEMP_FOLDER, FOLDER_NAME)):\n",
    "    pass\n",
    "  else:\n",
    "    FOLDER_NAME = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "OHS5daIFGbLI"
   },
   "outputs": [],
   "source": [
    "def create_filename(FILENAME, TEMP_FOLDER):\n",
    "  FILES_NAMES = FILENAME.split(\".\")[0].split(\"/\")[-1]\n",
    "  FILES_NAMES = FILES_NAMES.rsplit(\"-\", 1)[0]\n",
    "  # FILES_NAMES = os.path.join(TEMP_FOLDER, FILES_NAMES)\n",
    "  return FILES_NAMES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "xvgg28WxGj1R"
   },
   "outputs": [],
   "source": [
    "def get_date_string(FILESNAMES):\n",
    "  date_string = FILESNAMES.split(\"-\")[-1]\n",
    "  return date_string\n",
    "\n",
    "def strip_date(FILESNAMES):\n",
    "  date_string = get_date_string(FILESNAMES)\n",
    "  ob = datetime.strptime(date_string, \"%Y%m%d\")\n",
    "  date_to_analyse = ob.strftime(\"%a %b %d\")\n",
    "  year = ob.strftime(\"%Y\")\n",
    "  return date_to_analyse, year\n",
    "\n",
    "def get_date_numbers(date_string):\n",
    "    ob = datetime.strptime(date_string, \"%a %b %d\")\n",
    "    return ob.strftime(\"%d\"), ob.strftime(\"%m\")\n",
    "    \n",
    "def get_next_date(current_date,year):\n",
    "    day, month = get_date_numbers(current_date)\n",
    "    date = datetime(int(year), int(month), int(day))\n",
    "    date += timedelta(days = 1)\n",
    "    return date.strftime(\"%Y%m%d\")\n",
    "    \n",
    "def daterange(start_date, end_date):\n",
    "    for n in range(int((end_date - start_date).days)):\n",
    "        yield start_date + timedelta(n)\n",
    "        \n",
    "# h01-20201001-20201008\n",
    "def get_date_range(FILESNAMES):\n",
    "    filesnames = FILESNAMES.split('.')[0]\n",
    "    date_string = filesnames.split(\"-\")[-2:]\n",
    "    start_date = strip_date(date_string[0])\n",
    "    end_date = strip_date(date_string[1])\n",
    "    start_date, start_year, end_date, end_year = start_date[0], start_date[1], end_date[0], end_date[1]\n",
    "    start_day, start_month = get_date_numbers(start_date)\n",
    "    end_day, end_month = get_date_numbers(end_date)\n",
    "    \n",
    "    start_date = date(int(start_year), int(start_month), int(start_day))\n",
    "    end_date = date(int(end_year), int(end_month), int(end_day))\n",
    "    return start_date, end_date "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_directory(directory):\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_folder_name_from_zip(FILENAME):\n",
    "    name = FILENAME.split('.')[0].split(\"/\")[-1]\n",
    "    return name\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "added_ids = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "ZuK9Y_DSL0cL"
   },
   "outputs": [],
   "source": [
    "# Keep data in english only\n",
    "def remove_other_langs(data):\n",
    "  data = data[data['lang'] == 'en'].reset_index(drop=True)\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_other_date(data, old_data, current_date, TEMP_FOLDER, FILES_NAMES, FOLDER_NAME):\n",
    "    if data.equals(old_data):\n",
    "        return\n",
    "    date_to_save = get_next_date(current_date, year)\n",
    "    print(date_to_save)\n",
    "    #directory = \"data\"\n",
    "    \n",
    "    #directory = \"/media/manikya_varshney/Backup/Yale Data/data\"\n",
    "    directory = '{}/{}'.format(TEMP_FOLDER, FOLDER_NAME)\n",
    "    \n",
    "    create_directory(directory)\n",
    "    combined = pd.concat([data, old_data]).drop_duplicates(keep=False)\n",
    "    \n",
    "    combined.to_csv(\"{}/h01-{}.csv\".format(directory, date_to_save))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Xwsew-o1MzvI"
   },
   "outputs": [],
   "source": [
    "# Keep specific date\n",
    "def remove_other_dates(data, date_to_analyse, TEMP_FOLDER, FILES_NAMES, FOLDER_NAME):\n",
    "  new_data = data[data['created_at'].str[:10] == date_to_analyse].reset_index(drop=True)\n",
    "  save_other_date(new_data, data, date_to_analyse, TEMP_FOLDER, FILES_NAMES, FOLDER_NAME)\n",
    "  return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "fnpiNeSksPHD"
   },
   "outputs": [],
   "source": [
    "# Creating the RT Column\n",
    "def create_rt_column(data):\n",
    "  data['RT'] = data['text'].str[:2]=='RT'\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "jDie85V6J4IJ"
   },
   "outputs": [],
   "source": [
    "# Parse CSV data\n",
    "def parse_data_from_file(filename, date_to_analyse, TEMP_FOLDER, FILES_NAMES, FOLDER_NAME):\n",
    "  print(\"reading file {}\".format(filename))\n",
    "  try:\n",
    "    data = pd.read_csv(filename, index_col = None, header=0, engine = 'python',encoding = \"utf-8\")\n",
    "    data = remove_other_langs(data)\n",
    "    data = remove_other_dates(data, date_to_analyse, TEMP_FOLDER, FILES_NAMES, FOLDER_NAME)\n",
    "    data = create_rt_column(data)\n",
    "    return data\n",
    "  except:\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "oksZ2gyHLTYM"
   },
   "outputs": [],
   "source": [
    "# Parse all files of the same date\n",
    "def parse_all_files(TEMP_FOLDER, FILES_NAMES, date_to_analyse, FOLDER_NAME):\n",
    "  files = os.listdir(os.path.join(TEMP_FOLDER, FOLDER_NAME))\n",
    "  #print(files)\n",
    "  new_data = []\n",
    "  for file in files:\n",
    "    if file.startswith(FILES_NAMES):\n",
    "      parsed_data = parse_data_from_file(os.path.join(TEMP_FOLDER, FOLDER_NAME, file), date_to_analyse, TEMP_FOLDER, FILES_NAMES, FOLDER_NAME)\n",
    "      new_data.append(parsed_data)\n",
    "  return pd.concat(new_data, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "d6dvi4iZuO7z"
   },
   "outputs": [],
   "source": [
    "def get_text_or_extended_text(data, added_ids):\n",
    "  data[\"FINAL_TEXT\"] = np.where(data['extended_tweet_full_text'].notnull(), data[\"extended_tweet_full_text\"], data[\"text\"])\n",
    "  id = data[\"id\"].to_dict()\n",
    "  added_ids.update(id)\n",
    "  return data[\"FINAL_TEXT\"]\n",
    "\n",
    "def get_quoted_text(data, added_ids):\n",
    "  data[\"FINAL_TEXT\"] = np.where(data['QT_full_text'].notnull(), data[\"QT_full_text\"], data[\"QT_text\"])\n",
    "  added_ids.update(data[\"id\"].to_dict())\n",
    "  added_ids.update(data[\"QT_id\"].to_dict())\n",
    "  return data[\"FINAL_TEXT\"]\n",
    "\n",
    "def get_text_or_full_text_rt(data, added_ids):\n",
    "  data[\"FINAL_TEXT\"] = np.where(data['RT_full_text'].notnull(), data[\"RT_full_text\"], data[\"RT_text\"])\n",
    "  added_ids.update(data[\"id\"].to_dict())\n",
    "  added_ids.update(data[\"RT_id\"].to_dict())\n",
    "  return data[\"FINAL_TEXT\"]\n",
    "\n",
    "def get_quote_rt_full(data, added_ids):\n",
    "  data[\"FINAL_TEXT\"] = get_text_or_full_text_rt(data, added_ids) + get_quoted_text(data, added_ids)\n",
    "  return data[\"FINAL_TEXT\"]\n",
    "\n",
    "\n",
    "\n",
    "def get_quote_rt(data, is_quote, is_rt):\n",
    "  print(\"get quoted text {} {}\".format(is_quote, is_rt))\n",
    "  data = data.loc[(data['is_quote_tweet'] == is_quote) & (data['RT'] == is_rt)]\n",
    "\n",
    "  if not is_quote and not is_rt:\n",
    "    added_ids.update(dict(zip(data[\"id\"], data[\"is_quote_tweet\"])))\n",
    "    data[\"FINAL_TEXT\"] = np.where(data['extended_tweet_full_text'].notnull(), data[\"extended_tweet_full_text\"], data[\"text\"])\n",
    "\n",
    "  if is_quote and not is_rt:\n",
    "    data[\"FINAL_TEXT\"] = np.where(data[\"QT_id\"].isin(added_ids.keys()), get_text_or_extended_text(data, added_ids), get_quoted_text(data, added_ids))\n",
    "\n",
    "  if not is_quote and is_rt:\n",
    "    data[\"FINAL_TEXT\"] = np.where(data[\"RT_id\"].isin(added_ids.keys()), added_ids.update(data['id'].to_dict()), get_text_or_full_text_rt(data, added_ids))\n",
    "    data = data[data[\"FINAL_TEXT\"].notna()]\n",
    "\n",
    "  if is_quote and is_rt:\n",
    "    data[\"FINAL_TEXT\"] = np.where((data[\"RT_id\"].isin(added_ids.keys())) & (data[\"QT_id\"].isin(added_ids.keys())), added_ids.update(data['id'].to_dict()), None)\n",
    "    data[\"FINAL_TEXT\"] = np.where((data[\"RT_id\"].isin(added_ids.keys())) & (~data[\"QT_id\"].isin(added_ids.keys())), get_quoted_text(data, added_ids), None)\n",
    "    data[\"FINAL_TEXT\"] = np.where((~data[\"RT_id\"].isin(added_ids.keys())) & (data[\"QT_id\"].isin(added_ids.keys())), get_text_or_full_text_rt(data, added_ids), None)\n",
    "    data[\"FINAL_TEXT\"] = np.where((~data[\"RT_id\"].isin(added_ids.keys())) & (~data[\"QT_id\"].isin(added_ids.keys())), get_quote_rt_full(data, added_ids), None)\n",
    "    data = data[data[\"FINAL_TEXT\"].notna()]\n",
    "\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_df(filename, TEMP_FOLDER, FILES_NAMES, date_to_analyse, FOLDER_NAME):\n",
    "    extract_zips(filename, TEMP_FOLDER)\n",
    "    data = parse_all_files(TEMP_FOLDER, FILES_NAMES, date_to_analyse, FOLDER_NAME)\n",
    "\n",
    "    non_quote_non_rt = get_quote_rt(data, False, False)\n",
    "    quote_non_rt = get_quote_rt(data, True, False)\n",
    "    quote_rt = get_quote_rt(data, True, True)\n",
    "    non_quote_rt = get_quote_rt(data, False, True)\n",
    "    \n",
    "    return non_quote_non_rt, quote_non_rt, quote_rt, non_quote_rt\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HCpVf_aD1lV-"
   },
   "source": [
    "1. Casing (Upper or lower case)\n",
    "2. Noise Removal (Removal of punctuation, white spaces, special characters, HTML tags)\n",
    "3. Tokenization (Tweets to tokens i.e. words seprated by spaces)\n",
    "4. Stopword Removal\n",
    "5. Text Normalization (Stemming and Lemmatization) bold text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "e212hr-q2LKf"
   },
   "outputs": [],
   "source": [
    "#Removing emojis\n",
    "def demoji(text):\n",
    "  emoji_pattern = re.compile(\"[\"\n",
    "  u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                u\"\\U00002702-\\U000027B0\"\n",
    "                u\"\\U000024C2-\\U0001F251\"\n",
    "                u\"\\U0001f926-\\U0001f937\"\n",
    "                u'\\U00010000-\\U0010ffff'\n",
    "                u\"\\u200d\"\n",
    "                u\"\\u2640-\\u2642\"\n",
    "                u\"\\u2600-\\u2B55\"\n",
    "                u\"\\u23cf\"\n",
    "                u\"\\u23e9\"\n",
    "                u\"\\u231a\"\n",
    "                u\"\\u3030\"\n",
    "                u\"\\ufe0f\"\n",
    "                        \"]+\", flags=re.UNICODE)\n",
    "  return(emoji_pattern.sub(r'', text.decode('utf-8')))\n",
    "\n",
    "#POSTags\n",
    "def get_wordnet_pos(word):\n",
    "  \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "  tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "  tag_dict = {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV}\n",
    "  return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "def apply_stemming(row):\n",
    "  row_list = row[\"FINAL_TEXT\"]\n",
    "  stemmed_list = [ps.stem(word) for word in row_list]\n",
    "  return (stemmed_list)\n",
    "\n",
    "#Remove URLs, user@, punctutions\n",
    "def df_cleaning(data_):\n",
    "  punc = string.punctuation\n",
    "  data_[\"FINAL_TEXT\"] = data_[\"FINAL_TEXT\"].astype(str).str.replace(r\"http\\S+| www\\S+| https\\S+| \\S+\\.com\\S+| \\S+\\.com| \\@[\\w]+\", \"\", regex=True)\n",
    "\n",
    "  # ##################-------Punctutions-------##################\n",
    "  data_[\"FINAL_TEXT\"] = data_[\"FINAL_TEXT\"].astype(str).str.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    " \n",
    "  # ##################-------More Cleaning-------##################\n",
    "  data_[\"FINAL_TEXT\"] = data_[\"FINAL_TEXT\"].astype(str).str.replace(\"/[^a-zA-Z0-9 ]/g\", \"\", regex=True).str.replace(\"\\n\",\" \", regex=True).str.replace(\"—\",\" \", regex=True).str.strip(\"“\").str.strip(\"”\").str.strip(\"’\").str.lstrip(\" \").str.rstrip(\" \")\n",
    "  \n",
    "  # ##################-------Emojis-------##################\n",
    "#   data_[\"FINAL_TEXT\"] = data_[\"FINAL_TEXT\"].astype(str).apply(lambda x:demoji(x.encode('utf8')))\n",
    "  data_[\"FINAL_TEXT\"] = data_[\"FINAL_TEXT\"].astype(str).apply(lambda x: x.encode('ascii', 'ignore').decode('ascii'))\n",
    "  # ##################-------Tokenizing-------##################\n",
    "  data_[\"FINAL_TEXT\"] = data_[\"FINAL_TEXT\"].astype(str)\n",
    "  data_[\"FINAL_TEXT\"] = data_[\"FINAL_TEXT\"].apply(nltk.word_tokenize)\n",
    "#   data_[\"FINAL_TEXT\"] = data_.apply(lambda row: nltk.word_tokenize(row[\"FINAL_TEXT\"]), axis=1)\n",
    "\n",
    "  # ##################-------Lower characters---------##################\n",
    "  data_[\"FINAL_TEXT\"] = data_[\"FINAL_TEXT\"].apply(lambda x: [word.lower() for word in x])\n",
    "\n",
    "  # ##################-------Remove punctuations-------##################\n",
    "  data_[\"FINAL_TEXT\"] = data_[\"FINAL_TEXT\"].apply(lambda x: [word for word in x if word not in punc])\n",
    "\n",
    "  # ##################-------Removing stopwords-------##################\n",
    "#   data_[\"FINAL_TEXT\"] = data_[\"FINAL_TEXT\"].astype(str).apply(lambda x: [word for word in x if word not in stop_words])\n",
    "  \n",
    "  # data_[\"FINAL_TEXT\"] = data_[\"FINAL_TEXT\"].astype(str).apply(lambda x: [word for word in x if word not in stop_words])\n",
    "  data_[\"FINAL_TEXT\"] = data_[\"FINAL_TEXT\"].apply(lambda words: ' '.join(word for word in words if word not in stop_words))\n",
    " \n",
    "  \n",
    "  # ##################-------Stemming-------##################\n",
    "  ps = PorterStemmer()\n",
    "#   data_[\"FINAL_TEXT\"] = data_[\"FINAL_TEXT\"].apply(lambda x: [ps.stem(y) for y in x])\n",
    "  data_[\"FINAL_TEXT\"] = data_[\"FINAL_TEXT\"].apply(lambda x: [ps.stem(y) for y in x.split(' ')])\n",
    "#   data_[\"FINAL_TEXT\"] = data_.apply(apply_stemming, axis = 1)\n",
    "#   data_[\"FINAL_TEXT\"] = data_.apply(ps.stem)\n",
    "\n",
    "#   print(data_[\"FINAL_TEXT\"], \"from here\")\n",
    "\n",
    "\n",
    "  # ##################-------Lemmatizing-------##################\n",
    "#   lemmatizer = WordNetLemmatizer()\n",
    "#   data[\"FINAL_TEXT\"] = data[\"FINAL_TEXT\"].astype(str).apply(lambda x: [lemmatizer.lemmatize(y, get_wordnet_pos(y)) for y in x])\n",
    "  \n",
    "  # ##################-------Joining the lemmetized tokens to form string-------##################\n",
    "  data_[\"FINAL_TEXT\"] = data_[\"FINAL_TEXT\"].apply(lambda x: ' '.join(x))\n",
    " \n",
    "  # ##################-------Remove punctuations-------##################\n",
    "#   data_[\"FINAL_TEXT\"] = data_[\"FINAL_TEXT\"].astype(str).str.translate(str.maketrans(\"\", \"\", string.punctuation)).str.replace(\"’\", \" \").str.replace(\"“\", \" \").str.replace(\"”\", \" \")\n",
    "\n",
    "#   final_df[\"FINAL_TEXT\"] = data_[\"FINAL_TEXT\"]\n",
    "#   final_df = data_\n",
    "  return data_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "ROcVKfprPobe"
   },
   "outputs": [],
   "source": [
    "def split_dataframe(df, nums): \n",
    "    chunks = list()\n",
    "    num_chunks = nums\n",
    "    chunk_size = len(df) // nums\n",
    "    for i in range(num_chunks):\n",
    "        chunks.append(df[i*chunk_size:(i+1)*chunk_size])\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data_and_save(data_,filename, year):\n",
    "    start = time.time()\n",
    "    num_cores = mp.cpu_count()\n",
    "    pool = Pool(num_cores)\n",
    "    df = split_dataframe(data_, num_cores)\n",
    "    data1 = pd.concat(pool.map(df_cleaning, df))\n",
    "    data = data1.copy()\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    end = time.time()\n",
    "    #directory = \"output_data/{}-{}\".format(date_to_analyse, year)\n",
    "    \n",
    "#     directory = \"/media/manikya_varshney/Backup/Yale Data/output_data/{}-{}\".format(date_to_analyse, year)\n",
    "    directory = \"output_data/{}-{}\".format(date_to_analyse, year)\n",
    "    \n",
    "    create_directory(directory)\n",
    "#     data1.to_hdf(\"{}/{}.h5\".format(directory, filename), key='stage', mode='w')\n",
    "    \n",
    "    data1.to_csv(\"{}/{}.csv\".format(directory, filename))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_all_data_and_save(data_, year):\n",
    "    non_quote_non_rt, quote_non_rt, quote_rt, non_quote_rt = data_\n",
    "    non_quote_non_rt = clean_data_and_save(non_quote_non_rt, 'non_quote_non_rt', year)\n",
    "    quote_non_rt = clean_data_and_save(quote_non_rt, 'quote_non_rt', year)    \n",
    "    non_quote_rt = clean_data_and_save(non_quote_rt, 'non_quote_rt', year)\n",
    "    quote_rt = clean_data_and_save(quote_rt, 'quote_rt', year)\n",
    "    return non_quote_non_rt, quote_non_rt, quote_rt, non_quote_rt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_all_data(data_, year):\n",
    "    non_quote_non_rt, quote_non_rt, quote_rt, non_quote_rt = data_\n",
    "    combined = pd.concat([non_quote_non_rt, quote_non_rt, quote_rt, non_quote_rt])\n",
    "    #directory = \"output_data/{}-{}\".format(date_to_analyse, year)\n",
    "    \n",
    "#     directory = \"/media/manikya_varshney/Backup/Yale Data/output_data/{}-{}\".format(date_to_analyse, year)\n",
    "    directory = \"output_data/{}-{}\".format(date_to_analyse, year)\n",
    "    \n",
    "    create_directory(directory)\n",
    "#     combined.to_hdf('{}/combined_{}.csv'.format(directory, date_to_analyse), index = False, key='stage', mode='w')    \n",
    "    combined.to_csv('{}/combined_{}.csv'.format(directory, date_to_analyse), index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_clean(FILENAME, TEMP_FOLDER, FILES_NAMES, date_to_analyse, FOLDER_NAME, year):\n",
    "    \n",
    "    all_data = separate_df(FILENAME, TEMP_FOLDER, FILES_NAMES, date_to_analyse, FOLDER_NAME)\n",
    "    non_quote_non_rt, quote_non_rt, quote_rt, non_quote_rt = clean_all_data_and_save(all_data, year)\n",
    "    combine_all_data((non_quote_non_rt, quote_non_rt, quote_rt, non_quote_rt), year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "Axem73RZAAkA"
   },
   "outputs": [],
   "source": [
    "# Constants\n",
    "FILENAME = \"h01-20201001-20201008.zip\"\n",
    "\n",
    "# FILENAME = \"/media/manikya_varshney/Backup/Yale Data/h01-20200818-20200824.zip\"\n",
    "\n",
    "TEMP_FOLDER = \"tmp\"\n",
    "\n",
    "# TEMP_FOLDER = \"/media/manikya_varshney/Backup/Yale Data/tmp\"\n",
    "\n",
    "FOLDER_NAME = get_folder_name_from_zip(FILENAME)\n",
    "\n",
    "FILES_NAMES = create_filename(FILENAME, TEMP_FOLDER)\n",
    "date_to_analyse, year = strip_date(FILES_NAMES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading file tmp/h01-20201001-173324.csv\n",
      "reading file tmp/h01-20201001-090915.csv\n",
      "reading file tmp/h01-20201001-073456.csv\n",
      "reading file tmp/h01-20201001-215958.csv\n",
      "20201002\n",
      "reading file tmp/h01-20201001-111205.csv\n",
      "reading file tmp/h01-20201001-080246.csv\n",
      "reading file tmp/h01-20201001-095059.csv\n",
      "reading file tmp/h01-20201001-093158.csv\n",
      "reading file tmp/h01-20201001-113955.csv\n",
      "reading file tmp/h01-20201001-175346.csv\n",
      "reading file tmp/h01-20201001-145520.csv\n",
      "reading file tmp/h01-20201001-160415.csv\n",
      "reading file tmp/h01-20201001-012054.csv\n",
      "reading file tmp/h01-20201001-103603.csv\n",
      "reading file tmp/h01-20201001-153034.csv\n",
      "reading file tmp/h01-20201001-105931.csv\n",
      "reading file tmp/h01-20201001-162028.csv\n",
      "reading file tmp/h01-20201001-203308.csv\n",
      "20201002\n",
      "reading file tmp/h01-20201001-225456.csv\n",
      "reading file tmp/h01-20201001-194740.csv\n",
      "reading file tmp/h01-20201001-090136.csv\n",
      "reading file tmp/h01-20201001-082240.csv\n",
      "reading file tmp/h01-20201001-020150.csv\n",
      "reading file tmp/h01-20201001-235724.csv\n",
      "20201002\n",
      "reading file tmp/h01-20201001-212402.csv\n",
      "20201002\n",
      "reading file tmp/h01-20201001-095506.csv\n",
      "reading file tmp/h01-20201001-071455.csv\n",
      "reading file tmp/h01-20201001-113549.csv\n",
      "reading file tmp/h01-20201001-162549.csv\n",
      "reading file tmp/h01-20201001-114811.csv\n",
      "reading file tmp/h01-20201001-084225.csv\n",
      "reading file tmp/h01-20201001-090522.csv\n",
      "reading file tmp/h01-20201001-174841.csv\n",
      "reading file tmp/h01-20201001-132800.csv\n",
      "reading file tmp/h01-20201001-134530.csv\n",
      "reading file tmp/h01-20201001-133934.csv\n",
      "reading file tmp/h01-20201001-111559.csv\n",
      "reading file tmp/h01-20201001-092042.csv\n",
      "reading file tmp/h01-20201001-172820.csv\n",
      "reading file tmp/h01-20201001-184422.csv\n",
      "reading file tmp/h01-20201001-160940.csv\n",
      "reading file tmp/h01-20201001-151551.csv\n",
      "reading file tmp/h01-20201001-170212.csv\n",
      "reading file tmp/h01-20201001-215424.csv\n",
      "20201002\n",
      "reading file tmp/h01-20201001-154832.csv\n",
      "reading file tmp/h01-20201001-083830.csv\n",
      "reading file tmp/h01-20201001-095920.csv\n",
      "reading file tmp/h01-20201001-221623.csv\n",
      "20201002\n",
      "reading file tmp/h01-20201001-010452.csv\n",
      "reading file tmp/h01-20201001-230712.csv\n",
      "20201002\n",
      "reading file tmp/h01-20201001-164112.csv\n",
      "reading file tmp/h01-20201001-155913.csv\n",
      "reading file tmp/h01-20201001-165146.csv\n",
      "reading file tmp/h01-20201001-172313.csv\n",
      "reading file tmp/h01-20201001-205123.csv\n",
      "20201002\n",
      "reading file tmp/h01-20201001-214204.csv\n",
      "20201002\n",
      "reading file tmp/h01-20201001-074548.csv\n",
      "reading file tmp/h01-20201001-093927.csv\n",
      "reading file tmp/h01-20201001-124747.csv\n",
      "reading file tmp/h01-20201001-000627.csv\n",
      "reading file tmp/h01-20201001-155356.csv\n",
      "reading file tmp/h01-20201001-081846.csv\n",
      "reading file tmp/h01-20201001-102813.csv\n",
      "reading file tmp/h01-20201001-130453.csv\n",
      "reading file tmp/h01-20201001-124208.csv\n",
      "reading file tmp/h01-20201001-112751.csv\n",
      "reading file tmp/h01-20201001-214818.csv\n",
      "20201002\n",
      "reading file tmp/h01-20201001-115215.csv\n",
      "reading file tmp/h01-20201001-081448.csv\n",
      "reading file tmp/h01-20201001-180742.csv\n",
      "reading file tmp/h01-20201001-193925.csv\n",
      "reading file tmp/h01-20201001-231904.csv\n",
      "20201002\n",
      "reading file tmp/h01-20201001-112354.csv\n",
      "reading file tmp/h01-20201001-085004.csv\n",
      "reading file tmp/h01-20201001-221050.csv\n",
      "20201002\n",
      "reading file tmp/h01-20201001-075042.csv\n",
      "reading file tmp/h01-20201001-072922.csv\n",
      "reading file tmp/h01-20201001-181458.csv\n",
      "reading file tmp/h01-20201001-092431.csv\n",
      "reading file tmp/h01-20201001-093539.csv\n",
      "reading file tmp/h01-20201001-131629.csv\n",
      "reading file tmp/h01-20201001-164628.csv\n",
      "reading file tmp/h01-20201001-140952.csv\n",
      "reading file tmp/h01-20201001-125927.csv\n",
      "reading file tmp/h01-20201001-191500.csv\n",
      "reading file tmp/h01-20201001-210717.csv\n",
      "20201002\n",
      "reading file tmp/h01-20201001-171806.csv\n",
      "reading file tmp/h01-20201001-224101.csv\n",
      "20201002\n",
      "reading file tmp/h01-20201001-075447.csv\n",
      "reading file tmp/h01-20201001-211812.csv\n",
      "20201002\n",
      "reading file tmp/h01-20201001-142216.csv\n",
      "reading file tmp/h01-20201001-004928.csv\n",
      "reading file tmp/h01-20201001-182218.csv\n",
      "reading file tmp/h01-20201001-014534.csv\n",
      "reading file tmp/h01-20201001-142835.csv\n",
      "reading file tmp/h01-20201001-201208.csv\n",
      "20201002\n",
      "reading file tmp/h01-20201001-065910.csv\n",
      "reading file tmp/h01-20201001-180036.csv\n",
      "reading file tmp/h01-20201001-211230.csv\n",
      "20201002\n",
      "reading file tmp/h01-20201001-105148.csv\n",
      "reading file tmp/h01-20201001-013716.csv\n",
      "reading file tmp/h01-20201001-195556.csv\n",
      "reading file tmp/h01-20201001-233103.csv\n",
      "20201002\n",
      "reading file tmp/h01-20201001-122101.csv\n",
      "reading file tmp/h01-20201001-082639.csv\n",
      "reading file tmp/h01-20201001-002005.csv\n",
      "reading file tmp/h01-20201001-091309.csv\n",
      "reading file tmp/h01-20201001-074018.csv\n",
      "reading file tmp/h01-20201001-001308.csv\n",
      "reading file tmp/h01-20201001-115619.csv\n",
      "reading file tmp/h01-20201001-203941.csv\n",
      "20201002\n",
      "reading file tmp/h01-20201001-163604.csv\n",
      "reading file tmp/h01-20201001-011245.csv\n",
      "reading file tmp/h01-20201001-094703.csv\n",
      "reading file tmp/h01-20201001-222420.csv\n",
      "20201002\n",
      "reading file tmp/h01-20201001-185155.csv\n",
      "reading file tmp/h01-20201001-085357.csv\n",
      "reading file tmp/h01-20201001-081051.csv\n",
      "reading file tmp/h01-20201001-094312.csv\n",
      "reading file tmp/h01-20201001-170731.csv\n",
      "reading file tmp/h01-20201001-085753.csv\n",
      "reading file tmp/h01-20201001-140341.csv\n",
      "reading file tmp/h01-20201001-000017.csv\n",
      "reading file tmp/h01-20201001-123631.csv\n",
      "reading file tmp/h01-20201001-204540.csv\n",
      "20201002\n",
      "reading file tmp/h01-20201001-213000.csv\n",
      "20201002\n",
      "reading file tmp/h01-20201001-015350.csv\n",
      "reading file tmp/h01-20201001-193109.csv\n",
      "reading file tmp/h01-20201001-174341.csv\n",
      "reading file tmp/h01-20201001-135137.csv\n",
      "reading file tmp/h01-20201001-091656.csv\n",
      "reading file tmp/h01-20201001-102003.csv\n",
      "reading file tmp/h01-20201001-092821.csv\n",
      "reading file tmp/h01-20201001-070913.csv\n",
      "reading file tmp/h01-20201001-183657.csv\n",
      "reading file tmp/h01-20201001-213559.csv\n",
      "20201002\n",
      "reading file tmp/h01-20201001-154325.csv\n",
      "reading file tmp/h01-20201001-150157.csv\n",
      "reading file tmp/h01-20201001-122516.csv\n",
      "reading file tmp/h01-20201001-133341.csv\n",
      "reading file tmp/h01-20201001-143459.csv\n",
      "reading file tmp/h01-20201001-165703.csv\n",
      "reading file tmp/h01-20201001-173834.csv\n",
      "reading file tmp/h01-20201001-070327.csv\n",
      "reading file tmp/h01-20201001-205700.csv\n",
      "20201002\n",
      "reading file tmp/h01-20201001-075854.csv\n",
      "reading file tmp/h01-20201001-120417.csv\n",
      "reading file tmp/h01-20201001-202632.csv\n",
      "20201002\n",
      "reading file tmp/h01-20201001-182934.csv\n",
      "reading file tmp/h01-20201001-083035.csv\n",
      "reading file tmp/h01-20201001-161503.csv\n",
      "reading file tmp/h01-20201001-125340.csv\n",
      "reading file tmp/h01-20201001-171249.csv\n",
      "reading file tmp/h01-20201001-120827.csv\n",
      "reading file tmp/h01-20201001-224838.csv\n",
      "20201002\n",
      "reading file tmp/h01-20201001-080649.csv\n",
      "reading file tmp/h01-20201001-153820.csv\n",
      "reading file tmp/h01-20201001-114404.csv\n",
      "reading file tmp/h01-20201001-111959.csv\n",
      "reading file tmp/h01-20201001-192305.csv\n",
      "reading file tmp/h01-20201001-005722.csv\n",
      "reading file tmp/h01-20201001-163100.csv\n",
      "reading file tmp/h01-20201001-012909.csv\n",
      "reading file tmp/h01-20201001-131040.csv\n",
      "reading file tmp/h01-20201001-210220.csv\n",
      "20201002\n",
      "reading file tmp/h01-20201001-083437.csv\n",
      "reading file tmp/h01-20201001-121243.csv\n",
      "reading file tmp/h01-20201001-084614.csv\n",
      "reading file tmp/h01-20201001-004148.csv\n",
      "reading file tmp/h01-20201001-230057.csv\n",
      "20201002\n",
      "reading file tmp/h01-20201001-220522.csv\n",
      "20201002\n",
      "reading file tmp/h01-20201001-104404.csv\n",
      "reading file tmp/h01-20201001-113145.csv\n",
      "reading file tmp/h01-20201001-121652.csv\n",
      "reading file tmp/h01-20201001-123045.csv\n",
      "reading file tmp/h01-20201001-190714.csv\n",
      "reading file tmp/h01-20201001-020941.csv\n",
      "reading file tmp/h01-20201001-223228.csv\n",
      "20201002\n",
      "reading file tmp/h01-20201001-144141.csv\n",
      "reading file tmp/h01-20201001-144828.csv\n",
      "reading file tmp/h01-20201001-135747.csv\n",
      "reading file tmp/h01-20201001-185938.csv\n",
      "reading file tmp/h01-20201001-132214.csv\n",
      "reading file tmp/h01-20201001-150903.csv\n",
      "reading file tmp/h01-20201001-003426.csv\n",
      "reading file tmp/h01-20201001-141601.csv\n",
      "reading file tmp/h01-20201001-002710.csv\n",
      "reading file tmp/h01-20201001-071926.csv\n",
      "reading file tmp/h01-20201001-201934.csv\n",
      "20201002\n",
      "reading file tmp/h01-20201001-120017.csv\n",
      "reading file tmp/h01-20201001-200357.csv\n",
      "20201002\n",
      "reading file tmp/h01-20201001-110711.csv\n",
      "reading file tmp/h01-20201001-152244.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading file tmp/h01-20201001-234347.csv\n",
      "reading file tmp/h01-20201001-072341.csv\n",
      "get quoted text False False\n",
      "get quoted text True False\n",
      "get quoted text True True\n",
      "get quoted text False True\n",
      "reading file tmp/h01-20201002-082516.csv\n",
      "reading file tmp/h01-20201002-012105.csv\n",
      "reading file tmp/h01-20201002-025756.csv\n",
      "reading file tmp/h01-20201002-051216.csv\n",
      "reading file tmp/h01-20201002-181553.csv\n",
      "reading file tmp/h01-20201002-202806.csv\n",
      "20201003\n",
      "reading file tmp/h01-20201002-135035.csv\n",
      "reading file tmp/h01-20201002-010316.csv\n",
      "reading file tmp/h01-20201002-093553.csv\n",
      "reading file tmp/h01-20201002-234740.csv\n",
      "20201003\n",
      "reading file tmp/h01-20201002-072949.csv\n",
      "reading file tmp/h01-20201002-031131.csv\n",
      "reading file tmp/h01-20201002-131130.csv\n",
      "reading file tmp/h01-20201002-131656.csv\n",
      "reading file tmp/h01-20201002-190932.csv\n",
      "reading file tmp/h01-20201002-184012.csv\n",
      "reading file tmp/h01-20201002-084748.csv\n",
      "reading file tmp/h01-20201002-101024.csv\n",
      "reading file tmp/h01-20201002-171058.csv\n",
      "reading file tmp/h01-20201002-060948.csv\n",
      "reading file tmp/h01-20201002-005652.csv\n",
      "reading file tmp/h01-20201002-013742.csv\n",
      "reading file tmp/h01-20201002-211304.csv\n",
      "20201003\n",
      "reading file tmp/h01-20201002-040124.csv\n",
      "reading file tmp/h01-20201002-183804.csv\n",
      "reading file tmp/h01-20201002-032639.csv\n",
      "reading file tmp/h01-20201002-191421.csv\n",
      "reading file tmp/h01-20201002-233010.csv\n",
      "20201003\n",
      "reading file tmp/h01-20201002-003802.csv\n",
      "reading file tmp/h01-20201002-010843.csv\n",
      "reading file tmp/h01-20201002-135619.csv\n",
      "reading file tmp/h01-20201002-195336.csv\n",
      "reading file tmp/h01-20201002-050136.csv\n",
      "reading file tmp/h01-20201002-092539.csv\n",
      "reading file tmp/h01-20201002-042042.csv\n",
      "reading file tmp/h01-20201002-112107.csv\n",
      "reading file tmp/h01-20201002-083636.csv\n",
      "reading file tmp/h01-20201002-013231.csv\n",
      "reading file tmp/h01-20201002-012315.csv\n",
      "reading file tmp/h01-20201002-033456.csv\n",
      "reading file tmp/h01-20201002-194004.csv\n",
      "reading file tmp/h01-20201002-183550.csv\n",
      "reading file tmp/h01-20201002-154022.csv\n",
      "reading file tmp/h01-20201002-102516.csv\n",
      "reading file tmp/h01-20201002-141407.csv\n",
      "reading file tmp/h01-20201002-164321.csv\n",
      "reading file tmp/h01-20201002-130610.csv\n",
      "reading file tmp/h01-20201002-185530.csv\n",
      "reading file tmp/h01-20201002-224346.csv\n",
      "20201003\n",
      "reading file tmp/h01-20201002-061846.csv\n",
      "reading file tmp/h01-20201002-195053.csv\n",
      "reading file tmp/h01-20201002-021904.csv\n",
      "reading file tmp/h01-20201002-013008.csv\n",
      "reading file tmp/h01-20201002-174911.csv\n",
      "reading file tmp/h01-20201002-200033.csv\n",
      "20201003\n",
      "reading file tmp/h01-20201002-125551.csv\n",
      "reading file tmp/h01-20201002-043039.csv\n",
      "reading file tmp/h01-20201002-191157.csv\n",
      "reading file tmp/h01-20201002-002918.csv\n",
      "reading file tmp/h01-20201002-165015.csv\n",
      "reading file tmp/h01-20201002-144540.csv\n",
      "reading file tmp/h01-20201002-070815.csv\n",
      "reading file tmp/h01-20201002-015901.csv\n",
      "reading file tmp/h01-20201002-085321.csv\n",
      "reading file tmp/h01-20201002-105535.csv\n",
      "reading file tmp/h01-20201002-045111.csv\n",
      "reading file tmp/h01-20201002-133905.csv\n",
      "reading file tmp/h01-20201002-180926.csv\n",
      "reading file tmp/h01-20201002-115242.csv\n",
      "reading file tmp/h01-20201002-133326.csv\n",
      "reading file tmp/h01-20201002-172420.csv\n",
      "reading file tmp/h01-20201002-114202.csv\n",
      "reading file tmp/h01-20201002-220150.csv\n",
      "20201003\n",
      "reading file tmp/h01-20201002-112614.csv\n",
      "reading file tmp/h01-20201002-014926.csv\n",
      "reading file tmp/h01-20201002-011043.csv\n",
      "reading file tmp/h01-20201002-113640.csv\n",
      "reading file tmp/h01-20201002-003335.csv\n",
      "reading file tmp/h01-20201002-062734.csv\n",
      "reading file tmp/h01-20201002-021433.csv\n",
      "reading file tmp/h01-20201002-142624.csv\n",
      "reading file tmp/h01-20201002-150546.csv\n",
      "reading file tmp/h01-20201002-222424.csv\n",
      "20201003\n",
      "reading file tmp/h01-20201002-060042.csv\n",
      "reading file tmp/h01-20201002-140806.csv\n",
      "reading file tmp/h01-20201002-125052.csv\n",
      "reading file tmp/h01-20201002-011450.csv\n",
      "reading file tmp/h01-20201002-214553.csv\n",
      "20201003\n",
      "reading file tmp/h01-20201002-181916.csv\n",
      "reading file tmp/h01-20201002-174237.csv\n",
      "reading file tmp/h01-20201002-123151.csv\n",
      "reading file tmp/h01-20201002-105035.csv\n",
      "reading file tmp/h01-20201002-113125.csv\n",
      "reading file tmp/h01-20201002-193719.csv\n",
      "reading file tmp/h01-20201002-223704.csv\n",
      "20201003\n",
      "reading file tmp/h01-20201002-115756.csv\n",
      "reading file tmp/h01-20201002-200452.csv\n",
      "20201003\n",
      "reading file tmp/h01-20201002-063616.csv\n",
      "reading file tmp/h01-20201002-165717.csv\n",
      "reading file tmp/h01-20201002-111055.csv\n",
      "reading file tmp/h01-20201002-011653.csv\n",
      "reading file tmp/h01-20201002-110039.csv\n",
      "reading file tmp/h01-20201002-181734.csv\n",
      "reading file tmp/h01-20201002-183335.csv\n",
      "reading file tmp/h01-20201002-021019.csv\n",
      "reading file tmp/h01-20201002-191916.csv\n",
      "reading file tmp/h01-20201002-123626.csv\n",
      "reading file tmp/h01-20201002-190003.csv\n",
      "reading file tmp/h01-20201002-195621.csv\n",
      "20201003\n",
      "reading file tmp/h01-20201002-083057.csv\n",
      "reading file tmp/h01-20201002-022855.csv\n",
      "reading file tmp/h01-20201002-090942.csv\n",
      "reading file tmp/h01-20201002-011246.csv\n",
      "reading file tmp/h01-20201002-161548.csv\n",
      "reading file tmp/h01-20201002-205710.csv\n",
      "20201003\n",
      "reading file tmp/h01-20201002-023948.csv\n",
      "reading file tmp/h01-20201002-093047.csv\n",
      "reading file tmp/h01-20201002-101519.csv\n",
      "reading file tmp/h01-20201002-011859.csv\n",
      "reading file tmp/h01-20201002-104529.csv\n",
      "reading file tmp/h01-20201002-151931.csv\n",
      "reading file tmp/h01-20201002-054308.csv\n",
      "reading file tmp/h01-20201002-185747.csv\n",
      "reading file tmp/h01-20201002-095540.csv\n",
      "reading file tmp/h01-20201002-005812.csv\n",
      "reading file tmp/h01-20201002-120310.csv\n",
      "reading file tmp/h01-20201002-204236.csv\n",
      "20201003\n",
      "reading file tmp/h01-20201002-160132.csv\n",
      "reading file tmp/h01-20201002-122329.csv\n",
      "reading file tmp/h01-20201002-230540.csv\n",
      "20201003\n",
      "reading file tmp/h01-20201002-154718.csv\n",
      "reading file tmp/h01-20201002-163639.csv\n",
      "reading file tmp/h01-20201002-182439.csv\n",
      "reading file tmp/h01-20201002-194529.csv\n",
      "reading file tmp/h01-20201002-004648.csv\n",
      "reading file tmp/h01-20201002-173615.csv\n",
      "reading file tmp/h01-20201002-064433.csv\n",
      "reading file tmp/h01-20201002-102013.csv\n",
      "reading file tmp/h01-20201002-103523.csv\n",
      "reading file tmp/h01-20201002-010205.csv\n",
      "reading file tmp/h01-20201002-184855.csv\n",
      "reading file tmp/h01-20201002-212119.csv\n",
      "20201003\n",
      "reading file tmp/h01-20201002-091500.csv\n",
      "reading file tmp/h01-20201002-182232.csv\n",
      "reading file tmp/h01-20201002-200909.csv\n",
      "20201003\n",
      "reading file tmp/h01-20201002-153325.csv\n",
      "reading file tmp/h01-20201002-110546.csv\n",
      "reading file tmp/h01-20201002-160843.csv\n",
      "reading file tmp/h01-20201002-152629.csv\n",
      "reading file tmp/h01-20201002-130053.csv\n",
      "reading file tmp/h01-20201002-014318.csv\n",
      "reading file tmp/h01-20201002-180237.csv\n",
      "reading file tmp/h01-20201002-220950.csv\n",
      "20201003\n",
      "reading file tmp/h01-20201002-124557.csv\n",
      "reading file tmp/h01-20201002-143241.csv\n",
      "reading file tmp/h01-20201002-072249.csv\n",
      "reading file tmp/h01-20201002-140210.csv\n",
      "reading file tmp/h01-20201002-080752.csv\n",
      "reading file tmp/h01-20201002-055147.csv\n",
      "reading file tmp/h01-20201002-041052.csv\n",
      "reading file tmp/h01-20201002-023416.csv\n",
      "reading file tmp/h01-20201002-092020.csv\n",
      "reading file tmp/h01-20201002-100030.csv\n",
      "reading file tmp/h01-20201002-114723.csv\n",
      "reading file tmp/h01-20201002-213745.csv\n",
      "20201003\n",
      "reading file tmp/h01-20201002-044048.csv\n",
      "reading file tmp/h01-20201002-025139.csv\n",
      "reading file tmp/h01-20201002-171743.csv\n",
      "reading file tmp/h01-20201002-073632.csv\n",
      "reading file tmp/h01-20201002-191648.csv\n",
      "reading file tmp/h01-20201002-235542.csv\n",
      "20201003\n",
      "reading file tmp/h01-20201002-142010.csv\n",
      "reading file tmp/h01-20201002-183116.csv\n",
      "reading file tmp/h01-20201002-081339.csv\n",
      "reading file tmp/h01-20201002-012747.csv\n",
      "reading file tmp/h01-20201002-010636.csv\n",
      "reading file tmp/h01-20201002-192147.csv\n",
      "reading file tmp/h01-20201002-065242.csv\n",
      "reading file tmp/h01-20201002-225807.csv\n",
      "reading file tmp/h01-20201002-094053.csv\n",
      "reading file tmp/h01-20201002-173019.csv\n",
      "reading file tmp/h01-20201002-080152.csv\n",
      "reading file tmp/h01-20201002-013504.csv\n",
      "reading file tmp/h01-20201002-184644.csv\n",
      "reading file tmp/h01-20201002-204954.csv\n",
      "20201003\n",
      "reading file tmp/h01-20201002-202100.csv\n",
      "20201003\n",
      "reading file tmp/h01-20201002-192420.csv\n",
      "reading file tmp/h01-20201002-232148.csv\n",
      "20201003\n",
      "reading file tmp/h01-20201002-010423.csv\n",
      "reading file tmp/h01-20201002-015229.csv\n",
      "reading file tmp/h01-20201002-210458.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20201003\n",
      "reading file tmp/h01-20201002-121403.csv\n",
      "reading file tmp/h01-20201002-162249.csv\n",
      "reading file tmp/h01-20201002-233900.csv\n",
      "20201003\n",
      "reading file tmp/h01-20201002-155426.csv\n",
      "reading file tmp/h01-20201002-194246.csv\n",
      "reading file tmp/h01-20201002-212931.csv\n",
      "20201003\n",
      "reading file tmp/h01-20201002-143913.csv\n",
      "reading file tmp/h01-20201002-231332.csv\n",
      "20201003\n",
      "reading file tmp/h01-20201002-190222.csv\n",
      "reading file tmp/h01-20201002-090418.csv\n",
      "reading file tmp/h01-20201002-001301.csv\n",
      "reading file tmp/h01-20201002-145219.csv\n",
      "reading file tmp/h01-20201002-081930.csv\n",
      "reading file tmp/h01-20201002-122731.csv\n",
      "reading file tmp/h01-20201002-070038.csv\n",
      "reading file tmp/h01-20201002-132223.csv\n",
      "reading file tmp/h01-20201002-151237.csv\n",
      "reading file tmp/h01-20201002-010531.csv\n",
      "reading file tmp/h01-20201002-182053.csv\n",
      "reading file tmp/h01-20201002-215354.csv\n",
      "20201003\n",
      "reading file tmp/h01-20201002-132753.csv\n",
      "reading file tmp/h01-20201002-182655.csv\n",
      "reading file tmp/h01-20201002-175550.csv\n",
      "reading file tmp/h01-20201002-190709.csv\n",
      "reading file tmp/h01-20201002-005511.csv\n",
      "reading file tmp/h01-20201002-020236.csv\n",
      "reading file tmp/h01-20201002-223036.csv\n",
      "20201003\n",
      "reading file tmp/h01-20201002-134451.csv\n",
      "reading file tmp/h01-20201002-010042.csv\n",
      "reading file tmp/h01-20201002-193439.csv\n",
      "reading file tmp/h01-20201002-035215.csv\n",
      "reading file tmp/h01-20201002-145902.csv\n",
      "reading file tmp/h01-20201002-192928.csv\n",
      "reading file tmp/h01-20201002-034321.csv\n",
      "reading file tmp/h01-20201002-185105.csv\n",
      "reading file tmp/h01-20201002-022351.csv\n",
      "reading file tmp/h01-20201002-193203.csv\n",
      "reading file tmp/h01-20201002-094550.csv\n",
      "reading file tmp/h01-20201002-192653.csv\n",
      "reading file tmp/h01-20201002-085851.csv\n",
      "reading file tmp/h01-20201002-005937.csv\n",
      "reading file tmp/h01-20201002-075552.csv\n",
      "reading file tmp/h01-20201002-014621.csv\n",
      "reading file tmp/h01-20201002-182904.csv\n",
      "reading file tmp/h01-20201002-014027.csv\n",
      "reading file tmp/h01-20201002-095049.csv\n",
      "reading file tmp/h01-20201002-103017.csv\n",
      "reading file tmp/h01-20201002-104026.csv\n",
      "reading file tmp/h01-20201002-020621.csv\n",
      "reading file tmp/h01-20201002-053340.csv\n",
      "reading file tmp/h01-20201002-162943.csv\n",
      "reading file tmp/h01-20201002-052300.csv\n",
      "reading file tmp/h01-20201002-071532.csv\n",
      "reading file tmp/h01-20201002-185317.csv\n",
      "reading file tmp/h01-20201002-203519.csv\n",
      "20201003\n",
      "reading file tmp/h01-20201002-194811.csv\n",
      "reading file tmp/h01-20201002-201326.csv\n",
      "20201003\n",
      "reading file tmp/h01-20201002-005107.csv\n",
      "reading file tmp/h01-20201002-074932.csv\n",
      "reading file tmp/h01-20201002-221747.csv\n",
      "20201003\n",
      "reading file tmp/h01-20201002.csv\n",
      "reading file tmp/h01-20201002-030433.csv\n",
      "reading file tmp/h01-20201002-120839.csv\n",
      "reading file tmp/h01-20201002-111602.csv\n",
      "reading file tmp/h01-20201002-015538.csv\n",
      "reading file tmp/h01-20201002-084209.csv\n",
      "reading file tmp/h01-20201002-170405.csv\n",
      "reading file tmp/h01-20201002-031854.csv\n",
      "reading file tmp/h01-20201002-074306.csv\n",
      "reading file tmp/h01-20201002-024531.csv\n",
      "reading file tmp/h01-20201002-121850.csv\n",
      "reading file tmp/h01-20201002-012530.csv\n",
      "reading file tmp/h01-20201002-184220.csv\n",
      "reading file tmp/h01-20201002-124109.csv\n",
      "reading file tmp/h01-20201002-190447.csv\n",
      "reading file tmp/h01-20201002-184432.csv\n",
      "reading file tmp/h01-20201002-004226.csv\n",
      "reading file tmp/h01-20201002-100524.csv\n",
      "reading file tmp/h01-20201002-225046.csv\n",
      "20201003\n",
      "get quoted text False False\n",
      "get quoted text True False\n",
      "get quoted text True True\n",
      "get quoted text False True\n"
     ]
    }
   ],
   "source": [
    "start_date, end_date = get_date_range(FILENAME)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "if os.path.isdir(os.path.join(TEMP_FOLDER, FOLDER_NAME)):\n",
    "    pass\n",
    "else:\n",
    "    FOLDER_NAME = \"\"\n",
    "\n",
    "for single_date in daterange(start_date, end_date):\n",
    "    file_name_start = 'h01-{}'.format(single_date.strftime(\"%Y%m%d\"))\n",
    "    date_to_analyse, year = strip_date(file_name_start)\n",
    "    final_clean(FILENAME, TEMP_FOLDER, file_name_start, date_to_analyse, FOLDER_NAME, year)\n",
    "    \n",
    "    #print(date_to_analyse, year)\n",
    "    #print(single_date.strftime(\"%Y-%m-%d\"))\n",
    "    \n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree(TEMP_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "################ BEFORE USE ################\n",
    "1. In Block [22] :- Change FILENAME and TEMP_FOLDER accordingly\n",
    "2. In Block [9] :- Change directory accordingly\n",
    "3. In Block [18] :- Change directory accordingly\n",
    "4. In Block [20] :- Change directory accordingly\n",
    "\n",
    "################ USAGE ################\n",
    "1. In Block [22] :- Pass the location of zip file to be cleaned in FILENAME\n",
    "2. In Block [23] :- Pass the name of zip file to be cleaned in get_date_range function\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Data_Cleaning_and_Analysis_V2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
