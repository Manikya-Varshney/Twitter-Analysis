{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JHBSrnVsmQT1"
   },
   "source": [
    "\n",
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "iN58fBbFo48-"
   },
   "source": [
    "# Imports for GDrive\n",
    "!pip install -U -q PyDrive\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from google.colab import auth\n",
    "from oauth2client.client import GoogleCredentials"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "XssQCdN5pE03"
   },
   "source": [
    "# Mounting the drive\n",
    "auth.authenticate_user()\n",
    "gauth = GoogleAuth()\n",
    "gauth.credentials = GoogleCredentials.get_application_default()\n",
    "drive = GoogleDrive(gauth)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "3zJK_ICLpTmX"
   },
   "source": [
    "#downloaded = drive.CreateFile({'id':'https://drive.google.com/file/d/17dnH3TDdLmxos83OTHEtXBCD5Tmu-L_k/view?usp=sharing'}) # replace the id with id of file you want to access\n",
    "#downloaded.GetContentFile('h01-20201001-20201008.zip') "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QOdpxl2-w7sR",
    "outputId": "94499167-2ecf-416c-c6ce-b153f5acbd91"
   },
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1xtF___3xLV3",
    "outputId": "152e3328-6a9b-4d40-b40b-959df2d60dfc"
   },
   "source": [
    "import os\n",
    "os.listdir('/content/gdrive/Shared drives/') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XiUQenL10eNC",
    "outputId": "56b52020-09be-4791-e2ca-7fcc010aae99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/anaconda3/envs/twitter/lib/python3.8/site-packages (1.3.2)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /opt/anaconda3/envs/twitter/lib/python3.8/site-packages (from pandas) (1.21.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/anaconda3/envs/twitter/lib/python3.8/site-packages (from pandas) (2021.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/anaconda3/envs/twitter/lib/python3.8/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/twitter/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas) (1.16.0)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/envs/twitter/lib/python3.8/site-packages (1.21.2)\n",
      "Requirement already satisfied: nltk in /opt/anaconda3/envs/twitter/lib/python3.8/site-packages (3.6.2)\n",
      "Requirement already satisfied: joblib in /opt/anaconda3/envs/twitter/lib/python3.8/site-packages (from nltk) (1.0.1)\n",
      "Requirement already satisfied: regex in /opt/anaconda3/envs/twitter/lib/python3.8/site-packages (from nltk) (2021.8.28)\n",
      "Requirement already satisfied: click in /opt/anaconda3/envs/twitter/lib/python3.8/site-packages (from nltk) (8.0.1)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/envs/twitter/lib/python3.8/site-packages (from nltk) (4.62.2)\n",
      "Requirement already satisfied: multiprocess in /opt/anaconda3/envs/twitter/lib/python3.8/site-packages (0.70.11.1)\n",
      "Requirement already satisfied: dill>=0.3.3 in /opt/anaconda3/envs/twitter/lib/python3.8/site-packages (from multiprocess) (0.3.4)\n",
      "Requirement already satisfied: emoji in /opt/anaconda3/envs/twitter/lib/python3.8/site-packages (1.4.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/priyanshkedia/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/priyanshkedia/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/priyanshkedia/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/priyanshkedia/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n",
    "!pip install numpy\n",
    "!pip install nltk\n",
    "\n",
    "# Import statements\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import csv\n",
    "import re\n",
    "import string\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "!pip install multiprocess\n",
    "import multiprocess\n",
    "from multiprocess import Pool, Process\n",
    "\n",
    "!pip install emoji\n",
    "import emoji\n",
    "import nltk\n",
    "from nltk import pos_tag, pos_tag_sents\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "stop_words = set(stopwords.words('english')) - set(['at', 'do', 'your', 'from', 'to', 'out', 'no', 'the'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "id": "G7-qNi6n_pMM"
   },
   "outputs": [],
   "source": [
    "# Extract zip file\n",
    "\n",
    "def extract_zips(filename, temp_folder):\n",
    "  zip_ref = zipfile.ZipFile(filename, 'r')\n",
    "  zip_ref.extractall(temp_folder)\n",
    "  zip_ref.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "id": "OHS5daIFGbLI"
   },
   "outputs": [],
   "source": [
    "def create_filename(FILENAME, TEMP_FOLDER):\n",
    "  FILES_NAMES = FILENAME.split(\".\")[0].split(\"/\")[-1]\n",
    "  FILES_NAMES = FILES_NAMES.rsplit(\"-\", 1)[0]\n",
    "  # FILES_NAMES = os.path.join(TEMP_FOLDER, FILES_NAMES)\n",
    "  return FILES_NAMES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "id": "xvgg28WxGj1R"
   },
   "outputs": [],
   "source": [
    "def get_date_string(FILESNAMES):\n",
    "  date_string = FILESNAMES.split(\"-\")[-1]\n",
    "  return date_string\n",
    "\n",
    "def strip_date(FILESNAMES):\n",
    "  date_string = get_date_string(FILESNAMES)\n",
    "  ob = datetime.strptime(date_string, \"%Y%m%d\")\n",
    "  date_to_analyse = ob.strftime(\"%a %b %d\")\n",
    "  year = ob.strftime(\"%Y\")\n",
    "  return date_to_analyse, year\n",
    "\n",
    "def get_date_numbers(date_string):\n",
    "    ob = datetime.strptime(date_string, \"%a %b %d\")\n",
    "    return ob.strftime(\"%d\"), ob.strftime(\"%m\")\n",
    "    \n",
    "def get_next_date(current_date,year):\n",
    "    day, month = get_date_numbers(current_date)\n",
    "    date = datetime(int(year), int(month), int(day))\n",
    "    date += timedelta(days = 1)\n",
    "    return date.strftime(\"%Y%m%d\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_directory(directory):\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "id": "Axem73RZAAkA"
   },
   "outputs": [],
   "source": [
    "# Constants\n",
    "FILENAME = \"h01-20200818-10files.zip\"\n",
    "TEMP_FOLDER = \"/tmp\"\n",
    "FILES_NAMES = create_filename(FILENAME, TEMP_FOLDER)\n",
    "date_to_analyse = strip_date(FILES_NAMES)\n",
    "# print(FILES_NAMES, date_to_analyse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "added_ids = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "id": "ZuK9Y_DSL0cL"
   },
   "outputs": [],
   "source": [
    "# Keep data in english only\n",
    "def remove_other_langs(data):\n",
    "  data = data[data['lang'] == 'en'].reset_index(drop=True)\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_other_date(data, old_data, current_date):\n",
    "    if data.equals(old_data):\n",
    "        return\n",
    "    date_to_save = get_next_date(current_date, year)\n",
    "    print(date_to_save)\n",
    "    directory = \"data\"\n",
    "    create_directory(directory)\n",
    "    combined = pd.concat([data, old_data]).drop_duplicates(keep=False)\n",
    "    \n",
    "    combined.to_csv(\"{}/{}.csv\".format(directory, date_to_save))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "id": "Xwsew-o1MzvI"
   },
   "outputs": [],
   "source": [
    "# Keep specific date\n",
    "def remove_other_dates(data, date_to_analyse):\n",
    "  new_data = data[data['created_at'].str[:10] == date_to_analyse].reset_index(drop=True)\n",
    "  save_other_date(new_data, data, date_to_analyse)\n",
    "  return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "id": "fnpiNeSksPHD"
   },
   "outputs": [],
   "source": [
    "# Creating the RT Column\n",
    "def create_rt_column(data):\n",
    "  data['RT'] = data['text'].str[:2]=='RT'\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "id": "jDie85V6J4IJ"
   },
   "outputs": [],
   "source": [
    "# Parse CSV data\n",
    "def parse_data_from_file(filename, date_to_analyse):\n",
    "  data = pd.read_csv(filename, index_col = None, header=0, engine = 'python')\n",
    "  data = remove_other_langs(data)\n",
    "  data = remove_other_dates(data, date_to_analyse)\n",
    "  data = create_rt_column(data)\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "id": "oksZ2gyHLTYM"
   },
   "outputs": [],
   "source": [
    "# Parse all files of the same date\n",
    "def parse_all_files(TEMP_FOLDER, FILES_NAMES, date_to_analyse):\n",
    "  files = os.listdir(TEMP_FOLDER)\n",
    "  new_data = []\n",
    "  for file in files:\n",
    "    if file.startswith(FILES_NAMES):\n",
    "      parsed_data = parse_data_from_file(os.path.join(TEMP_FOLDER, file), date_to_analyse)\n",
    "      new_data.append(parsed_data)\n",
    "  return pd.concat(new_data, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "id": "d6dvi4iZuO7z"
   },
   "outputs": [],
   "source": [
    "def get_text_or_extended_text(data, added_ids):\n",
    "  data[\"FINAL_TEXT\"] = np.where(data['extended_tweet_full_text'].notnull(), data[\"extended_tweet_full_text\"], data[\"text\"])\n",
    "  id = data[\"id\"].to_dict()\n",
    "  added_ids.update(id)\n",
    "  return data[\"FINAL_TEXT\"]\n",
    "\n",
    "def get_quoted_text(data, added_ids):\n",
    "  data[\"FINAL_TEXT\"] = np.where(data['QT_full_text'].notnull(), data[\"QT_full_text\"], data[\"QT_text\"])\n",
    "  added_ids.update(data[\"id\"].to_dict())\n",
    "  added_ids.update(data[\"QT_id\"].to_dict())\n",
    "  return data[\"FINAL_TEXT\"]\n",
    "\n",
    "def get_text_or_full_text_rt(data, added_ids):\n",
    "  data[\"FINAL_TEXT\"] = np.where(data['RT_full_text'].notnull(), data[\"RT_full_text\"], data[\"RT_text\"])\n",
    "  added_ids.update(data[\"id\"].to_dict())\n",
    "  added_ids.update(data[\"RT_id\"].to_dict())\n",
    "  return data[\"FINAL_TEXT\"]\n",
    "\n",
    "def get_quote_rt_full(data, added_ids):\n",
    "  data[\"FINAL_TEXT\"] = get_text_or_full_text_rt(data, added_ids) + get_quoted_text(data, added_ids)\n",
    "  return data[\"FINAL_TEXT\"]\n",
    "\n",
    "\n",
    "\n",
    "def get_quote_rt(data, is_quote, is_rt):\n",
    "  data = data.loc[(data['is_quote_tweet'] == is_quote) & (data['RT'] == is_rt)]\n",
    "\n",
    "  if not is_quote and not is_rt:\n",
    "    added_ids.update(dict(zip(data[\"id\"], data[\"is_quote_tweet\"])))\n",
    "    data[\"FINAL_TEXT\"] = np.where(data['extended_tweet_full_text'].notnull(), data[\"extended_tweet_full_text\"], data[\"text\"])\n",
    "\n",
    "  if is_quote and not is_rt:\n",
    "    data[\"FINAL_TEXT\"] = np.where(data[\"QT_id\"].isin(added_ids.keys()), get_text_or_extended_text(data, added_ids), get_quoted_text(data, added_ids))\n",
    "\n",
    "  if not is_quote and is_rt:\n",
    "    data[\"FINAL_TEXT\"] = np.where(data[\"RT_id\"].isin(added_ids.keys()), added_ids.update(data['id'].to_dict()), get_text_or_full_text_rt(data, added_ids))\n",
    "    data = data[data[\"FINAL_TEXT\"].notna()]\n",
    "\n",
    "  if is_quote and is_rt:\n",
    "    data[\"FINAL_TEXT\"] = np.where((data[\"RT_id\"].isin(added_ids.keys())) & (data[\"QT_id\"].isin(added_ids.keys())), added_ids.update(data['id'].to_dict()), None)\n",
    "    data[\"FINAL_TEXT\"] = np.where((data[\"RT_id\"].isin(added_ids.keys())) & (~data[\"QT_id\"].isin(added_ids.keys())), get_quoted_text(data, added_ids), None)\n",
    "    data[\"FINAL_TEXT\"] = np.where((~data[\"RT_id\"].isin(added_ids.keys())) & (data[\"QT_id\"].isin(added_ids.keys())), get_text_or_full_text_rt(data, added_ids), None)\n",
    "    data[\"FINAL_TEXT\"] = np.where((~data[\"RT_id\"].isin(added_ids.keys())) & (~data[\"QT_id\"].isin(added_ids.keys())), get_quote_rt_full(data, added_ids), None)\n",
    "    data = data[data[\"FINAL_TEXT\"].notna()]\n",
    "\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_df(filename, TEMP_FOLDER, FILES_NAMES, date_to_analyse):\n",
    "    extract_zips(filename, TEMP_FOLDER)\n",
    "    data = parse_all_files(TEMP_FOLDER, FILES_NAMES, date_to_analyse)\n",
    "\n",
    "    non_quote_non_rt = get_quote_rt(data, False, False)\n",
    "    quote_non_rt = get_quote_rt(data, True, False)\n",
    "    quote_rt = get_quote_rt(data, True, True)\n",
    "    non_quote_rt = get_quote_rt(data, False, True)\n",
    "    \n",
    "    return non_quote_non_rt, quote_non_rt, quote_rt, non_quote_rt\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HCpVf_aD1lV-"
   },
   "source": [
    "1. Casing (Upper or lower case)\n",
    "2. Noise Removal (Removal of punctuation, white spaces, special characters, HTML tags)\n",
    "3. Tokenization (Tweets to tokens i.e. words seprated by spaces)\n",
    "4. Stopword Removal\n",
    "5. Text Normalization (Stemming and Lemmatization) bold text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "id": "e212hr-q2LKf"
   },
   "outputs": [],
   "source": [
    "#Removing emojis\n",
    "def demoji(text):\n",
    "  emoji_pattern = re.compile(\"[\"\n",
    "  u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                u\"\\U00002702-\\U000027B0\"\n",
    "                u\"\\U000024C2-\\U0001F251\"\n",
    "                u\"\\U0001f926-\\U0001f937\"\n",
    "                u'\\U00010000-\\U0010ffff'\n",
    "                u\"\\u200d\"\n",
    "                u\"\\u2640-\\u2642\"\n",
    "                u\"\\u2600-\\u2B55\"\n",
    "                u\"\\u23cf\"\n",
    "                u\"\\u23e9\"\n",
    "                u\"\\u231a\"\n",
    "                u\"\\u3030\"\n",
    "                u\"\\ufe0f\"\n",
    "                        \"]+\", flags=re.UNICODE)\n",
    "  return(emoji_pattern.sub(r'', text.decode('utf-8')))\n",
    "\n",
    "#POSTags\n",
    "def get_wordnet_pos(word):\n",
    "  \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "  tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "  tag_dict = {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV}\n",
    "  return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "def apply_stemming(row):\n",
    "  row_list = row[\"FINAL_TEXT\"]\n",
    "  stemmed_list = [ps.stem(word) for word in row_list]\n",
    "  return (stemmed_list)\n",
    "\n",
    "#Remove URLs, user@, punctutions\n",
    "def df_cleaning(data_):\n",
    "  punc = string.punctuation\n",
    "  data_[\"FINAL_TEXT\"] = data_[\"FINAL_TEXT\"].astype(str).str.replace(r\"http\\S+| www\\S+| https\\S+| \\S+\\.com\\S+| \\S+\\.com| \\@[\\w]+\", \"\", regex=True)\n",
    "\n",
    "  # ##################-------Punctutions-------##################\n",
    "  data_[\"FINAL_TEXT\"] = data_[\"FINAL_TEXT\"].astype(str).str.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    " \n",
    "  # ##################-------More Cleaning-------##################\n",
    "  data_[\"FINAL_TEXT\"] = data_[\"FINAL_TEXT\"].astype(str).str.replace(\"/[^a-zA-Z0-9 ]/g\", \"\", regex=True).str.replace(\"\\n\",\" \", regex=True).str.replace(\"—\",\" \", regex=True).str.strip(\"“\").str.strip(\"”\").str.strip(\"’\").str.lstrip(\" \").str.rstrip(\" \")\n",
    "  \n",
    "  # ##################-------Emojis-------##################\n",
    "  data_[\"FINAL_TEXT\"] = data_[\"FINAL_TEXT\"].astype(str).apply(lambda x:demoji(x.encode('utf8')))\n",
    "\n",
    "  # ##################-------Tokenizing-------##################\n",
    "  data_[\"FINAL_TEXT\"] = data_[\"FINAL_TEXT\"].astype(str)\n",
    "  data_[\"FINAL_TEXT\"] = data_[\"FINAL_TEXT\"].apply(nltk.word_tokenize)\n",
    "#   data_[\"FINAL_TEXT\"] = data_.apply(lambda row: nltk.word_tokenize(row[\"FINAL_TEXT\"]), axis=1)\n",
    "\n",
    "  # ##################-------Lower characters---------##################\n",
    "  data_[\"FINAL_TEXT\"] = data_[\"FINAL_TEXT\"].apply(lambda x: [word.lower() for word in x])\n",
    "\n",
    "  # ##################-------Remove punctuations-------##################\n",
    "  data_[\"FINAL_TEXT\"] = data_[\"FINAL_TEXT\"].apply(lambda x: [word for word in x if word not in punc])\n",
    "\n",
    "  # ##################-------Removing stopwords-------##################\n",
    "#   data_[\"FINAL_TEXT\"] = data_[\"FINAL_TEXT\"].astype(str).apply(lambda x: [word for word in x if word not in stop_words])\n",
    "  \n",
    "  # data_[\"FINAL_TEXT\"] = data_[\"FINAL_TEXT\"].astype(str).apply(lambda x: [word for word in x if word not in stop_words])\n",
    "  data_[\"FINAL_TEXT\"] = data_[\"FINAL_TEXT\"].apply(lambda words: ' '.join(word for word in words if word not in stop_words))\n",
    " \n",
    "  \n",
    "  # ##################-------Stemming-------##################\n",
    "  ps = PorterStemmer()\n",
    "#   data_[\"FINAL_TEXT\"] = data_[\"FINAL_TEXT\"].apply(lambda x: [ps.stem(y) for y in x])\n",
    "  data_[\"FINAL_TEXT\"] = data_[\"FINAL_TEXT\"].apply(lambda x: [ps.stem(y) for y in x.split(' ')])\n",
    "#   data_[\"FINAL_TEXT\"] = data_.apply(apply_stemming, axis = 1)\n",
    "#   data_[\"FINAL_TEXT\"] = data_.apply(ps.stem)\n",
    "\n",
    "#   print(data_[\"FINAL_TEXT\"], \"from here\")\n",
    "\n",
    "\n",
    "  # ##################-------Lemmatizing-------##################\n",
    "#   lemmatizer = WordNetLemmatizer()\n",
    "#   data[\"FINAL_TEXT\"] = data[\"FINAL_TEXT\"].astype(str).apply(lambda x: [lemmatizer.lemmatize(y, get_wordnet_pos(y)) for y in x])\n",
    "  \n",
    "  # ##################-------Joining the lemmetized tokens to form string-------##################\n",
    "  data_[\"FINAL_TEXT\"] = data_[\"FINAL_TEXT\"].apply(lambda x: ' '.join(x))\n",
    " \n",
    "  # ##################-------Remove punctuations-------##################\n",
    "#   data_[\"FINAL_TEXT\"] = data_[\"FINAL_TEXT\"].astype(str).str.translate(str.maketrans(\"\", \"\", string.punctuation)).str.replace(\"’\", \" \").str.replace(\"“\", \" \").str.replace(\"”\", \" \")\n",
    "\n",
    "#   final_df[\"FINAL_TEXT\"] = data_[\"FINAL_TEXT\"]\n",
    "#   final_df = data_\n",
    "  return data_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "id": "ROcVKfprPobe"
   },
   "outputs": [],
   "source": [
    "def split_dataframe(df, nums = 4): \n",
    "    chunks = list()\n",
    "    num_chunks = nums\n",
    "    chunk_size = len(df) // nums\n",
    "    for i in range(num_chunks):\n",
    "        chunks.append(df[i*chunk_size:(i+1)*chunk_size])\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data_and_save(data_,filename):\n",
    "    print(data_.shape)\n",
    "    start = time.time()\n",
    "    pool = Pool(8)\n",
    "    df = split_dataframe(data_, 8)\n",
    "    data1 = pd.concat(pool.map(df_cleaning, df))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    end = time.time()\n",
    "    directory = \"output_data/{}\".format(date_to_analyse)\n",
    "    create_directory(directory)\n",
    "    data1.to_csv(\"{}/{}.csv\".format(directory, filename))\n",
    "    print(\"time = \", end - start)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_all_data_and_save(data_):\n",
    "    non_quote_non_rt, quote_non_rt, quote_rt, non_quote_rt = data_\n",
    "    clean_data_and_save(non_quote_non_rt, 'non_quote_non_rt')\n",
    "    clean_data_and_save(quote_non_rt, 'quote_non_rt')    \n",
    "    clean_data_and_save(non_quote_rt, 'non_quote_rt')\n",
    "    clean_data_and_save(quote_rt, 'quote_rt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_all_data(data_):\n",
    "    non_quote_non_rt, quote_non_rt, quote_rt, non_quote_rt = data_\n",
    "    combined = pd.concat([non_quote_non_rt, quote_non_rt, quote_rt, non_quote_rt])\n",
    "    directory = \"output_data/{}\".format(date_to_analyse)\n",
    "    create_directory(directory)\n",
    "    combined.to_csv('{}/combined_{}.csv'.format(directory, date_to_analyse))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILENAME = \"h01-20200818-10files.zip\"\n",
    "TEMP_FOLDER = \"/tmp\"\n",
    "FILES_NAMES = create_filename(FILENAME, TEMP_FOLDER)\n",
    "date_to_analyse, year = strip_date(FILES_NAMES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20200819\n",
      "20200819\n",
      "20200819\n",
      "20200819\n",
      "20200819\n",
      "20200819\n",
      "20200819\n",
      "20200819\n",
      "20200819\n",
      "20200819\n"
     ]
    }
   ],
   "source": [
    "all_data = separate_df(FILENAME, TEMP_FOLDER, FILES_NAMES, date_to_analyse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28689, 81)\n",
      "time =  5.30834698677063\n",
      "(9171, 81)\n",
      "time =  1.7333340644836426\n",
      "(49999, 81)\n",
      "time =  7.5633440017700195\n",
      "(17882, 81)\n",
      "time =  4.577805995941162\n"
     ]
    }
   ],
   "source": [
    "clean_all_data_and_save(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_all_data(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (439156515.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/var/folders/xv/dy0flwrs2712ftrn4zct001r0000gn/T/ipykernel_4173/439156515.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    def fun_name(row, keywords, facet)\u001b[0m\n\u001b[0m                                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def fun_name(row, keywords, facet)\n",
    "row[\"final_row_{}\".format(facet.name)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Facet(Enum):\n",
    "    ADAPTATION = \"Ada\""
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Data_Cleaning_and_Analysis_V2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
