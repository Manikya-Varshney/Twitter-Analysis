{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AkwA0L1rzo19",
    "outputId": "7f76ccf7-17aa-4218-c766-c72fd7f6bb3f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: fuzzywuzzy in /home/manikya_varshney/.local/lib/python3.8/site-packages (0.18.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.3.3; however, version 21.2.4 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: python-Levenshtein in /home/manikya_varshney/.local/lib/python3.8/site-packages (0.12.2)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from python-Levenshtein) (45.2.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.3.3; however, version 21.2.4 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: sklearn in /home/manikya_varshney/.local/lib/python3.8/site-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in /home/manikya_varshney/.local/lib/python3.8/site-packages (from sklearn) (0.24.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/manikya_varshney/.local/lib/python3.8/site-packages (from scikit-learn->sklearn) (1.0.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /home/manikya_varshney/.local/lib/python3.8/site-packages (from scikit-learn->sklearn) (1.19.5)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /home/manikya_varshney/.local/lib/python3.8/site-packages (from scikit-learn->sklearn) (1.6.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/manikya_varshney/.local/lib/python3.8/site-packages (from scikit-learn->sklearn) (2.1.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.3.3; however, version 21.2.4 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in /home/manikya_varshney/.local/lib/python3.8/site-packages (1.2.3)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/lib/python3/dist-packages (from pandas) (2019.3)\n",
      "Requirement already satisfied: numpy>=1.16.5 in /home/manikya_varshney/.local/lib/python3.8/site-packages (from pandas) (1.19.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/lib/python3/dist-packages (from pandas) (2.7.3)\n",
      "\u001b[33mWARNING: You are using pip version 20.3.3; however, version 21.2.4 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: numpy in /home/manikya_varshney/.local/lib/python3.8/site-packages (1.19.5)\n",
      "\u001b[33mWARNING: You are using pip version 20.3.3; however, version 21.2.4 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: multiprocess in /home/manikya_varshney/.local/lib/python3.8/site-packages (0.70.12.2)\n",
      "Requirement already satisfied: dill>=0.3.4 in /home/manikya_varshney/.local/lib/python3.8/site-packages (from multiprocess) (0.3.4)\n",
      "\u001b[33mWARNING: You are using pip version 20.3.3; however, version 21.2.4 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/manikya_varshney/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/manikya_varshney/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "#Make imports\n",
    "!pip install fuzzywuzzy\n",
    "!pip install python-Levenshtein\n",
    "!pip install sklearn\n",
    "!pip install pandas\n",
    "!pip install numpy\n",
    "\n",
    "import pandas as pd\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "import string\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import time\n",
    "from enum import Enum\n",
    "\n",
    "!pip install multiprocess\n",
    "import multiprocess\n",
    "import multiprocessing as mp\n",
    "from multiprocessing import Pool, Process\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk import pos_tag, pos_tag_sents\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "stop_words = set(stopwords.words('english')) - set(['at', 'do', 'your', 'from', 'to', 'out', 'no', 'the'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read csv with pandas:  1.15108323097229 sec\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "path = '/home/manikya_varshney/Documents/Python/Yale/h01-20200818-10files/h01-20200818-153021.csv'\n",
    "data = pd.read_csv(path, index_col=None, header=0, engine='python' )\n",
    "end = time.time()\n",
    "print(\"Read csv with pandas: \",(end-start),\"sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "lPX-5sYhz5-s"
   },
   "outputs": [],
   "source": [
    "keywords_Imp = ['stay at home' , 'do your part', 'Responsible', \n",
    "            'home', 'house', 'cancel', 'shutdown', 'postpone',\n",
    "            'school closure', 'Closure', 'business closure',\n",
    "            'suspension', 'quarantine', 'lockdown', 'social distance', \n",
    "            'social distancing', 'self quarantine', 'isolat', '6-feet',\n",
    "            'distance', '#clubquarantine', '#quarantinelife', '#quarantineacitivites']\n",
    "\n",
    "keywords_Ada = ['school from home' , 'learn', 'remote', 'school food service', \n",
    "            'online shopping', 'online purchase', 'online church', 'delivery',\n",
    "            'drive thru', 'to go', 'take out', 'Tiktok', 'Netflix', 'telework', \n",
    "            'zoom', 'telehealth', 'telemedicine', 'work from home', 'wfh',\n",
    "            'working at home', 'working remotely', 'online meeting']\n",
    "\n",
    "keywords_Ne = ['bored' , 'lonely', 'stress', \n",
    "            'anxiety', 'scared', 'worry', 'end', 'cabin fever',\n",
    "            '#sideeffectsofquarantinelife', 'tissue paper', 'toilet paper']\n",
    "\n",
    "keywords_Sd = ['social functions' , 'gathering', 'empty streets', \n",
    "            'interaction', 'large', 'no cars', 'non-essential',\n",
    "            'travel', 'unnecessary', 'crowd']\n",
    "\n",
    "keywords_Purp = ['Flatten the curve' , 'Slow the spread', 'slow transmission', \n",
    "            'protect', 'save', '#stayhomesavelives']\n",
    "\n",
    "keywords_Pe = ['silver lining' , 'optimistic', 'hope', \n",
    "            'bright side', 'Safe', '#togetherapart']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "#FINAL_COL_NAME = \"FINAL_TEXT\"\n",
    "num_cores = mp.cpu_count()\n",
    "FINAL_COL_NAME = \"QT_text\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "MRI1T2thz-ha"
   },
   "outputs": [],
   "source": [
    "def keywords_cleaning(keywords_list):\n",
    "    \n",
    "    #Convert to lower\n",
    "    for i in range(len(keywords_list)): \n",
    "        keywords_list[i] = keywords_list[i].lower()\n",
    "    \n",
    "    #Remove punctuations\n",
    "    for i in range(len(keywords_list)):\n",
    "        keywords_list[i] = keywords_list[i].translate(str.maketrans('','',string.punctuation))\n",
    "    \n",
    "    #More cleaning\n",
    "    for i in range(len(keywords_list)):\n",
    "        keywords_list[i] = keywords_list[i].replace('/[^a-zA-Z0-9 ]/g', '').replace('\\n',' ').strip('“').strip('“').strip('’').lstrip(' ').rstrip(' ')\n",
    "\n",
    "    #Remove stop words\n",
    "    def remove_stopwords(data):\n",
    "        output_array=[]\n",
    "        for sentence in data:\n",
    "            temp_list=[]\n",
    "            for word in sentence.split():\n",
    "                if word not in stop_words:\n",
    "                    temp_list.append(word)\n",
    "            output_array.append(' '.join(temp_list))\n",
    "        return output_array\n",
    "\n",
    "    keywords_list=remove_stopwords(keywords_list)\n",
    "\n",
    "    #Stemming\n",
    "    ps = PorterStemmer()\n",
    "    keywords_list_stem = [[ps.stem(word) for word in sentence.split(\" \")] for sentence in keywords_list]\n",
    "    keywords_list_final = [\" \".join(sentence) for sentence in keywords_list_stem]\n",
    "\n",
    "    return keywords_list_final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enum for facets\n",
    "class Facets(Enum):\n",
    "    IMPLEMENTATION = \"Imp\"\n",
    "    ADAPTATION = \"Ada\"\n",
    "    NEGATIVE_EMOTIONS = \"Ne\"\n",
    "    SOCIAL_DISRUPTION = \"Sd\"\n",
    "    PURPOSE = \"Purp\"\n",
    "    POSITIVE_EMOTION = \"Pe\""
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def fuzzy_logic(row, FINAL_COL_NAME, keywords, facet):\n",
    "    keyword_match, score = process.extractOne(row[FINAL_COL_NAME], keywords, scorer = fuzz.partial_ratio)\n",
    "    row['final_score_{}'.format(facet.value)] = score\n",
    "    row['final_keyword_match_{}'.format(facet.value)] = keyword_match\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fuzzy_logic(args):\n",
    "    row, FINAL_COL_NAME, keywords, facet, qwargs = args\n",
    "    print(**qwargs['choices'])\n",
    "    keyword_match, score = row[FINAL_COL_NAME].apply(process.extractOne, **qwargs)\n",
    "    #keyword_match, score = process.extractOne(row[FINAL_COL_NAME], keywords, scorer = fuzz.partial_ratio)\n",
    "    row['final_score_{}'.format(facet.value)] = score\n",
    "    row['final_keyword_match_{}'.format(facet.value)] = keyword_match\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keep_only_highest(data, high_value, facet):\n",
    "    data['final_score_{}'.format(facet.value)] = data['final_score_{}'.format(facet.value)].astype(int)    \n",
    "    data = data[data['final_score_{}'.format(facet.value)] == 100].reset_index(drop=True)    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proportion(interim_data):\n",
    "    denominator = interim_data.shape[0]\n",
    "    numerator = interim_data.shape[0]\n",
    "    prop_val = (numerator/denominator)\n",
    "    return prop_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataframe(df, nums = 4): \n",
    "    chunks = list()\n",
    "    num_chunks = nums\n",
    "    chunk_size = len(df) // nums\n",
    "    for i in range(num_chunks):\n",
    "        chunks.append(df[i*chunk_size:(i+1)*chunk_size])\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4737, 79)\n",
      "8\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "print() argument after ** must be a mapping, not list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/usr/lib/python3.8/multiprocessing/pool.py\", line 125, in worker\n    result = (True, func(*args, **kwds))\n  File \"/usr/lib/python3.8/multiprocessing/pool.py\", line 48, in mapstar\n    return list(map(*args))\n  File \"<ipython-input-94-77b2361dd5a9>\", line 3, in fuzzy_logic\n    print(**qwargs['choices'])\nTypeError: print() argument after ** must be a mapping, not list\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-98-8fc6e2ec1f21>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"time = \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mmultiprocessing_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeywords_Imp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFacets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIMPLEMENTATION\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchoices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeywords_Imp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscorer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfuzz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartial_ratio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-98-8fc6e2ec1f21>\u001b[0m in \u001b[0;36mmultiprocessing_\u001b[0;34m(data_, keywords_, facet_, **qwargs)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m#data1 = pd.concat(pool.map(fuzzy_logic, tuples))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m#data1 = pool.starmap(test, [(1,2,3)])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mdata1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfuzzy_logic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/multiprocessing/pool.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    362\u001b[0m         \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mthat\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mreturned\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m         '''\n\u001b[0;32m--> 364\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapstar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstarmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    769\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 771\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    772\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    773\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: print() argument after ** must be a mapping, not list"
     ]
    }
   ],
   "source": [
    "def multiprocessing_(data_, keywords_, facet_, **qwargs):\n",
    "    print(data_.shape)\n",
    "    start = time.time()\n",
    "    pool = Pool(num_cores)\n",
    "    df = split_dataframe(data_, num_cores)\n",
    "    tuples = [(x, FINAL_COL_NAME, keywords_, facet_, qwargs) for x in df]\n",
    "    print(len(tuples))\n",
    "    #data1 = pd.concat(pool.map(fuzzy_logic, tuples))\n",
    "    #data1 = pool.starmap(test, [(1,2,3)])\n",
    "    data1 = pool.map(fuzzy_logic, tuples)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    end = time.time()\n",
    "    print(\"time = \", end - start)\n",
    "    return data1\n",
    "multiprocessing_(data, keywords_Imp, Facets.IMPLEMENTATION, choices = keywords_Imp, scorer = fuzz.partial_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keywords Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_Imp = keywords_cleaning(keywords_Imp)\n",
    "keywords_Ada = keywords_cleaning(keywords_Ada)\n",
    "keywords_Ne = keywords_cleaning(keywords_Ne)\n",
    "keywords_Sd = keywords_cleaning(keywords_Sd)\n",
    "keywords_Purp = keywords_cleaning(keywords_Purp)\n",
    "keywords_Pe = keywords_cleaning(keywords_Pe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop Rows with Empty Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_value = float(\"NaN\")\n",
    "data[FINAL_COL_NAME].replace(\"\", nan_value, inplace=True)\n",
    "data.dropna(subset = [FINAL_COL_NAME], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iASl8afS7JMY"
   },
   "source": [
    "# 1_Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "interim_Imp = multiprocessing_(data, keywords_Imp, Facets.IMPLEMENTATION)\n",
    "#interim_Imp = data.apply(fuzzy_logic, axis = 1, args = (FINAL_COL_NAME, keywords_Imp, Facets.IMPLEMENTATION))\n",
    "#interim_Imp = keep_only_highest(interim_Imp, 100, Facets.IMPLEMENTATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2_Adaptation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interim_Ada = data.apply(fuzzy_logic, axis = 1, args = (FINAL_COL_NAME, keywords_Ada, Facets.ADAPTATION))\n",
    "interim_Ada = keep_only_highest(interim_Ada, 100, Facets.ADAPTATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3_Negative Emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interim_Ne = data.apply(fuzzy_logic, axis = 1, args = (FINAL_COL_NAME, keywords_Ne, Facets.NEGATIVE_EMOTIONS))\n",
    "interim_Ne = keep_only_highest(interim_Ne, 100, Facets.NEGATIVE_EMOTIONS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4_Social Disruption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interim_Sd = data.apply(fuzzy_logic, axis = 1, args = (FINAL_COL_NAME, keywords_Sd, Facets.SOCIAL_DISRUPTION))\n",
    "interim_Sd = keep_only_highest(interim_Sd, 100, Facets.SOCIAL_DISRUPTION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5_Purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interim_Purp = data.apply(fuzzy_logic, axis = 1, args = (FINAL_COL_NAME, keywords_Purp, Facets.PURPOSE))\n",
    "interim_Purp = keep_only_highest(interim_Purp, 100, Facets.PURPOSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6_Positive Emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interim_Pe = data.apply(fuzzy_logic, axis = 1, args = (FINAL_COL_NAME, keywords_Pe, Facets.POSITIVE_EMOTION))\n",
    "interim_Pe = keep_only_highest(interim_Pe, 100, Facets.POSITIVE_EMOTION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All Proportions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"For IMPLEMENTATION:\", proportion(interim_Imp))\n",
    "print(\"For ADAPTATION:\", proportion(interim_Ada))\n",
    "print(\"For NEGATIVE EMOTIONS:\", proportion(interim_Ne))\n",
    "print(\"For SOCIAL DISRUPTION:\", proportion(interim_Sd))\n",
    "print(\"For PURPOSE:\", proportion(interim_Purp))\n",
    "print(\"For POSITIVE EMOTION:\", proportion(Pe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "All Facets Match_V1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
