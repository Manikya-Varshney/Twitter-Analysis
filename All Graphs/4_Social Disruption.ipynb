{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!pip install -U python-Levenshtein"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import sys \n",
    "!{sys.executable} -m pip install python-Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/manikya_varshney/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "import string\n",
    "\n",
    "import nltk\n",
    "from nltk import pos_tag, pos_tag_sents\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english')) - set(['at', 'do', 'your', 'from', 'to', 'out', 'no', 'the'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/manikya_varshney/Documents/Python/Yale/All Graphs/final_processed_april01.csv'\n",
    "data = pd.read_csv(path, low_memory=False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "social functions\n",
    "gathering\n",
    "empty streets\n",
    "interaction\n",
    "large\n",
    "no cars\n",
    "non-essential\n",
    "travel\n",
    "unnecessary\n",
    "crowd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = ['social functions' , 'gathering', 'empty streets', \n",
    "            'interaction', 'large', 'no cars', 'non-essential',\n",
    "            'travel', 'unnecessary', 'crowd',]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Casing (Upper or lower case)\n",
    "##### 2. Noise Removal (Removal of punctuation, white spaces, special characters, HTML tags)\n",
    "##### 3. Tokenization (Tweets to tokens i.e. words seprated by spaces)\n",
    "##### 4. Stopword Removal\n",
    "##### 5. Text Normalization (Stemming and Lemmatization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert to lower\n",
    "for i in range(len(keywords)): \n",
    "    keywords[i] = keywords[i].lower()\n",
    "\n",
    "#Remove punctuations   \n",
    "for i in range(len(keywords)): \n",
    "    keywords[i] = keywords[i].translate(str.maketrans('','',string.punctuation))\n",
    "\n",
    "#More cleaning\n",
    "for i in range(len(keywords)): \n",
    "    keywords[i] = keywords[i].replace('/[^a-zA-Z0-9 ]/g', '').replace('\\n',' ').strip('“').strip('“').strip('’').lstrip(' ').rstrip(' ')\n",
    "\n",
    "#Tokenize\n",
    "#keywords_tokens = [sub.split() for sub in keywords] \n",
    "\n",
    "#Remove stop words\n",
    "def remove_stopwords(data):\n",
    "    output_array=[]\n",
    "    for sentence in data:\n",
    "        temp_list=[]\n",
    "        for word in sentence.split():\n",
    "            if word not in stop_words:\n",
    "                temp_list.append(word)\n",
    "        output_array.append(' '.join(temp_list))\n",
    "    return output_array\n",
    "\n",
    "keywords_filtered=remove_stopwords(keywords)\n",
    "\n",
    "#Stemming\n",
    "ps = PorterStemmer()\n",
    "keywords_stem = [[ps.stem(word) for word in sentence.split(\" \")] for sentence in keywords_filtered]\n",
    "keywords_stem = [\" \".join(sentence) for sentence in keywords_stem]\n",
    "\n",
    "#Lemmetizing\n",
    "\n",
    "#POSTags\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "keywords_lem = [[lemmatizer.lemmatize(y, get_wordnet_pos(y)) for y in sentence.split(\" \")] for sentence in keywords_filtered]\n",
    "keywords_final = [\" \".join(sentence) for sentence in keywords_lem]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fuzzy Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['final'] = data['final'].apply(str)\n",
    "choices = data['final'].tolist()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def fuzzy_m(row):\n",
    "    matches = process.extract(row['final'], keywords_final, limit=3)\n",
    "    exactMatch = matches[0][1] == 90\n",
    "    row['Match 1'] = matches[0][0]\n",
    "    row['Match 1 Score'] = matches[0][1]\n",
    "    row['Match 2'] = \"\" if exactMatch else matches[1][0]\n",
    "    row['Match 2 Score'] = \"\" if exactMatch else matches[1][1]\n",
    "    row['Match 3'] = \"\" if exactMatch else matches[2][0]\n",
    "    row['Match 3 Score'] = \"\" if exactMatch else matches[2][1]\n",
    "    return row    "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "interim_sd = data.apply(fuzzy_m, axis=1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "interim_sd"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "interim_sd['Match 1 Score'] = interim_sd['Match 1 Score'].astype(int)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "interim_sd[interim_sd['Match 1 Score'] == 90].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    " def fuzzy_m(row):\n",
    "        keyword_match, score = process.extractOne(row['final'], keywords_final, scorer = fuzz.partial_ratio)\n",
    "        row['final_score'] = score\n",
    "        row['final_keyword_match'] = keyword_match\n",
    "        return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "interim_sd = data.apply(fuzzy_m, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>text_duplicate</th>\n",
       "      <th>final</th>\n",
       "      <th>final_score</th>\n",
       "      <th>final_keyword_match</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28433993</td>\n",
       "      <td>As Chicago households fill out 2020 census dur...</td>\n",
       "      <td>chicago household fill out 2020 census coronav...</td>\n",
       "      <td>55</td>\n",
       "      <td>interaction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>101043870</td>\n",
       "      <td>Most of the press corps is very busy covering ...</td>\n",
       "      <td>the press corp busy cover the really important...</td>\n",
       "      <td>67</td>\n",
       "      <td>travel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16475267</td>\n",
       "      <td>@realDonaldTrump Mar 10–he was very specific: ...</td>\n",
       "      <td>mar 10–he specific  google 1700 engineer work ...</td>\n",
       "      <td>60</td>\n",
       "      <td>large</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16475267</td>\n",
       "      <td>@realDonaldTrump , You were caustic &amp;amp; sarc...</td>\n",
       "      <td>caustic amp sarcastic the rollout  come even f...</td>\n",
       "      <td>73</td>\n",
       "      <td>interaction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>345397708</td>\n",
       "      <td>Florida nears 8,000 coronavirus cases, as stat...</td>\n",
       "      <td>florida nears 8000 coronavirus case state repo...</td>\n",
       "      <td>60</td>\n",
       "      <td>crowd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16185</th>\n",
       "      <td>751644712570847236</td>\n",
       "      <td>If in 1938 Turkey’s government could produce a...</td>\n",
       "      <td>1938 turkey  government could produce health m...</td>\n",
       "      <td>60</td>\n",
       "      <td>large</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16186</th>\n",
       "      <td>286628737</td>\n",
       "      <td>Sandy Medford was a friend of my family for de...</td>\n",
       "      <td>sandy medford friend family decade suspect hea...</td>\n",
       "      <td>50</td>\n",
       "      <td>no car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16187</th>\n",
       "      <td>361010205</td>\n",
       "      <td>One month ago Trump claimed the number of Amer...</td>\n",
       "      <td>one month ago trump claimed the number america...</td>\n",
       "      <td>60</td>\n",
       "      <td>large</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16188</th>\n",
       "      <td>481389842</td>\n",
       "      <td>Yes. This. Especially if a substantial proport...</td>\n",
       "      <td>yes especially substantial proportion workforc...</td>\n",
       "      <td>60</td>\n",
       "      <td>large</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16189</th>\n",
       "      <td>11347222</td>\n",
       "      <td>I'm sticking to my original lowball prediction...</td>\n",
       "      <td>im stick to original lowball prediction at lea...</td>\n",
       "      <td>67</td>\n",
       "      <td>no car</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16190 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  user_id                                     text_duplicate  \\\n",
       "0                28433993  As Chicago households fill out 2020 census dur...   \n",
       "1               101043870  Most of the press corps is very busy covering ...   \n",
       "2                16475267  @realDonaldTrump Mar 10–he was very specific: ...   \n",
       "3                16475267  @realDonaldTrump , You were caustic &amp; sarc...   \n",
       "4               345397708  Florida nears 8,000 coronavirus cases, as stat...   \n",
       "...                   ...                                                ...   \n",
       "16185  751644712570847236  If in 1938 Turkey’s government could produce a...   \n",
       "16186           286628737  Sandy Medford was a friend of my family for de...   \n",
       "16187           361010205  One month ago Trump claimed the number of Amer...   \n",
       "16188           481389842  Yes. This. Especially if a substantial proport...   \n",
       "16189            11347222  I'm sticking to my original lowball prediction...   \n",
       "\n",
       "                                                   final  final_score  \\\n",
       "0      chicago household fill out 2020 census coronav...           55   \n",
       "1      the press corp busy cover the really important...           67   \n",
       "2      mar 10–he specific  google 1700 engineer work ...           60   \n",
       "3      caustic amp sarcastic the rollout  come even f...           73   \n",
       "4      florida nears 8000 coronavirus case state repo...           60   \n",
       "...                                                  ...          ...   \n",
       "16185  1938 turkey  government could produce health m...           60   \n",
       "16186  sandy medford friend family decade suspect hea...           50   \n",
       "16187  one month ago trump claimed the number america...           60   \n",
       "16188  yes especially substantial proportion workforc...           60   \n",
       "16189  im stick to original lowball prediction at lea...           67   \n",
       "\n",
       "      final_keyword_match  \n",
       "0             interaction  \n",
       "1                  travel  \n",
       "2                   large  \n",
       "3             interaction  \n",
       "4                   crowd  \n",
       "...                   ...  \n",
       "16185               large  \n",
       "16186              no car  \n",
       "16187               large  \n",
       "16188               large  \n",
       "16189              no car  \n",
       "\n",
       "[16190 rows x 5 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interim_sd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "interim_sd['final_score'] = interim_sd['final_score'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>text_duplicate</th>\n",
       "      <th>final</th>\n",
       "      <th>final_score</th>\n",
       "      <th>final_keyword_match</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>318592768</td>\n",
       "      <td>Governor DeSantis, STOP caving to politics..li...</td>\n",
       "      <td>governor desantis stop cave to politicslisten ...</td>\n",
       "      <td>100</td>\n",
       "      <td>large</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>31676696</td>\n",
       "      <td>Coronavirus has led to sweeping travel restric...</td>\n",
       "      <td>coronavirus lead to sweep travel restriction a...</td>\n",
       "      <td>100</td>\n",
       "      <td>travel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>250891272</td>\n",
       "      <td>‼️ Tomorrow at 8:45 am., I will be interview b...</td>\n",
       "      <td>‼ tomorrow at 845 interview 10 covid19 virus i...</td>\n",
       "      <td>100</td>\n",
       "      <td>large</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>366</th>\n",
       "      <td>3354387689</td>\n",
       "      <td>Cities that have stay-at-home orders should al...</td>\n",
       "      <td>city stayathome order also schedule allow the ...</td>\n",
       "      <td>100</td>\n",
       "      <td>nonessential</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>285182559</td>\n",
       "      <td>Inoculum dose is very important. Small exposur...</td>\n",
       "      <td>inoculum dose important small exposure v large...</td>\n",
       "      <td>100</td>\n",
       "      <td>large</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15898</th>\n",
       "      <td>950335999</td>\n",
       "      <td>There’s a mandated stay at home order in my ci...</td>\n",
       "      <td>mandate stay at home order city influx covid ...</td>\n",
       "      <td>100</td>\n",
       "      <td>travel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15978</th>\n",
       "      <td>20119986</td>\n",
       "      <td>REMEMBER: As recently as THIS MONTH the Orange...</td>\n",
       "      <td>remember recently month the orange idiot lamen...</td>\n",
       "      <td>100</td>\n",
       "      <td>unnecessary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16050</th>\n",
       "      <td>721484105850822656</td>\n",
       "      <td>Where was your congressperson when the #WuhanV...</td>\n",
       "      <td>your congressperson the wuhanvirus make to ame...</td>\n",
       "      <td>100</td>\n",
       "      <td>travel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16063</th>\n",
       "      <td>15769010</td>\n",
       "      <td>I keep reading about spring breakers coming ba...</td>\n",
       "      <td>keep reading spring breaker come back infect r...</td>\n",
       "      <td>100</td>\n",
       "      <td>travel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16128</th>\n",
       "      <td>329681844</td>\n",
       "      <td>In Fairfax County. VIRGINIA....perhaps the ric...</td>\n",
       "      <td>fairfax county virginiaperhaps the richest one...</td>\n",
       "      <td>100</td>\n",
       "      <td>large</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>195 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  user_id                                     text_duplicate  \\\n",
       "154             318592768  Governor DeSantis, STOP caving to politics..li...   \n",
       "197              31676696  Coronavirus has led to sweeping travel restric...   \n",
       "276             250891272  ‼️ Tomorrow at 8:45 am., I will be interview b...   \n",
       "366            3354387689  Cities that have stay-at-home orders should al...   \n",
       "411             285182559  Inoculum dose is very important. Small exposur...   \n",
       "...                   ...                                                ...   \n",
       "15898           950335999  There’s a mandated stay at home order in my ci...   \n",
       "15978            20119986  REMEMBER: As recently as THIS MONTH the Orange...   \n",
       "16050  721484105850822656  Where was your congressperson when the #WuhanV...   \n",
       "16063            15769010  I keep reading about spring breakers coming ba...   \n",
       "16128           329681844  In Fairfax County. VIRGINIA....perhaps the ric...   \n",
       "\n",
       "                                                   final  final_score  \\\n",
       "154    governor desantis stop cave to politicslisten ...          100   \n",
       "197    coronavirus lead to sweep travel restriction a...          100   \n",
       "276    ‼ tomorrow at 845 interview 10 covid19 virus i...          100   \n",
       "366    city stayathome order also schedule allow the ...          100   \n",
       "411    inoculum dose important small exposure v large...          100   \n",
       "...                                                  ...          ...   \n",
       "15898   mandate stay at home order city influx covid ...          100   \n",
       "15978  remember recently month the orange idiot lamen...          100   \n",
       "16050  your congressperson the wuhanvirus make to ame...          100   \n",
       "16063  keep reading spring breaker come back infect r...          100   \n",
       "16128  fairfax county virginiaperhaps the richest one...          100   \n",
       "\n",
       "      final_keyword_match  \n",
       "154                 large  \n",
       "197                travel  \n",
       "276                 large  \n",
       "366          nonessential  \n",
       "411                 large  \n",
       "...                   ...  \n",
       "15898              travel  \n",
       "15978         unnecessary  \n",
       "16050              travel  \n",
       "16063              travel  \n",
       "16128               large  \n",
       "\n",
       "[195 rows x 5 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interim_sd[interim_sd['final_score'] == 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#interim_sd[interim_sd['final_score'] == 100].to_csv('interim_sd.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
