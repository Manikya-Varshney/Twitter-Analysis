{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!pip install -U python-Levenshtein"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import sys \n",
    "!{sys.executable} -m pip install python-Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/manikya_varshney/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "import string\n",
    "\n",
    "import nltk\n",
    "from nltk import pos_tag, pos_tag_sents\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english')) - set(['at', 'do', 'your', 'from', 'to', 'out', 'no', 'the'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/manikya_varshney/Documents/Python/Yale/All Graphs/final_processed_data/final_processed_april01.csv'\n",
    "data = pd.read_csv(path, low_memory=False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "silver lining\n",
    "optimistic\n",
    "hope\n",
    "bright side\n",
    "Safe\n",
    "#togetherapart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = ['silver lining' , 'optimistic', 'hope', \n",
    "            'bright side', 'Safe', '#togetherapart']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Casing (Upper or lower case)\n",
    "##### 2. Noise Removal (Removal of punctuation, white spaces, special characters, HTML tags)\n",
    "##### 3. Tokenization (Tweets to tokens i.e. words seprated by spaces)\n",
    "##### 4. Stopword Removal\n",
    "##### 5. Text Normalization (Stemming and Lemmatization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert to lower\n",
    "for i in range(len(keywords)): \n",
    "    keywords[i] = keywords[i].lower()\n",
    "\n",
    "#Remove punctuations   \n",
    "for i in range(len(keywords)): \n",
    "    keywords[i] = keywords[i].translate(str.maketrans('','',string.punctuation))\n",
    "\n",
    "#More cleaning\n",
    "for i in range(len(keywords)): \n",
    "    keywords[i] = keywords[i].replace('/[^a-zA-Z0-9 ]/g', '').replace('\\n',' ').strip('“').strip('“').strip('’').lstrip(' ').rstrip(' ')\n",
    "\n",
    "#Tokenize\n",
    "#keywords_tokens = [sub.split() for sub in keywords] \n",
    "\n",
    "#Remove stop words\n",
    "def remove_stopwords(data):\n",
    "    output_array=[]\n",
    "    for sentence in data:\n",
    "        temp_list=[]\n",
    "        for word in sentence.split():\n",
    "            if word not in stop_words:\n",
    "                temp_list.append(word)\n",
    "        output_array.append(' '.join(temp_list))\n",
    "    return output_array\n",
    "\n",
    "keywords_filtered=remove_stopwords(keywords)\n",
    "\n",
    "#Stemming\n",
    "ps = PorterStemmer()\n",
    "keywords_stem = [[ps.stem(word) for word in sentence.split(\" \")] for sentence in keywords_filtered]\n",
    "keywords_stem = [\" \".join(sentence) for sentence in keywords_stem]\n",
    "\n",
    "#Lemmetizing\n",
    "\n",
    "#POSTags\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "keywords_lem = [[lemmatizer.lemmatize(y, get_wordnet_pos(y)) for y in sentence.split(\" \")] for sentence in keywords_filtered]\n",
    "keywords_final = [\" \".join(sentence) for sentence in keywords_lem]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fuzzy Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['final'] = data['final'].apply(str)\n",
    "choices = data['final'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    " def fuzzy_m(row):\n",
    "        keyword_match, score = process.extractOne(row['final'], keywords_final, scorer = fuzz.partial_ratio)\n",
    "        row['final_score'] = score\n",
    "        row['final_keyword_match'] = keyword_match\n",
    "        return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "interim_pe = data.apply(fuzzy_m, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>text_duplicate</th>\n",
       "      <th>final</th>\n",
       "      <th>final_score</th>\n",
       "      <th>final_keyword_match</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28433993</td>\n",
       "      <td>As Chicago households fill out 2020 census dur...</td>\n",
       "      <td>chicago household fill out 2020 census coronav...</td>\n",
       "      <td>50</td>\n",
       "      <td>safe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>101043870</td>\n",
       "      <td>Most of the press corps is very busy covering ...</td>\n",
       "      <td>the press corp busy cover the really important...</td>\n",
       "      <td>64</td>\n",
       "      <td>bright side</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16475267</td>\n",
       "      <td>@realDonaldTrump Mar 10–he was very specific: ...</td>\n",
       "      <td>mar 10–he specific  google 1700 engineer work ...</td>\n",
       "      <td>55</td>\n",
       "      <td>bright side</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16475267</td>\n",
       "      <td>@realDonaldTrump , You were caustic &amp;amp; sarc...</td>\n",
       "      <td>caustic amp sarcastic the rollout  come even f...</td>\n",
       "      <td>50</td>\n",
       "      <td>hope</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>345397708</td>\n",
       "      <td>Florida nears 8,000 coronavirus cases, as stat...</td>\n",
       "      <td>florida nears 8000 coronavirus case state repo...</td>\n",
       "      <td>54</td>\n",
       "      <td>togetherapart</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16185</th>\n",
       "      <td>751644712570847236</td>\n",
       "      <td>If in 1938 Turkey’s government could produce a...</td>\n",
       "      <td>1938 turkey  government could produce health m...</td>\n",
       "      <td>50</td>\n",
       "      <td>hope</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16186</th>\n",
       "      <td>286628737</td>\n",
       "      <td>Sandy Medford was a friend of my family for de...</td>\n",
       "      <td>sandy medford friend family decade suspect hea...</td>\n",
       "      <td>55</td>\n",
       "      <td>bright side</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16187</th>\n",
       "      <td>361010205</td>\n",
       "      <td>One month ago Trump claimed the number of Amer...</td>\n",
       "      <td>one month ago trump claimed the number america...</td>\n",
       "      <td>50</td>\n",
       "      <td>safe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16188</th>\n",
       "      <td>481389842</td>\n",
       "      <td>Yes. This. Especially if a substantial proport...</td>\n",
       "      <td>yes especially substantial proportion workforc...</td>\n",
       "      <td>50</td>\n",
       "      <td>hope</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16189</th>\n",
       "      <td>11347222</td>\n",
       "      <td>I'm sticking to my original lowball prediction...</td>\n",
       "      <td>im stick to original lowball prediction at lea...</td>\n",
       "      <td>55</td>\n",
       "      <td>silver line</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16190 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  user_id                                     text_duplicate  \\\n",
       "0                28433993  As Chicago households fill out 2020 census dur...   \n",
       "1               101043870  Most of the press corps is very busy covering ...   \n",
       "2                16475267  @realDonaldTrump Mar 10–he was very specific: ...   \n",
       "3                16475267  @realDonaldTrump , You were caustic &amp; sarc...   \n",
       "4               345397708  Florida nears 8,000 coronavirus cases, as stat...   \n",
       "...                   ...                                                ...   \n",
       "16185  751644712570847236  If in 1938 Turkey’s government could produce a...   \n",
       "16186           286628737  Sandy Medford was a friend of my family for de...   \n",
       "16187           361010205  One month ago Trump claimed the number of Amer...   \n",
       "16188           481389842  Yes. This. Especially if a substantial proport...   \n",
       "16189            11347222  I'm sticking to my original lowball prediction...   \n",
       "\n",
       "                                                   final  final_score  \\\n",
       "0      chicago household fill out 2020 census coronav...           50   \n",
       "1      the press corp busy cover the really important...           64   \n",
       "2      mar 10–he specific  google 1700 engineer work ...           55   \n",
       "3      caustic amp sarcastic the rollout  come even f...           50   \n",
       "4      florida nears 8000 coronavirus case state repo...           54   \n",
       "...                                                  ...          ...   \n",
       "16185  1938 turkey  government could produce health m...           50   \n",
       "16186  sandy medford friend family decade suspect hea...           55   \n",
       "16187  one month ago trump claimed the number america...           50   \n",
       "16188  yes especially substantial proportion workforc...           50   \n",
       "16189  im stick to original lowball prediction at lea...           55   \n",
       "\n",
       "      final_keyword_match  \n",
       "0                    safe  \n",
       "1             bright side  \n",
       "2             bright side  \n",
       "3                    hope  \n",
       "4           togetherapart  \n",
       "...                   ...  \n",
       "16185                hope  \n",
       "16186         bright side  \n",
       "16187                safe  \n",
       "16188                hope  \n",
       "16189         silver line  \n",
       "\n",
       "[16190 rows x 5 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interim_pe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "denominator = interim_pe.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "interim_pe['final_score'] = interim_pe['final_score'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "interim_pe = interim_pe[interim_pe['final_score'] == 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>text_duplicate</th>\n",
       "      <th>final</th>\n",
       "      <th>final_score</th>\n",
       "      <th>final_keyword_match</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>823354136863252481</td>\n",
       "      <td>It is one thing to be optimistic or reassuring...</td>\n",
       "      <td>one thing to optimistic reassure entirely diff...</td>\n",
       "      <td>100</td>\n",
       "      <td>optimistic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>29292924</td>\n",
       "      <td>Happy #NationalPoetryMonth - new pandemic poet...</td>\n",
       "      <td>happy nationalpoetrymonth new pandemic poetry ...</td>\n",
       "      <td>100</td>\n",
       "      <td>safe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>92139131</td>\n",
       "      <td>I hope you get Coronavirus</td>\n",
       "      <td>hope get coronavirus</td>\n",
       "      <td>100</td>\n",
       "      <td>hope</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>298798289</td>\n",
       "      <td>Yeah! #Nantucket\\nSTAY HOME SAFE LIVES\\nCenter...</td>\n",
       "      <td>yeah nantucket stay home safe life center dise...</td>\n",
       "      <td>100</td>\n",
       "      <td>safe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>911365226</td>\n",
       "      <td>@Zigmanfreud We often miss mild cases of the f...</td>\n",
       "      <td>often miss mild case the flu well hope the fin...</td>\n",
       "      <td>100</td>\n",
       "      <td>hope</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16097</th>\n",
       "      <td>96454964</td>\n",
       "      <td>Praying that our healthcare workers, on the fr...</td>\n",
       "      <td>pray healthcare worker the frontlines the figh...</td>\n",
       "      <td>100</td>\n",
       "      <td>safe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16153</th>\n",
       "      <td>805822016469110784</td>\n",
       "      <td>You really have to be unhinged to do this to y...</td>\n",
       "      <td>really to unhinged to do to your follower hope...</td>\n",
       "      <td>100</td>\n",
       "      <td>hope</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16155</th>\n",
       "      <td>700473181308481536</td>\n",
       "      <td>@ASRA_Society @ESRA_Society \\n\\n#COVIDBlocks\\n...</td>\n",
       "      <td>covidblocks choose the right procedure vigilan...</td>\n",
       "      <td>100</td>\n",
       "      <td>safe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16160</th>\n",
       "      <td>113667535</td>\n",
       "      <td>MI has been home for me for last 5 years , i p...</td>\n",
       "      <td>mi home last 5 year pray amd declare the goodn...</td>\n",
       "      <td>100</td>\n",
       "      <td>safe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16170</th>\n",
       "      <td>1265999760</td>\n",
       "      <td>@TheAtlantic piece = spot on. We were given no...</td>\n",
       "      <td>piece spot give notice note go government give...</td>\n",
       "      <td>100</td>\n",
       "      <td>bright side</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  user_id                                     text_duplicate  \\\n",
       "14     823354136863252481  It is one thing to be optimistic or reassuring...   \n",
       "27               29292924  Happy #NationalPoetryMonth - new pandemic poet...   \n",
       "33               92139131                         I hope you get Coronavirus   \n",
       "49              298798289  Yeah! #Nantucket\\nSTAY HOME SAFE LIVES\\nCenter...   \n",
       "53              911365226  @Zigmanfreud We often miss mild cases of the f...   \n",
       "...                   ...                                                ...   \n",
       "16097            96454964  Praying that our healthcare workers, on the fr...   \n",
       "16153  805822016469110784  You really have to be unhinged to do this to y...   \n",
       "16155  700473181308481536  @ASRA_Society @ESRA_Society \\n\\n#COVIDBlocks\\n...   \n",
       "16160           113667535  MI has been home for me for last 5 years , i p...   \n",
       "16170          1265999760  @TheAtlantic piece = spot on. We were given no...   \n",
       "\n",
       "                                                   final  final_score  \\\n",
       "14     one thing to optimistic reassure entirely diff...          100   \n",
       "27     happy nationalpoetrymonth new pandemic poetry ...          100   \n",
       "33                                  hope get coronavirus          100   \n",
       "49     yeah nantucket stay home safe life center dise...          100   \n",
       "53     often miss mild case the flu well hope the fin...          100   \n",
       "...                                                  ...          ...   \n",
       "16097  pray healthcare worker the frontlines the figh...          100   \n",
       "16153  really to unhinged to do to your follower hope...          100   \n",
       "16155  covidblocks choose the right procedure vigilan...          100   \n",
       "16160  mi home last 5 year pray amd declare the goodn...          100   \n",
       "16170  piece spot give notice note go government give...          100   \n",
       "\n",
       "      final_keyword_match  \n",
       "14             optimistic  \n",
       "27                   safe  \n",
       "33                   hope  \n",
       "49                   safe  \n",
       "53                   hope  \n",
       "...                   ...  \n",
       "16097                safe  \n",
       "16153                hope  \n",
       "16155                safe  \n",
       "16160                safe  \n",
       "16170         bright side  \n",
       "\n",
       "[569 rows x 5 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interim_pe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerator = interim_pe.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03514515132798023\n"
     ]
    }
   ],
   "source": [
    "proportion = (numerator/denominator)\n",
    "print(proportion)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
