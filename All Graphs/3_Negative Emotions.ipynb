{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!pip install -U python-Levenshtein"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import sys \n",
    "!{sys.executable} -m pip install python-Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/manikya_varshney/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "import string\n",
    "\n",
    "import nltk\n",
    "from nltk import pos_tag, pos_tag_sents\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english')) - set(['at', 'do', 'your', 'from', 'to', 'out', 'no', 'the'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/manikya_varshney/Documents/Python/Yale/All Graphs/final_processed_april01.csv'\n",
    "data = pd.read_csv(path, low_memory=False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "bored\n",
    "lonely\n",
    "stress\n",
    "anxiety\n",
    "scared\n",
    "worry\n",
    "end\n",
    "cabin fever\n",
    "#sideeffectsofquarantinelife\n",
    "tissue paper\n",
    "toilet paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = ['bored' , 'lonely', 'stress', \n",
    "            'anxiety', 'scared', 'worry', 'end', 'cabin fever',\n",
    "            '#sideeffectsofquarantinelife', 'tissue paper', 'toilet paper']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Casing (Upper or lower case)\n",
    "##### 2. Noise Removal (Removal of punctuation, white spaces, special characters, HTML tags)\n",
    "##### 3. Tokenization (Tweets to tokens i.e. words seprated by spaces)\n",
    "##### 4. Stopword Removal\n",
    "##### 5. Text Normalization (Stemming and Lemmatization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert to lower\n",
    "for i in range(len(keywords)): \n",
    "    keywords[i] = keywords[i].lower()\n",
    "\n",
    "#Remove punctuations   \n",
    "for i in range(len(keywords)): \n",
    "    keywords[i] = keywords[i].translate(str.maketrans('','',string.punctuation))\n",
    "\n",
    "#More cleaning\n",
    "for i in range(len(keywords)): \n",
    "    keywords[i] = keywords[i].replace('/[^a-zA-Z0-9 ]/g', '').replace('\\n',' ').strip('“').strip('“').strip('’').lstrip(' ').rstrip(' ')\n",
    "\n",
    "#Tokenize\n",
    "#keywords_tokens = [sub.split() for sub in keywords] \n",
    "\n",
    "#Remove stop words\n",
    "def remove_stopwords(data):\n",
    "    output_array=[]\n",
    "    for sentence in data:\n",
    "        temp_list=[]\n",
    "        for word in sentence.split():\n",
    "            if word not in stop_words:\n",
    "                temp_list.append(word)\n",
    "        output_array.append(' '.join(temp_list))\n",
    "    return output_array\n",
    "\n",
    "keywords_filtered=remove_stopwords(keywords)\n",
    "\n",
    "#Stemming\n",
    "ps = PorterStemmer()\n",
    "keywords_stem = [[ps.stem(word) for word in sentence.split(\" \")] for sentence in keywords_filtered]\n",
    "keywords_stem = [\" \".join(sentence) for sentence in keywords_stem]\n",
    "\n",
    "#Lemmetizing\n",
    "\n",
    "#POSTags\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "keywords_lem = [[lemmatizer.lemmatize(y, get_wordnet_pos(y)) for y in sentence.split(\" \")] for sentence in keywords_filtered]\n",
    "keywords_final = [\" \".join(sentence) for sentence in keywords_lem]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fuzzy Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['final'] = data['final'].apply(str)\n",
    "choices = data['final'].tolist()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def fuzzy_m(row):\n",
    "    matches = process.extract(row['final'], keywords_final, limit=3)\n",
    "    exactMatch = matches[0][1] == 90\n",
    "    row['Match 1'] = matches[0][0]\n",
    "    row['Match 1 Score'] = matches[0][1]\n",
    "    row['Match 2'] = \"\" if exactMatch else matches[1][0]\n",
    "    row['Match 2 Score'] = \"\" if exactMatch else matches[1][1]\n",
    "    row['Match 3'] = \"\" if exactMatch else matches[2][0]\n",
    "    row['Match 3 Score'] = \"\" if exactMatch else matches[2][1]\n",
    "    return row    "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "interim_ne = data.apply(fuzzy_m, axis=1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "interim_ne"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "interim_ne['Match 1 Score'] = interim_ne['Match 1 Score'].astype(int)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "interim_ne[interim_ne['Match 1 Score'] == 90].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    " def fuzzy_m(row):\n",
    "        keyword_match, score = process.extractOne(row['final'], keywords_final, scorer = fuzz.partial_ratio)\n",
    "        row['final_score'] = score\n",
    "        row['final_keyword_match'] = keyword_match\n",
    "        return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "interim_ne = data.apply(fuzzy_m, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>text_duplicate</th>\n",
       "      <th>final</th>\n",
       "      <th>final_score</th>\n",
       "      <th>final_keyword_match</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28433993</td>\n",
       "      <td>As Chicago households fill out 2020 census dur...</td>\n",
       "      <td>chicago household fill out 2020 census coronav...</td>\n",
       "      <td>75</td>\n",
       "      <td>bore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>101043870</td>\n",
       "      <td>Most of the press corps is very busy covering ...</td>\n",
       "      <td>the press corp busy cover the really important...</td>\n",
       "      <td>50</td>\n",
       "      <td>bore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16475267</td>\n",
       "      <td>@realDonaldTrump Mar 10–he was very specific: ...</td>\n",
       "      <td>mar 10–he specific  google 1700 engineer work ...</td>\n",
       "      <td>100</td>\n",
       "      <td>end</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16475267</td>\n",
       "      <td>@realDonaldTrump , You were caustic &amp;amp; sarc...</td>\n",
       "      <td>caustic amp sarcastic the rollout  come even f...</td>\n",
       "      <td>75</td>\n",
       "      <td>scar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>345397708</td>\n",
       "      <td>Florida nears 8,000 coronavirus cases, as stat...</td>\n",
       "      <td>florida nears 8000 coronavirus case state repo...</td>\n",
       "      <td>67</td>\n",
       "      <td>end</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16185</th>\n",
       "      <td>751644712570847236</td>\n",
       "      <td>If in 1938 Turkey’s government could produce a...</td>\n",
       "      <td>1938 turkey  government could produce health m...</td>\n",
       "      <td>50</td>\n",
       "      <td>bore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16186</th>\n",
       "      <td>286628737</td>\n",
       "      <td>Sandy Medford was a friend of my family for de...</td>\n",
       "      <td>sandy medford friend family decade suspect hea...</td>\n",
       "      <td>100</td>\n",
       "      <td>end</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16187</th>\n",
       "      <td>361010205</td>\n",
       "      <td>One month ago Trump claimed the number of Amer...</td>\n",
       "      <td>one month ago trump claimed the number america...</td>\n",
       "      <td>67</td>\n",
       "      <td>end</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16188</th>\n",
       "      <td>481389842</td>\n",
       "      <td>Yes. This. Especially if a substantial proport...</td>\n",
       "      <td>yes especially substantial proportion workforc...</td>\n",
       "      <td>67</td>\n",
       "      <td>end</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16189</th>\n",
       "      <td>11347222</td>\n",
       "      <td>I'm sticking to my original lowball prediction...</td>\n",
       "      <td>im stick to original lowball prediction at lea...</td>\n",
       "      <td>67</td>\n",
       "      <td>end</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16190 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  user_id                                     text_duplicate  \\\n",
       "0                28433993  As Chicago households fill out 2020 census dur...   \n",
       "1               101043870  Most of the press corps is very busy covering ...   \n",
       "2                16475267  @realDonaldTrump Mar 10–he was very specific: ...   \n",
       "3                16475267  @realDonaldTrump , You were caustic &amp; sarc...   \n",
       "4               345397708  Florida nears 8,000 coronavirus cases, as stat...   \n",
       "...                   ...                                                ...   \n",
       "16185  751644712570847236  If in 1938 Turkey’s government could produce a...   \n",
       "16186           286628737  Sandy Medford was a friend of my family for de...   \n",
       "16187           361010205  One month ago Trump claimed the number of Amer...   \n",
       "16188           481389842  Yes. This. Especially if a substantial proport...   \n",
       "16189            11347222  I'm sticking to my original lowball prediction...   \n",
       "\n",
       "                                                   final  final_score  \\\n",
       "0      chicago household fill out 2020 census coronav...           75   \n",
       "1      the press corp busy cover the really important...           50   \n",
       "2      mar 10–he specific  google 1700 engineer work ...          100   \n",
       "3      caustic amp sarcastic the rollout  come even f...           75   \n",
       "4      florida nears 8000 coronavirus case state repo...           67   \n",
       "...                                                  ...          ...   \n",
       "16185  1938 turkey  government could produce health m...           50   \n",
       "16186  sandy medford friend family decade suspect hea...          100   \n",
       "16187  one month ago trump claimed the number america...           67   \n",
       "16188  yes especially substantial proportion workforc...           67   \n",
       "16189  im stick to original lowball prediction at lea...           67   \n",
       "\n",
       "      final_keyword_match  \n",
       "0                    bore  \n",
       "1                    bore  \n",
       "2                     end  \n",
       "3                    scar  \n",
       "4                     end  \n",
       "...                   ...  \n",
       "16185                bore  \n",
       "16186                 end  \n",
       "16187                 end  \n",
       "16188                 end  \n",
       "16189                 end  \n",
       "\n",
       "[16190 rows x 5 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interim_ne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "interim_ne['final_score'] = interim_ne['final_score'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>text_duplicate</th>\n",
       "      <th>final</th>\n",
       "      <th>final_score</th>\n",
       "      <th>final_keyword_match</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16475267</td>\n",
       "      <td>@realDonaldTrump Mar 10–he was very specific: ...</td>\n",
       "      <td>mar 10–he specific  google 1700 engineer work ...</td>\n",
       "      <td>100</td>\n",
       "      <td>end</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>394499101</td>\n",
       "      <td>AOC slams Cuomo's decision to suspend New York...</td>\n",
       "      <td>aoc slam cuomos decision to suspend new yorker...</td>\n",
       "      <td>100</td>\n",
       "      <td>end</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>394499101</td>\n",
       "      <td>AOC slams Cuomo's decision to suspend New York...</td>\n",
       "      <td>aoc slam cuomos decision to suspend new yorker...</td>\n",
       "      <td>100</td>\n",
       "      <td>end</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>95027069</td>\n",
       "      <td>How are we at #jpcarpet spending our #quaranti...</td>\n",
       "      <td>at jpcarpet spending quarantine building close...</td>\n",
       "      <td>100</td>\n",
       "      <td>end</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>15201225</td>\n",
       "      <td>NEW BLOG! \"10 Self-Care Tips during a Stay-At-...</td>\n",
       "      <td>new blog 10 selfcare tip stayathome order frie...</td>\n",
       "      <td>100</td>\n",
       "      <td>end</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16116</th>\n",
       "      <td>2233488978</td>\n",
       "      <td>“For weeks, experts and advocates have been ra...</td>\n",
       "      <td>week expert advocate raise alarm the coronavir...</td>\n",
       "      <td>100</td>\n",
       "      <td>stress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16137</th>\n",
       "      <td>867183369976766465</td>\n",
       "      <td>What?! The People of @Australia @COVID_Austral...</td>\n",
       "      <td>the people  allow to defend amp family from ph...</td>\n",
       "      <td>100</td>\n",
       "      <td>end</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16156</th>\n",
       "      <td>396214595</td>\n",
       "      <td>Trump sent workers without protection or train...</td>\n",
       "      <td>trump sent worker without protection training ...</td>\n",
       "      <td>100</td>\n",
       "      <td>end</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16168</th>\n",
       "      <td>566883318</td>\n",
       "      <td>Helps me to see it this way. #Mitchplease\\nCOV...</td>\n",
       "      <td>help to see way mitchplease covid19 cdc warn j...</td>\n",
       "      <td>100</td>\n",
       "      <td>end</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16186</th>\n",
       "      <td>286628737</td>\n",
       "      <td>Sandy Medford was a friend of my family for de...</td>\n",
       "      <td>sandy medford friend family decade suspect hea...</td>\n",
       "      <td>100</td>\n",
       "      <td>end</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>975 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  user_id                                     text_duplicate  \\\n",
       "2                16475267  @realDonaldTrump Mar 10–he was very specific: ...   \n",
       "10              394499101  AOC slams Cuomo's decision to suspend New York...   \n",
       "11              394499101  AOC slams Cuomo's decision to suspend New York...   \n",
       "19               95027069  How are we at #jpcarpet spending our #quaranti...   \n",
       "42               15201225  NEW BLOG! \"10 Self-Care Tips during a Stay-At-...   \n",
       "...                   ...                                                ...   \n",
       "16116          2233488978  “For weeks, experts and advocates have been ra...   \n",
       "16137  867183369976766465  What?! The People of @Australia @COVID_Austral...   \n",
       "16156           396214595  Trump sent workers without protection or train...   \n",
       "16168           566883318  Helps me to see it this way. #Mitchplease\\nCOV...   \n",
       "16186           286628737  Sandy Medford was a friend of my family for de...   \n",
       "\n",
       "                                                   final  final_score  \\\n",
       "2      mar 10–he specific  google 1700 engineer work ...          100   \n",
       "10     aoc slam cuomos decision to suspend new yorker...          100   \n",
       "11     aoc slam cuomos decision to suspend new yorker...          100   \n",
       "19     at jpcarpet spending quarantine building close...          100   \n",
       "42     new blog 10 selfcare tip stayathome order frie...          100   \n",
       "...                                                  ...          ...   \n",
       "16116  week expert advocate raise alarm the coronavir...          100   \n",
       "16137  the people  allow to defend amp family from ph...          100   \n",
       "16156  trump sent worker without protection training ...          100   \n",
       "16168  help to see way mitchplease covid19 cdc warn j...          100   \n",
       "16186  sandy medford friend family decade suspect hea...          100   \n",
       "\n",
       "      final_keyword_match  \n",
       "2                     end  \n",
       "10                    end  \n",
       "11                    end  \n",
       "19                    end  \n",
       "42                    end  \n",
       "...                   ...  \n",
       "16116              stress  \n",
       "16137                 end  \n",
       "16156                 end  \n",
       "16168                 end  \n",
       "16186                 end  \n",
       "\n",
       "[975 rows x 5 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interim_ne[interim_ne['final_score'] == 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#interim_ne[interim_ne['final_score'] == 100].to_csv('interim_ne.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
