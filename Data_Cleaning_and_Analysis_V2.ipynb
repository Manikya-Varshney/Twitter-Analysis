{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JHBSrnVsmQT1"
   },
   "source": [
    "\n",
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iN58fBbFo48-"
   },
   "outputs": [],
   "source": [
    "# Imports for GDrive\n",
    "!pip install -U -q PyDrive\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from google.colab import auth\n",
    "from oauth2client.client import GoogleCredentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XssQCdN5pE03"
   },
   "outputs": [],
   "source": [
    "# Mounting the drive\n",
    "auth.authenticate_user()\n",
    "gauth = GoogleAuth()\n",
    "gauth.credentials = GoogleCredentials.get_application_default()\n",
    "drive = GoogleDrive(gauth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3zJK_ICLpTmX"
   },
   "outputs": [],
   "source": [
    "#downloaded = drive.CreateFile({'id':'https://drive.google.com/file/d/17dnH3TDdLmxos83OTHEtXBCD5Tmu-L_k/view?usp=sharing'}) # replace the id with id of file you want to access\n",
    "#downloaded.GetContentFile('h01-20201001-20201008.zip') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QOdpxl2-w7sR",
    "outputId": "94499167-2ecf-416c-c6ce-b153f5acbd91"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1xtF___3xLV3",
    "outputId": "152e3328-6a9b-4d40-b40b-959df2d60dfc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BPPC Acads']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.listdir('/content/gdrive/Shared drives/') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XiUQenL10eNC",
    "outputId": "a65897d7-7f05-4bab-ee84-a170cb2bd9d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/anaconda3/envs/twitter/lib/python3.8/site-packages (1.3.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/anaconda3/envs/twitter/lib/python3.8/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /opt/anaconda3/envs/twitter/lib/python3.8/site-packages (from pandas) (1.21.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/anaconda3/envs/twitter/lib/python3.8/site-packages (from pandas) (2021.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/twitter/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas) (1.16.0)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/envs/twitter/lib/python3.8/site-packages (1.21.2)\n",
      "Requirement already satisfied: nltk in /opt/anaconda3/envs/twitter/lib/python3.8/site-packages (3.6.2)\n",
      "Requirement already satisfied: joblib in /opt/anaconda3/envs/twitter/lib/python3.8/site-packages (from nltk) (1.0.1)\n",
      "Requirement already satisfied: click in /opt/anaconda3/envs/twitter/lib/python3.8/site-packages (from nltk) (8.0.1)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/envs/twitter/lib/python3.8/site-packages (from nltk) (4.62.2)\n",
      "Requirement already satisfied: regex in /opt/anaconda3/envs/twitter/lib/python3.8/site-packages (from nltk) (2021.8.28)\n",
      "Requirement already satisfied: multiprocess in /opt/anaconda3/envs/twitter/lib/python3.8/site-packages (0.70.12.2)\n",
      "Requirement already satisfied: dill>=0.3.4 in /opt/anaconda3/envs/twitter/lib/python3.8/site-packages (from multiprocess) (0.3.4)\n",
      "Requirement already satisfied: emoji in /opt/anaconda3/envs/twitter/lib/python3.8/site-packages (1.4.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/priyanshkedia/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/priyanshkedia/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/priyanshkedia/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/priyanshkedia/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n",
    "!pip install numpy\n",
    "!pip install nltk\n",
    "\n",
    "# Import statements\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import csv\n",
    "import re\n",
    "import string\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "!pip install multiprocess\n",
    "import multiprocess\n",
    "from multiprocess import Pool, Process\n",
    "\n",
    "!pip install emoji\n",
    "import emoji\n",
    "\n",
    "import nltk\n",
    "from nltk import pos_tag, pos_tag_sents\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "stop_words = set(stopwords.words('english')) - set(['at', 'do', 'your', 'from', 'to', 'out', 'no', 'the'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "G7-qNi6n_pMM"
   },
   "outputs": [],
   "source": [
    "# Extract zip file\n",
    "\n",
    "def extract_zips(filename, temp_folder):\n",
    "  zip_ref = zipfile.ZipFile(filename, 'r')\n",
    "  zip_ref.extractall(temp_folder)\n",
    "  zip_ref.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "OHS5daIFGbLI"
   },
   "outputs": [],
   "source": [
    "def create_filename(FILENAME, TEMP_FOLDER):\n",
    "  FILES_NAMES = FILENAME.split(\".\")[0].split(\"/\")[-1]\n",
    "  FILES_NAMES = FILES_NAMES.rsplit(\"-\", 1)[0]\n",
    "  # FILES_NAMES = os.path.join(TEMP_FOLDER, FILES_NAMES)\n",
    "  return FILES_NAMES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "xvgg28WxGj1R"
   },
   "outputs": [],
   "source": [
    "def get_date_string(FILESNAMES):\n",
    "  date_string = FILESNAMES.split(\"-\")[-1]\n",
    "  return date_string\n",
    "\n",
    "def strip_date(FILESNAMES):\n",
    "  date_string = get_date_string(FILESNAMES)\n",
    "  ob = datetime.strptime(date_string, \"%Y%m%d\")\n",
    "  date_to_analyse = ob.strftime(\"%a %b %d\")\n",
    "  return date_to_analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Axem73RZAAkA"
   },
   "outputs": [],
   "source": [
    "# Constants\n",
    "FILENAME = \"h01-20200818-10files.zip\"\n",
    "TEMP_FOLDER = \"/tmp\"\n",
    "FILES_NAMES = create_filename(FILENAME, TEMP_FOLDER)\n",
    "date_to_analyse = strip_date(FILES_NAMES)\n",
    "# print(FILES_NAMES, date_to_analyse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "ZuK9Y_DSL0cL"
   },
   "outputs": [],
   "source": [
    "# Keep data in english only\n",
    "def remove_other_langs(data):\n",
    "  data = data[data['lang'] == 'en'].reset_index(drop=True)\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Xwsew-o1MzvI"
   },
   "outputs": [],
   "source": [
    "# Keep specific date\n",
    "def remove_other_dates(data, date_to_analyse):\n",
    "  data = data[data['created_at'].str[:10] == date_to_analyse].reset_index(drop=True)\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "fnpiNeSksPHD"
   },
   "outputs": [],
   "source": [
    "# Creating the RT Column\n",
    "def create_rt_column(data):\n",
    "  data['RT'] = data['text'].str[:2]=='RT'\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "jDie85V6J4IJ"
   },
   "outputs": [],
   "source": [
    "# Parse CSV data\n",
    "def parse_data_from_file(filename, date_to_analyse):\n",
    "  data = pd.read_csv(filename, index_col = None, header=0, engine = 'python')\n",
    "  data = remove_other_langs(data)\n",
    "  data = remove_other_dates(data, date_to_analyse)\n",
    "  data = create_rt_column(data)\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "oksZ2gyHLTYM"
   },
   "outputs": [],
   "source": [
    "# Parse all files of the same date\n",
    "def parse_all_files(TEMP_FOLDER, FILES_NAMES, date_to_analyse):\n",
    "  files = os.listdir(TEMP_FOLDER)\n",
    "  new_data = []\n",
    "  for file in files:\n",
    "    if file.startswith(FILES_NAMES):\n",
    "      parsed_data = parse_data_from_file(os.path.join(TEMP_FOLDER, file), date_to_analyse)\n",
    "      new_data.append(parsed_data)\n",
    "  return pd.concat(new_data, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "d6dvi4iZuO7z"
   },
   "outputs": [],
   "source": [
    "def get_text_or_extended_text(data, added_ids):\n",
    "  data[\"FINAL_TEXT\"] = np.where(data['extended_tweet_full_text'].notnull(), data[\"extended_tweet_full_text\"], data[\"text\"])\n",
    "  id = data[\"id\"].to_dict()\n",
    "  added_ids.update(id)\n",
    "  return data[\"FINAL_TEXT\"]\n",
    "\n",
    "def get_quoted_text(data, added_ids):\n",
    "  data[\"FINAL_TEXT\"] = np.where(data['QT_full_text'].notnull(), data[\"QT_full_text\"], data[\"QT_text\"])\n",
    "  added_ids.update(data[\"id\"].to_dict())\n",
    "  added_ids.update(data[\"QT_id\"].to_dict())\n",
    "  return data[\"FINAL_TEXT\"]\n",
    "\n",
    "def get_text_or_full_text_rt(data, added_ids):\n",
    "  data[\"FINAL_TEXT\"] = np.where(data['RT_full_text'].notnull(), data[\"RT_full_text\"], data[\"RT_text\"])\n",
    "  added_ids.update(data[\"id\"].to_dict())\n",
    "  added_ids.update(data[\"RT_id\"].to_dict())\n",
    "  return data[\"FINAL_TEXT\"]\n",
    "\n",
    "def get_quote_rt_full(data, added_ids):\n",
    "  data[\"FINAL_TEXT\"] = get_text_or_full_text_rt(data, added_ids) + get_quoted_text(data, added_ids)\n",
    "  return data[\"FINAL_TEXT\"]\n",
    "\n",
    "\n",
    "\n",
    "def get_quote_rt(data, is_quote, is_rt):\n",
    "  data = data.loc[(data['is_quote_tweet'] == is_quote) & (data['RT'] == is_rt)]\n",
    "\n",
    "  if not is_quote and not is_rt:\n",
    "    added_ids.update(dict(zip(data[\"id\"], data[\"is_quote_tweet\"])))\n",
    "    data[\"FINAL_TEXT\"] = np.where(data['extended_tweet_full_text'].notnull(), data[\"extended_tweet_full_text\"], data[\"text\"])\n",
    "\n",
    "  if is_quote and not is_rt:\n",
    "    data[\"FINAL_TEXT\"] = np.where(data[\"QT_id\"].isin(added_ids.keys()), get_text_or_extended_text(data, added_ids), get_quoted_text(data, added_ids))\n",
    "\n",
    "  if not is_quote and is_rt:\n",
    "    data[\"FINAL_TEXT\"] = np.where(data[\"RT_id\"].isin(added_ids.keys()), added_ids.update(data['id'].to_dict()), get_text_or_full_text_rt(data, added_ids))\n",
    "    data = data[data[\"FINAL_TEXT\"].notna()]\n",
    "\n",
    "  if is_quote and is_rt:\n",
    "    data[\"FINAL_TEXT\"] = np.where((data[\"RT_id\"].isin(added_ids.keys())) & (data[\"QT_id\"].isin(added_ids.keys())), added_ids.update(data['id'].to_dict()), None)\n",
    "    data[\"FINAL_TEXT\"] = np.where((data[\"RT_id\"].isin(added_ids.keys())) & (~data[\"QT_id\"].isin(added_ids.keys())), get_quoted_text(data, added_ids), None)\n",
    "    data[\"FINAL_TEXT\"] = np.where((~data[\"RT_id\"].isin(added_ids.keys())) & (data[\"QT_id\"].isin(added_ids.keys())), get_text_or_full_text_rt(data, added_ids), None)\n",
    "    data[\"FINAL_TEXT\"] = np.where((~data[\"RT_id\"].isin(added_ids.keys())) & (~data[\"QT_id\"].isin(added_ids.keys())), get_quote_rt_full(data, added_ids), None)\n",
    "    data = data[data[\"FINAL_TEXT\"].notna()]\n",
    "\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H7R4r1VkAMtE",
    "outputId": "274d79f3-c140-425e-8971-01f7d2db0e32"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(112410, 80) <class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "# Driver code\n",
    "added_ids = {}\n",
    "extract_zips(FILENAME, TEMP_FOLDER)\n",
    "data = parse_all_files(TEMP_FOLDER, FILES_NAMES, date_to_analyse)\n",
    "print(data.shape, type(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "a0Ah-yD2q7yD"
   },
   "outputs": [],
   "source": [
    "# Check duplicates\n",
    "# print(data[\"text\"].nunique())\n",
    "# print(data[\"extended_tweet_full_text\"].nunique())\n",
    "# print(data[\"QT_full_text\"].nunique())\n",
    "# print(data[\"QT_text\"].nunique())\n",
    "# print(data[\"RT_full_text\"].nunique())\n",
    "# print(data[\"RT_text\"].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NMUvsAaLvwK-",
    "outputId": "b02a340c-174b-4bf9-c03a-47757be6e46f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1        @CarolineHutt @GodfreyOSWALD @littlemissj74 @S...\n",
      "4        This new platform is designed to be used by st...\n",
      "5        @Georgia_Levan @emmakennytv Sorry, correct. I ...\n",
      "6        @briduimhaoluala @redwenzen @DeeGilhawley @INT...\n",
      "9        @DaveHopper19 @baestheorum @randyhillier I've ...\n",
      "                               ...                        \n",
      "11135    FingerprintJS is looking for a remote Full-sta...\n",
      "11147    If the latest pandemic restrictions are causin...\n",
      "11148    Nice work on Multisystem Inflammatory Syndrome...\n",
      "11149    @pdobi Some people are pushing a plant extract...\n",
      "11150    Lunch time! Having lunch with some of our 'co-...\n",
      "Name: FINAL_TEXT, Length: 28689, dtype: object (28689, 81)\n"
     ]
    }
   ],
   "source": [
    "non_quote_non_rt = get_quote_rt(data, False, False)\n",
    "print(non_quote_non_rt[\"FINAL_TEXT\"], non_quote_non_rt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ntCphckKvwBC",
    "outputId": "6ffd56c4-3e71-4e4d-e7d1-625ce5919968"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2        $RLFTF $RLF\\nDHR Health Institute for Research...\n",
      "8        pregnancy and corona really neck and neck for ...\n",
      "14              This COVID shit lasting like a Honda Civic\n",
      "25       Covid-19's long-term effects, especially in yo...\n",
      "28       San Francisco, California: \\n\\nCOVID-19 denier...\n",
      "                               ...                        \n",
      "11180    UPDATE: COVID-19 cases in Ghana\\n\\nConfirmed -...\n",
      "11195    Never thought I would see the day I was sharin...\n",
      "11196    కావాల్సిన మందులు, పరికరాలు, ఇంజక్షన్లు, వెంటిల...\n",
      "11202    @realDonaldTrump The Trump Virus is making Ame...\n",
      "11228    Wuhan wave.\\n\\nPeople watch a performance as t...\n",
      "Name: FINAL_TEXT, Length: 9171, dtype: object (9171, 81)\n"
     ]
    }
   ],
   "source": [
    "quote_non_rt = get_quote_rt(data, True, False)\n",
    "print(quote_non_rt[\"FINAL_TEXT\"], quote_non_rt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "puDSndZUwJvE",
    "outputId": "437d6717-c669-42ca-c624-6a3281a4f289"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        Thousands attend pool party in Wuhan, China, t...\n",
      "10       Gov. Cuomo writing book about leading NY throu...\n",
      "12       BREAKING: 71 of Mississippi's 82 counties are ...\n",
      "15       .@NYGovCuomo's leadership was an American Cris...\n",
      "16       'Cashback of Rs 50,000 if you catch COVID-19 w...\n",
      "                               ...                        \n",
      "11189    Lakhs of students asking you for NOT taking Ex...\n",
      "11199    Join us Tuesday for an important back-to-schoo...\n",
      "11200    Due to the laughable \"recount\" of deaths in th...\n",
      "11220    As an ER doc, I've seen countless people die f...\n",
      "11223       https://t.co/K25QaQYTtzhttps://t.co/K25QaQYTtz\n",
      "Name: FINAL_TEXT, Length: 17873, dtype: object (17873, 81)\n"
     ]
    }
   ],
   "source": [
    "quote_rt = get_quote_rt(data, True, True)\n",
    "print(quote_rt[\"FINAL_TEXT\"], quote_rt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lrWIUjEHwJsc",
    "outputId": "49777c8c-8828-406a-d314-aa90c15dcb6e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3                 Corona Bouncer 🙄 https://t.co/W26X5A6yQa\n",
      "5        Taiwan's 1st coronavirus vaccine approved for ...\n",
      "7        I'll be happy when Kenyans can get the vaccine...\n",
      "9        Jyotsna Mohanty of BBSR GGP colony.Corona Cris...\n",
      "11              This COVID shit lasting like a Honda Civic\n",
      "                               ...                        \n",
      "11218    Florida reported 26,164 COVID-19 tests today w...\n",
      "11219    Happy that netball is back next month, but now...\n",
      "11221    The best \"own\" that Trump has is that Michelle...\n",
      "11226    Lessons from covid:\\n\\nLife is an absolute gif...\n",
      "11227    it’s a good thing tattoo parlors are closed du...\n",
      "Name: FINAL_TEXT, Length: 49887, dtype: object (49887, 81)\n"
     ]
    }
   ],
   "source": [
    "non_quote_rt = get_quote_rt(data, False, True)\n",
    "print(non_quote_rt[\"FINAL_TEXT\"], non_quote_rt.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HCpVf_aD1lV-"
   },
   "source": [
    "1. Casing (Upper or lower case)\n",
    "2. Noise Removal (Removal of punctuation, white spaces, special characters, HTML tags)\n",
    "3. Tokenization (Tweets to tokens i.e. words seprated by spaces)\n",
    "4. Stopword Removal\n",
    "5. Text Normalization (Stemming and Lemmatization) bold text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "e212hr-q2LKf"
   },
   "outputs": [],
   "source": [
    "#Removing emojis\n",
    "def demoji(text):\n",
    "  emoji_pattern = re.compile(\"[\"\n",
    "  u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "  u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "  u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "  u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "  u\"\\U00002702-\\U000027B0\"\n",
    "  u\"\\U000024C2-\\U0001F251\"\n",
    "  u\"\\U00010000-\\U0010ffff\"\n",
    "                        \"]+\", flags=re.UNICODE)\n",
    "  return(emoji_pattern.sub(r'', text))\n",
    "\n",
    "#POSTags\n",
    "def get_wordnet_pos(word):\n",
    "  \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "  tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "  tag_dict = {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV}\n",
    "  return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "#Remove URLs, user@, punctutions\n",
    "def df_cleaning(data_):\n",
    "  data_[\"FINAL_TEXT\"] = data_[\"FINAL_TEXT\"].astype(str).str.replace(r\"http\\S+| www\\S+| https\\S+| \\S+\\.com\\S+| \\S+\\.com| \\@[\\w]+\", \"\", regex=True)\n",
    "\n",
    "  # ##################-------Punctutions-------##################\n",
    "  data_[\"FINAL_TEXT\"] = data_[\"FINAL_TEXT\"].astype(str).str.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    " \n",
    "  # ##################-------More Cleaning-------##################\n",
    "  data_[\"FINAL_TEXT\"] = data_[\"FINAL_TEXT\"].astype(str).str.replace(\"/[^a-zA-Z0-9 ]/g\", \"\", regex=True).str.replace(\"\\n\",\" \", regex=True).str.replace(\"—\",\" \", regex=True).str.strip(\"“\").str.strip(\"”\").str.strip(\"’\").str.lstrip(\" \").str.rstrip(\" \")\n",
    "  \n",
    "  # ##################-------Emojis-------##################\n",
    "  data_[\"FINAL_TEXT\"] = data_[\"FINAL_TEXT\"].astype(str).apply(lambda x:demoji(x))\n",
    "  \n",
    "  # ##################-------Tokenizing-------##################\n",
    "  data_[\"FINAL_TEXT\"] = data_[\"FINAL_TEXT\"].astype(str)\n",
    "  # data[\"FINAL_TEXT\"] = data[\"FINAL_TEXT\"].apply(nltk.word_tokenize)\n",
    "  data_[\"FINAL_TEXT\"] = data_.apply(lambda row: nltk.word_tokenize(row.FINAL_TEXT), axis=1)\n",
    "  \n",
    "  # ##################-------Removing stopwords-------##################\n",
    "  data_[\"FINAL_TEXT\"] = data_[\"FINAL_TEXT\"].astype(str).apply(lambda x: [word for word in x if word not in stop_words])\n",
    "  \n",
    "  # ##################-------Stemming-------##################\n",
    "  ps = PorterStemmer()\n",
    "  data_[\"FINAL_TEXT\"] = data_[\"FINAL_TEXT\"].astype(str).apply(lambda x: [ps.stem(y) for y in x])\n",
    " \n",
    "  # ##################-------Lemmatizing-------##################\n",
    "#   lemmatizer = WordNetLemmatizer()\n",
    "#   data[\"FINAL_TEXT\"] = data[\"FINAL_TEXT\"].astype(str).apply(lambda x: [lemmatizer.lemmatize(y, get_wordnet_pos(y)) for y in x])\n",
    "  \n",
    "  # ##################-------Joining the lemmetized tokens to form string-------##################\n",
    "#   data[\"FINAL_TEXT\"] = data[\"FINAL_TEXT\"].astype(str).apply(lambda x: \" \".join([word for word in x]))\n",
    " \n",
    "  # ##################-------Remove punctuations-------##################\n",
    "  data_[\"FINAL_TEXT\"] = data_[\"FINAL_TEXT\"].astype(str).str.translate(str.maketrans(\"\", \"\", string.punctuation)).str.replace(\"’\", \" \").str.replace(\"“\", \" \").str.replace(\"”\", \" \")\n",
    " \n",
    "\n",
    "  return data_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "bYbVqTj9gR0X"
   },
   "outputs": [],
   "source": [
    "a = non_quote_non_rt.copy()\n",
    "b = non_quote_non_rt.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "kwNrdbep9Z0Z"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time =  18.805187940597534\n"
     ]
    }
   ],
   "source": [
    "# df_cleaning(non_quote_non_rt, quote_non_rt, quote_rt, non_quote_rt)\n",
    "start = time.time()\n",
    "resul = df_cleaning(a)\n",
    "end = time.time()\n",
    "print(\"time = \", end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "ROcVKfprPobe"
   },
   "outputs": [],
   "source": [
    "def split_dataframe(df, nums = 4): \n",
    "    chunks = list()\n",
    "    num_chunks = nums\n",
    "    chunk_size = len(df) // nums\n",
    "    for i in range(num_chunks):\n",
    "        chunks.append(df[i*chunk_size:(i+1)*chunk_size])\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "LT1tTK6aOjWp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "print(multiprocess.cpu_count())\n",
    "df = split_dataframe(b, 8)\n",
    "\n",
    "# for i in range(4): \n",
    "#   df.iloc[i * 25: i* 25+25, 0] = data[i]\n",
    "  # p.close()\n",
    "  # p.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "Ta1W24-4hW9o"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time =  6.200814247131348\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "with Pool(8) as p:\n",
    "  data = p.map(df_cleaning, df)\n",
    "  p.close()\n",
    "  p.join()\n",
    "    \n",
    "end = time.time()\n",
    "print(\"time = \", end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "MgyeOYXeN-fh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time =  4.1698899269104\n"
     ]
    }
   ],
   "source": [
    "\n",
    "processes = []\n",
    "start = time.time()\n",
    "\n",
    "for i in range(8):\n",
    "  p = Process(target = df_cleaning, args=(df[i],))\n",
    "  processes.append(p)\n",
    "  p.start()\n",
    "\n",
    "for p in processes: p.join()\n",
    "    \n",
    "end = time.time()\n",
    "print(\"time = \", end - start)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Data Cleaning and Analysis.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
