{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import csv\n",
    "import re\n",
    "import string\n",
    "import time\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/manikya_varshney/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/manikya_varshney/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/manikya_varshney/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/manikya_varshney/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import re\n",
    "import string\n",
    "import emoji\n",
    "\n",
    "import nltk\n",
    "from nltk import pos_tag, pos_tag_sents\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "stop_words = set(stopwords.words('english')) - set(['at', 'do', 'your', 'from', 'to', 'out', 'no', 'the'])\n",
    "#demoji.download_codes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "indir = \"/media/manikya_varshney/My Passport/Mani(Yale)/Data/h01-20200909-20200915/h01-20200914\"\n",
    "outfile = \"/media/manikya_varshney/My Passport/Mani(Yale)/Data/h01-20200909-20200915/(C&A)September14th.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Data_Cleaning(data):\n",
    "    \n",
    "    cols = [0, 1, 3, 4, 5,6, 7, 8, 14, 15, 16, 17, 18, 19, 22, 30, 31, 32, 34, 35, 36, 41, 42, 43, 45, 51, 52, 53, 54, 55, 56, 58, 63, 64, 65, 67, 73, 74, 75, 76, 77]\n",
    "    \n",
    "    data = data[data.columns.values[cols]]\n",
    "\n",
    "    data_en = data[data['lang'] == 'en'].reset_index(drop=True)\n",
    "\n",
    "    # RT flag\n",
    "\n",
    "    data_en['RT_Flag'] = data_en['text'].str[:2]=='RT'\n",
    "\n",
    "    # Date check\n",
    "\n",
    "    #data_en['Date_Flag'] = data_en['created_at'].str[:10] == 'Tue Aug 18'\n",
    "\n",
    "    #data_en = data_en[np.where((data_en['Date_Flag'] == True),True,False)].reset_index(drop=True)\n",
    "\n",
    "    # Tree\n",
    "\n",
    "    ############### is_quote_tweet --> False --> Non RT ###############\n",
    "\n",
    "    interim = data_en[np.where((data_en['is_quote_tweet'] == False) & (data_en['RT_Flag']==False),True,False)].reset_index(drop=True)\n",
    "\n",
    "    interim['extended_tweet_full_text'] = np.where(~interim['truncated'], interim['text'], interim['extended_tweet_full_text'])\n",
    "\n",
    "    non_rep = interim[['created_at', 'id','user_id', 'coordinates', 'location', 'extended_tweet_full_text']]\n",
    "\n",
    "    non_rep = non_rep.drop_duplicates(['id']).reset_index(drop=True)\n",
    "\n",
    "    ############### is_quote_tweet --> False --> RT ###############\n",
    "\n",
    "    interim = data_en[np.where((data_en['is_quote_tweet'] == False) & (data_en['RT_Flag']== True),True,False)].reset_index(drop=True)\n",
    "\n",
    "    interim['RT_full_text'] = np.where(interim['RT_full_text'].isna(),interim['RT_text'],interim['RT_full_text'])\n",
    "\n",
    "    interim = interim[['created_at', 'RT_id','RT_user_id', 'RT_coordinates', 'RT_location','RT_full_text']]\n",
    "\n",
    "    interim = interim.drop_duplicates(['RT_id']).reset_index(drop=True)\n",
    "\n",
    "    interim.columns = non_rep.columns\n",
    "\n",
    "    non_rep = non_rep.append(interim,ignore_index=True)\n",
    "\n",
    "    non_rep = non_rep.drop_duplicates(['id']).reset_index(drop=True)\n",
    "\n",
    "    ############### is_quote_tweet --> True --> Non RT ###############\n",
    "\n",
    "    interim = data_en[np.where((data_en['is_quote_tweet']== True) & (data_en['RT_Flag']== False),True,False)].reset_index(drop=True)\n",
    "\n",
    "    interim['extended_tweet_full_text'] = np.where(~interim['truncated'],interim['text'],interim['extended_tweet_full_text'])\n",
    "\n",
    "    interim = interim[['created_at', 'id','user_id', 'coordinates', 'location', 'extended_tweet_full_text']]\n",
    "\n",
    "    interim.columns = non_rep.columns\n",
    "\n",
    "    interim = interim.drop_duplicates(['id']).reset_index(drop=True)\n",
    "\n",
    "    non_rep = non_rep.append(interim,ignore_index=True)\n",
    "\n",
    "    non_rep = non_rep.drop_duplicates(['id']).reset_index(drop=True)\n",
    "\n",
    "    interim = data_en[np.where((data_en['is_quote_tweet'] == True) & (data_en['RT_Flag']== False),True,False)].reset_index(drop=True)\n",
    "\n",
    "    interim['QT_full_text'] = np.where(interim['QT_full_text'].isna(),interim['QT_text'],interim['QT_full_text'])\n",
    "\n",
    "    interim = interim[['created_at','QT_id','QT_user_id', 'QT_coordinates', 'QT_location', 'QT_full_text']]\n",
    "\n",
    "    interim = interim.drop_duplicates(['QT_id']).reset_index(drop=True)\n",
    "\n",
    "    interim.columns = non_rep.columns\n",
    "\n",
    "    non_rep = non_rep.append(interim,ignore_index=True)\n",
    "\n",
    "    non_rep = non_rep.drop_duplicates(['id']).reset_index(drop=True)\n",
    "\n",
    "    ############### is_quote_tweet --> True --> RT ###############\n",
    "\n",
    "    interim = data_en[np.where((data_en['is_quote_tweet'] == True) & (data_en['RT_Flag']== True),True,False)].reset_index(drop=True)\n",
    "\n",
    "    interim['RT_full_text'] = np.where(interim['RT_full_text'].isna(),interim['RT_text'],interim['RT_full_text'])\n",
    "    \n",
    "    interim = interim[['created_at', 'RT_id','RT_user_id', 'RT_coordinates', 'RT_location','RT_full_text']]\n",
    "\n",
    "    interim = interim.drop_duplicates(['RT_id']).reset_index(drop=True)\n",
    "\n",
    "    interim.columns = non_rep.columns\n",
    "\n",
    "    non_rep = non_rep.append(interim,ignore_index=True)\n",
    "\n",
    "    non_rep = non_rep.drop_duplicates(['id']).reset_index(drop=True)\n",
    "\n",
    "    interim = data_en[np.where((data_en['is_quote_tweet']== True ) & (data_en['RT_Flag']== True),True,False)].reset_index(drop=True)\n",
    "\n",
    "    interim['QT_full_text'] = np.where(interim['QT_full_text'].isna(),interim['QT_text'],interim['QT_full_text'])\n",
    "\n",
    "    interim = interim[['created_at', 'QT_id','QT_user_id', 'QT_coordinates', 'QT_location', 'QT_full_text']]\n",
    "\n",
    "    interim = interim.drop_duplicates(['QT_id']).reset_index(drop=True)\n",
    "\n",
    "    interim.columns = non_rep.columns\n",
    "\n",
    "    non_rep = non_rep.append(interim,ignore_index=True)\n",
    "\n",
    "    non_rep = non_rep.drop_duplicates(['id']).reset_index(drop=True)\n",
    "    \n",
    "    return non_rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Analysis(cleaned_data):\n",
    "    \n",
    "    #cleaned_data = non_rep.copy()\n",
    "\n",
    "    cleaned_data['extended_tweet_full_text_duplicate'] = cleaned_data['extended_tweet_full_text']\n",
    "\n",
    "    #Convert to lower case\n",
    "    cleaned_data['extended_tweet_full_text'] = cleaned_data['extended_tweet_full_text'].str.lower()\n",
    "\n",
    "    #Removing emojis\n",
    "    def demoji(text):\n",
    "        emoji_pattern = re.compile(\"[\"\n",
    "            u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "            u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "            u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "            u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "            u\"\\U00002702-\\U000027B0\"\n",
    "            u\"\\U000024C2-\\U0001F251\"\n",
    "            u\"\\U00010000-\\U0010ffff\"\n",
    "                                   \"]+\", flags=re.UNICODE)\n",
    "        return(emoji_pattern.sub(r'', text))\n",
    "\n",
    "    cleaned_data[u'extended_tweet_full_text'] = cleaned_data[u'extended_tweet_full_text'].astype(str)\n",
    "    cleaned_data[u'extended_tweet_full_text'] = cleaned_data[u'extended_tweet_full_text'].apply(lambda x:demoji(x))\n",
    "\n",
    "    #Remove URLs\n",
    "    cleaned_data['extended_tweet_full_text'] = cleaned_data['extended_tweet_full_text'].str.replace(r\"http\\S+| www\\S+| https\\S+| \\S+\\.com\\S+| \\S+\\.com\", \"\", regex=True)\n",
    "\n",
    "    #Remove user @\n",
    "    cleaned_data['extended_tweet_full_text'] = cleaned_data['extended_tweet_full_text'].str.replace(r'\\@[\\w]+', \"\", regex=True)\n",
    "\n",
    "    #Remove punctuations\n",
    "    cleaned_data['extended_tweet_full_text'] = cleaned_data['extended_tweet_full_text'].str.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "\n",
    "    #More Cleaning\n",
    "    cleaned_data['extended_tweet_full_text']=cleaned_data['extended_tweet_full_text'].astype(str).str.replace('/[^a-zA-Z0-9 ]/g', '', regex=True).str.replace('\\n',' ', regex=True).str.replace('—',' ', regex=True).str.strip('“').str.strip('”').str.strip('’').str.lstrip(' ').str.rstrip(' ')\n",
    "\n",
    "    #Tokenizing\n",
    "    cleaned_data.extended_tweet_full_text = cleaned_data.extended_tweet_full_text.astype(str)\n",
    "    cleaned_data['tokenized_extended_tweet_full_text'] = cleaned_data.apply(lambda row: nltk.word_tokenize(row.extended_tweet_full_text), axis=1)\n",
    "\n",
    "    # remove stopwords\n",
    "    cleaned_data['filtered_extended_tweet_full_text'] = cleaned_data['tokenized_extended_tweet_full_text'].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "\n",
    "    #Stemming\n",
    "    ps = PorterStemmer()\n",
    "    cleaned_data['stemmed_extended_tweet_full_text'] = cleaned_data['filtered_extended_tweet_full_text'].apply(lambda x: [ps.stem(y) for y in x])\n",
    "\n",
    "    #POSTags\n",
    "    def get_wordnet_pos(word):\n",
    "        \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "        tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "        tag_dict = {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV}\n",
    "        return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "    #Lemmatizing\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    cleaned_data['lemmatized_extended_tweet_full_text'] = cleaned_data['filtered_extended_tweet_full_text'].apply(lambda x: [lemmatizer.lemmatize(y, get_wordnet_pos(y)) for y in x])\n",
    "\n",
    "    #Joining the lemmetized tokens to form string\n",
    "    cleaned_data['final'] = cleaned_data['lemmatized_extended_tweet_full_text'].apply(lambda x: \" \".join([word for word in x]))\n",
    "\n",
    "    #Remove punctuations\n",
    "    cleaned_data['final'] = cleaned_data['final'].str.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    cleaned_data['final'] = cleaned_data['final'].str.replace(\"’\", '').str.replace(\"“\", '').str.replace(\"”\", '')\n",
    "    cleaned_data['extended_tweet_full_text']=cleaned_data['extended_tweet_full_text'].astype(str).str.replace('/[^a-zA-Z0-9 ]/g', '', regex=True).str.replace('\\n',' ', regex=True).str.replace('—',' ', regex=True).str.strip('“').str.strip('”').str.strip('’').str.lstrip(' ').str.rstrip(' ')\n",
    "\n",
    "    cols_2 = [0, 1, 2, 3 ,4 ,6, 11]\n",
    "    data_final = cleaned_data[cleaned_data.columns.values[cols_2]]\n",
    "\n",
    "    return data_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate(indir, outfile):\n",
    "    os.chdir(indir)\n",
    "    fileList = glob.glob(\"*.csv\")\n",
    "    dfList = []\n",
    "    for filename in fileList:\n",
    "        print(filename)\n",
    "        df = pd.read_csv(filename, header = 0, low_memory=False)\n",
    "        cleaned_df = Data_Cleaning(df)\n",
    "        final_df = Analysis(cleaned_df)\n",
    "        dfList.append(final_df)\n",
    "        print(len(dfList))\n",
    "    print(len(dfList))\n",
    "    concatDf = pd.concat(dfList, axis = 0)\n",
    "    concatDf.to_csv(outfile, index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h01-20200914-000341.csv\n",
      "1\n",
      "h01-20200914-001057.csv\n",
      "2\n",
      "h01-20200914-001648.csv\n",
      "3\n",
      "h01-20200914-002242.csv\n",
      "4\n",
      "h01-20200914-002841.csv\n",
      "5\n",
      "h01-20200914-003439.csv\n",
      "6\n",
      "h01-20200914-004052.csv\n",
      "7\n",
      "h01-20200914-004703.csv\n",
      "8\n",
      "h01-20200914-005319.csv\n",
      "9\n",
      "h01-20200914-005948.csv\n",
      "10\n",
      "h01-20200914-011248.csv\n",
      "11\n",
      "h01-20200914-012607.csv\n",
      "12\n",
      "h01-20200914-013922.csv\n",
      "13\n",
      "h01-20200914-020543.csv\n",
      "14\n",
      "h01-20200914-023127.csv\n",
      "15\n",
      "h01-20200914-032329.csv\n",
      "16\n",
      "h01-20200914-034907.csv\n",
      "17\n",
      "h01-20200914-041535.csv\n",
      "18\n",
      "h01-20200914-043543.csv\n",
      "19\n",
      "h01-20200914-044224.csv\n",
      "20\n",
      "h01-20200914-044905.csv\n",
      "21\n",
      "h01-20200914-045534.csv\n",
      "22\n",
      "h01-20200914-050144.csv\n",
      "23\n",
      "h01-20200914-050759.csv\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "bad operand type for unary ~: 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-bc80460a00ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-865ab527aa4b>\u001b[0m in \u001b[0;36mconcatenate\u001b[0;34m(indir, outfile)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlow_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mcleaned_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mData_Cleaning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mfinal_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAnalysis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcleaned_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mdfList\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-65fa72dcdb95>\u001b[0m in \u001b[0;36mData_Cleaning\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0minterim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_en\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_en\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'is_quote_tweet'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata_en\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'RT_Flag'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0minterim\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'extended_tweet_full_text'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0minterim\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'truncated'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterim\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterim\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'extended_tweet_full_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mnon_rep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minterim\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'created_at'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'id'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'user_id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'coordinates'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'location'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'extended_tweet_full_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__invert__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1434\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1436\u001b[0;31m         \u001b[0mnew_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1437\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_constructor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"__invert__\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1438\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, f, align_keys, ignore_failures, **kwargs)\u001b[0m\n\u001b[1;32m    423\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m                     \u001b[0mapplied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    426\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m                     \u001b[0mapplied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, **kwargs)\u001b[0m\n\u001b[1;32m    376\u001b[0m         \"\"\"\n\u001b[1;32m    377\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_split_op_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: bad operand type for unary ~: 'float'"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "concatenate(indir, outfile)\n",
    "\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total time : \",(end-start),\"sec\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
