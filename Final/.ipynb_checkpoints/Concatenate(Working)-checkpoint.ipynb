{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import csv\n",
    "import re\n",
    "import string\n",
    "import time\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/manikya_varshney/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/manikya_varshney/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/manikya_varshney/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/manikya_varshney/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import re\n",
    "import string\n",
    "import emoji\n",
    "\n",
    "import nltk\n",
    "from nltk import pos_tag, pos_tag_sents\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "stop_words = set(stopwords.words('english')) - set(['at', 'do', 'your', 'from', 'to', 'out', 'no', 'the'])\n",
    "#demoji.download_codes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "indir = \"/media/manikya_varshney/My Passport/Mani(Yale)/Data/h01-20200909-20200915/h01-20200909\"\n",
    "outfile = \"/media/manikya_varshney/My Passport/Mani(Yale)/Data/h01-20200909-20200915/(C&A)September9th.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Data_Cleaning(data):\n",
    "    \n",
    "    cols = [0, 1, 3, 4, 5,6, 7, 8, 14, 15, 16, 17, 18, 19, 22, 30, 31, 32, 34, 35, 36, 41, 42, 43, 45, 51, 52, 53, 54, 55, 56, 58, 63, 64, 65, 67, 73, 74, 75, 76, 77]\n",
    "    \n",
    "    data = data[data.columns.values[cols]]\n",
    "\n",
    "    data_en = data[data['lang'] == 'en'].reset_index(drop=True)\n",
    "\n",
    "    # RT flag\n",
    "\n",
    "    data_en['RT_Flag'] = data_en['text'].str[:2]=='RT'\n",
    "\n",
    "    # Date check\n",
    "\n",
    "    #data_en['Date_Flag'] = data_en['created_at'].str[:10] == 'Tue Aug 18'\n",
    "\n",
    "    #data_en = data_en[np.where((data_en['Date_Flag'] == True),True,False)].reset_index(drop=True)\n",
    "\n",
    "    # Tree\n",
    "\n",
    "    ############### is_quote_tweet --> False --> Non RT ###############\n",
    "\n",
    "    interim = data_en[np.where((data_en['is_quote_tweet'] == False) & (data_en['RT_Flag']==False),True,False)].reset_index(drop=True)\n",
    "\n",
    "    interim['extended_tweet_full_text'] = np.where(~interim['truncated'], interim['text'], interim['extended_tweet_full_text'])\n",
    "\n",
    "    non_rep = interim[['created_at', 'id','user_id', 'coordinates', 'location', 'extended_tweet_full_text']]\n",
    "\n",
    "    non_rep = non_rep.drop_duplicates(['id']).reset_index(drop=True)\n",
    "\n",
    "    ############### is_quote_tweet --> False --> RT ###############\n",
    "\n",
    "    interim = data_en[np.where((data_en['is_quote_tweet'] == False) & (data_en['RT_Flag']== True),True,False)].reset_index(drop=True)\n",
    "\n",
    "    interim['RT_full_text'] = np.where(interim['RT_full_text'].isna(),interim['RT_text'],interim['RT_full_text'])\n",
    "\n",
    "    interim = interim[['created_at', 'RT_id','RT_user_id', 'RT_coordinates', 'RT_location','RT_full_text']]\n",
    "\n",
    "    interim = interim.drop_duplicates(['RT_id']).reset_index(drop=True)\n",
    "\n",
    "    interim.columns = non_rep.columns\n",
    "\n",
    "    non_rep = non_rep.append(interim,ignore_index=True)\n",
    "\n",
    "    non_rep = non_rep.drop_duplicates(['id']).reset_index(drop=True)\n",
    "\n",
    "    ############### is_quote_tweet --> True --> Non RT ###############\n",
    "\n",
    "    interim = data_en[np.where((data_en['is_quote_tweet']== True) & (data_en['RT_Flag']== False),True,False)].reset_index(drop=True)\n",
    "\n",
    "    interim['extended_tweet_full_text'] = np.where(~interim['truncated'],interim['text'],interim['extended_tweet_full_text'])\n",
    "\n",
    "    interim = interim[['created_at', 'id','user_id', 'coordinates', 'location', 'extended_tweet_full_text']]\n",
    "\n",
    "    interim.columns = non_rep.columns\n",
    "\n",
    "    interim = interim.drop_duplicates(['id']).reset_index(drop=True)\n",
    "\n",
    "    non_rep = non_rep.append(interim,ignore_index=True)\n",
    "\n",
    "    non_rep = non_rep.drop_duplicates(['id']).reset_index(drop=True)\n",
    "\n",
    "    interim = data_en[np.where((data_en['is_quote_tweet'] == True) & (data_en['RT_Flag']== False),True,False)].reset_index(drop=True)\n",
    "\n",
    "    interim['QT_full_text'] = np.where(interim['QT_full_text'].isna(),interim['QT_text'],interim['QT_full_text'])\n",
    "\n",
    "    interim = interim[['created_at','QT_id','QT_user_id', 'QT_coordinates', 'QT_location', 'QT_full_text']]\n",
    "\n",
    "    interim = interim.drop_duplicates(['QT_id']).reset_index(drop=True)\n",
    "\n",
    "    interim.columns = non_rep.columns\n",
    "\n",
    "    non_rep = non_rep.append(interim,ignore_index=True)\n",
    "\n",
    "    non_rep = non_rep.drop_duplicates(['id']).reset_index(drop=True)\n",
    "\n",
    "    ############### is_quote_tweet --> True --> RT ###############\n",
    "\n",
    "    interim = data_en[np.where((data_en['is_quote_tweet'] == True) & (data_en['RT_Flag']== True),True,False)].reset_index(drop=True)\n",
    "\n",
    "    interim['RT_full_text'] = np.where(interim['RT_full_text'].isna(),interim['RT_text'],interim['RT_full_text'])\n",
    "    \n",
    "    interim = interim[['created_at', 'RT_id','RT_user_id', 'RT_coordinates', 'RT_location','RT_full_text']]\n",
    "\n",
    "    interim = interim.drop_duplicates(['RT_id']).reset_index(drop=True)\n",
    "\n",
    "    interim.columns = non_rep.columns\n",
    "\n",
    "    non_rep = non_rep.append(interim,ignore_index=True)\n",
    "\n",
    "    non_rep = non_rep.drop_duplicates(['id']).reset_index(drop=True)\n",
    "\n",
    "    interim = data_en[np.where((data_en['is_quote_tweet']== True ) & (data_en['RT_Flag']== True),True,False)].reset_index(drop=True)\n",
    "\n",
    "    interim['QT_full_text'] = np.where(interim['QT_full_text'].isna(),interim['QT_text'],interim['QT_full_text'])\n",
    "\n",
    "    interim = interim[['created_at', 'QT_id','QT_user_id', 'QT_coordinates', 'QT_location', 'QT_full_text']]\n",
    "\n",
    "    interim = interim.drop_duplicates(['QT_id']).reset_index(drop=True)\n",
    "\n",
    "    interim.columns = non_rep.columns\n",
    "\n",
    "    non_rep = non_rep.append(interim,ignore_index=True)\n",
    "\n",
    "    non_rep = non_rep.drop_duplicates(['id']).reset_index(drop=True)\n",
    "    \n",
    "    return non_rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Analysis(cleaned_data):\n",
    "    \n",
    "    #cleaned_data = non_rep.copy()\n",
    "\n",
    "    cleaned_data['extended_tweet_full_text_duplicate'] = cleaned_data['extended_tweet_full_text']\n",
    "\n",
    "    #Convert to lower case\n",
    "    cleaned_data['extended_tweet_full_text'] = cleaned_data['extended_tweet_full_text'].str.lower()\n",
    "\n",
    "    #Removing emojis\n",
    "    def demoji(text):\n",
    "        emoji_pattern = re.compile(\"[\"\n",
    "            u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "            u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "            u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "            u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "            u\"\\U00002702-\\U000027B0\"\n",
    "            u\"\\U000024C2-\\U0001F251\"\n",
    "            u\"\\U00010000-\\U0010ffff\"\n",
    "                                   \"]+\", flags=re.UNICODE)\n",
    "        return(emoji_pattern.sub(r'', text))\n",
    "\n",
    "    cleaned_data[u'extended_tweet_full_text'] = cleaned_data[u'extended_tweet_full_text'].astype(str)\n",
    "    cleaned_data[u'extended_tweet_full_text'] = cleaned_data[u'extended_tweet_full_text'].apply(lambda x:demoji(x))\n",
    "\n",
    "    #Remove URLs\n",
    "    cleaned_data['extended_tweet_full_text'] = cleaned_data['extended_tweet_full_text'].str.replace(r\"http\\S+| www\\S+| https\\S+| \\S+\\.com\\S+| \\S+\\.com\", \"\", regex=True)\n",
    "\n",
    "    #Remove user @\n",
    "    cleaned_data['extended_tweet_full_text'] = cleaned_data['extended_tweet_full_text'].str.replace(r'\\@[\\w]+', \"\", regex=True)\n",
    "\n",
    "    #Remove punctuations\n",
    "    cleaned_data['extended_tweet_full_text'] = cleaned_data['extended_tweet_full_text'].str.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "\n",
    "    #More Cleaning\n",
    "    cleaned_data['extended_tweet_full_text']=cleaned_data['extended_tweet_full_text'].astype(str).str.replace('/[^a-zA-Z0-9 ]/g', '', regex=True).str.replace('\\n',' ', regex=True).str.replace('—',' ', regex=True).str.strip('“').str.strip('”').str.strip('’').str.lstrip(' ').str.rstrip(' ')\n",
    "\n",
    "    #Tokenizing\n",
    "    cleaned_data.extended_tweet_full_text = cleaned_data.extended_tweet_full_text.astype(str)\n",
    "    cleaned_data['tokenized_extended_tweet_full_text'] = cleaned_data.apply(lambda row: nltk.word_tokenize(row.extended_tweet_full_text), axis=1)\n",
    "\n",
    "    # remove stopwords\n",
    "    cleaned_data['filtered_extended_tweet_full_text'] = cleaned_data['tokenized_extended_tweet_full_text'].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "\n",
    "    #Stemming\n",
    "    ps = PorterStemmer()\n",
    "    cleaned_data['stemmed_extended_tweet_full_text'] = cleaned_data['filtered_extended_tweet_full_text'].apply(lambda x: [ps.stem(y) for y in x])\n",
    "\n",
    "    #POSTags\n",
    "    def get_wordnet_pos(word):\n",
    "        \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "        tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "        tag_dict = {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV}\n",
    "        return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "    #Lemmatizing\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    cleaned_data['lemmatized_extended_tweet_full_text'] = cleaned_data['filtered_extended_tweet_full_text'].apply(lambda x: [lemmatizer.lemmatize(y, get_wordnet_pos(y)) for y in x])\n",
    "\n",
    "    #Joining the lemmetized tokens to form string\n",
    "    cleaned_data['final'] = cleaned_data['lemmatized_extended_tweet_full_text'].apply(lambda x: \" \".join([word for word in x]))\n",
    "\n",
    "    #Remove punctuations\n",
    "    cleaned_data['final'] = cleaned_data['final'].str.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    cleaned_data['final'] = cleaned_data['final'].str.replace(\"’\", '').str.replace(\"“\", '').str.replace(\"”\", '')\n",
    "    cleaned_data['extended_tweet_full_text']=cleaned_data['extended_tweet_full_text'].astype(str).str.replace('/[^a-zA-Z0-9 ]/g', '', regex=True).str.replace('\\n',' ', regex=True).str.replace('—',' ', regex=True).str.strip('“').str.strip('”').str.strip('’').str.lstrip(' ').str.rstrip(' ')\n",
    "\n",
    "    cols_2 = [0, 1, 2, 3 ,4 ,6, 11]\n",
    "    data_final = cleaned_data[cleaned_data.columns.values[cols_2]]\n",
    "\n",
    "    return data_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate(indir, outfile):\n",
    "    os.chdir(indir)\n",
    "    fileList = glob.glob(\"*.csv\")\n",
    "    dfList = []\n",
    "    for filename in fileList:\n",
    "        print(filename)\n",
    "        df = pd.read_csv(filename, header = 0, low_memory=False)\n",
    "        cleaned_df = Data_Cleaning(df)\n",
    "        final_df = Analysis(cleaned_df)\n",
    "        dfList.append(final_df)\n",
    "        print(len(dfList))\n",
    "    print(len(dfList))\n",
    "    concatDf = pd.concat(dfList, axis = 0)\n",
    "    concatDf.to_csv(outfile, index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h01-20200909-000007.csv\n",
      "1\n",
      "h01-20200909-000642.csv\n",
      "2\n",
      "h01-20200909-002330.csv\n",
      "3\n",
      "h01-20200909-002851.csv\n",
      "4\n",
      "h01-20200909-003358.csv\n",
      "5\n",
      "h01-20200909-004037.csv\n",
      "6\n",
      "h01-20200909-004551.csv\n",
      "7\n",
      "h01-20200909-005106.csv\n",
      "8\n",
      "h01-20200909-005629.csv\n",
      "9\n",
      "h01-20200909-010148.csv\n",
      "10\n",
      "h01-20200909-010717.csv\n",
      "11\n",
      "h01-20200909-011245.csv\n",
      "12\n",
      "h01-20200909-011814.csv\n",
      "13\n",
      "h01-20200909-012333.csv\n",
      "14\n",
      "h01-20200909-012904.csv\n",
      "15\n",
      "h01-20200909-014001.csv\n",
      "16\n",
      "h01-20200909-014531.csv\n",
      "17\n",
      "h01-20200909-015059.csv\n",
      "18\n",
      "h01-20200909-015623.csv\n",
      "19\n",
      "h01-20200909-020141.csv\n",
      "20\n",
      "h01-20200909-020658.csv\n",
      "21\n",
      "h01-20200909-021212.csv\n",
      "22\n",
      "h01-20200909-021726.csv\n",
      "23\n",
      "h01-20200909-022248.csv\n",
      "24\n",
      "h01-20200909-022809.csv\n",
      "25\n",
      "h01-20200909-023319.csv\n",
      "26\n",
      "h01-20200909-023837.csv\n",
      "27\n",
      "h01-20200909-024349.csv\n",
      "28\n",
      "h01-20200909-024858.csv\n",
      "29\n",
      "h01-20200909-025227.csv\n",
      "30\n",
      "h01-20200909-030018.csv\n",
      "31\n",
      "h01-20200909-030423.csv\n",
      "32\n",
      "h01-20200909-030837.csv\n",
      "33\n",
      "h01-20200909-031257.csv\n",
      "34\n",
      "h01-20200909-031719.csv\n",
      "35\n",
      "h01-20200909-032150.csv\n",
      "36\n",
      "h01-20200909-032613.csv\n",
      "37\n",
      "h01-20200909-033033.csv\n",
      "38\n",
      "h01-20200909-033510.csv\n",
      "39\n",
      "h01-20200909-033946.csv\n",
      "40\n",
      "h01-20200909-034423.csv\n",
      "41\n",
      "h01-20200909-034908.csv\n",
      "42\n",
      "h01-20200909-035349.csv\n",
      "43\n",
      "h01-20200909-035832.csv\n",
      "44\n",
      "h01-20200909-040303.csv\n",
      "45\n",
      "h01-20200909-041221.csv\n",
      "46\n",
      "h01-20200909-041710.csv\n",
      "47\n",
      "h01-20200909-042155.csv\n",
      "48\n",
      "h01-20200909-042642.csv\n",
      "49\n",
      "h01-20200909-043126.csv\n",
      "50\n",
      "h01-20200909-043617.csv\n",
      "51\n",
      "h01-20200909-044107.csv\n",
      "52\n",
      "h01-20200909-044550.csv\n",
      "53\n",
      "h01-20200909-045033.csv\n",
      "54\n",
      "h01-20200909-050003.csv\n",
      "55\n",
      "h01-20200909-050909.csv\n",
      "56\n",
      "h01-20200909-051806.csv\n",
      "57\n",
      "h01-20200909-052646.csv\n",
      "58\n",
      "h01-20200909-053532.csv\n",
      "59\n",
      "h01-20200909-054406.csv\n",
      "60\n",
      "h01-20200909-013435.csv\n",
      "61\n",
      "h01-20200909-025618.csv\n",
      "62\n",
      "h01-20200909-040743.csv\n",
      "63\n",
      "h01-20200909-055237.csv\n",
      "64\n",
      "h01-20200909-073736.csv\n",
      "65\n",
      "h01-20200909-090435.csv\n",
      "66\n",
      "h01-20200909-102624.csv\n",
      "67\n",
      "h01-20200909-112644.csv\n",
      "68\n",
      "h01-20200909-115413.csv\n",
      "69\n",
      "h01-20200909-122853.csv\n",
      "70\n",
      "h01-20200909-132222.csv\n",
      "71\n",
      "h01-20200909-152640.csv\n",
      "72\n",
      "h01-20200909-161556.csv\n",
      "73\n",
      "h01-20200909-174429.csv\n",
      "74\n",
      "h01-20200909-193952.csv\n",
      "75\n",
      "h01-20200909-204200.csv\n",
      "76\n",
      "h01-20200909-214806.csv\n",
      "77\n",
      "h01-20200909-060106.csv\n",
      "78\n",
      "h01-20200909-061741.csv\n",
      "79\n",
      "h01-20200909-063403.csv\n",
      "80\n",
      "h01-20200909-064834.csv\n",
      "81\n",
      "h01-20200909-065231.csv\n",
      "82\n",
      "h01-20200909-065747.csv\n",
      "83\n",
      "h01-20200909-070254.csv\n",
      "84\n",
      "h01-20200909-070724.csv\n",
      "85\n",
      "h01-20200909-071115.csv\n",
      "86\n",
      "h01-20200909-071510.csv\n",
      "87\n",
      "h01-20200909-071857.csv\n",
      "88\n",
      "h01-20200909-072245.csv\n",
      "89\n",
      "h01-20200909-072635.csv\n",
      "90\n",
      "h01-20200909-073018.csv\n",
      "91\n",
      "h01-20200909-073356.csv\n",
      "92\n",
      "h01-20200909-074113.csv\n",
      "93\n",
      "h01-20200909-074455.csv\n",
      "94\n",
      "h01-20200909-074830.csv\n",
      "95\n",
      "h01-20200909-075158.csv\n",
      "96\n",
      "h01-20200909-075526.csv\n",
      "97\n",
      "h01-20200909-075855.csv\n",
      "98\n",
      "h01-20200909-080319.csv\n",
      "99\n",
      "h01-20200909-080756.csv\n",
      "100\n",
      "h01-20200909-082120.csv\n",
      "101\n",
      "h01-20200909-083423.csv\n",
      "102\n",
      "h01-20200909-084533.csv\n",
      "103\n",
      "h01-20200909-084935.csv\n",
      "104\n",
      "h01-20200909-085328.csv\n",
      "105\n",
      "h01-20200909-085720.csv\n",
      "106\n",
      "h01-20200909-090058.csv\n",
      "107\n",
      "h01-20200909-090816.csv\n",
      "108\n",
      "h01-20200909-091350.csv\n",
      "109\n",
      "h01-20200909-091916.csv\n",
      "110\n",
      "h01-20200909-092434.csv\n",
      "111\n",
      "h01-20200909-092953.csv\n",
      "112\n",
      "h01-20200909-093502.csv\n",
      "113\n",
      "h01-20200909-094020.csv\n",
      "114\n",
      "h01-20200909-094532.csv\n",
      "115\n",
      "h01-20200909-095050.csv\n",
      "116\n",
      "h01-20200909-095607.csv\n",
      "117\n",
      "h01-20200909-100106.csv\n",
      "118\n",
      "h01-20200909-100609.csv\n",
      "119\n",
      "h01-20200909-101116.csv\n",
      "120\n",
      "h01-20200909-101625.csv\n",
      "121\n",
      "h01-20200909-102124.csv\n",
      "122\n",
      "h01-20200909-103638.csv\n",
      "123\n",
      "h01-20200909-104628.csv\n",
      "124\n",
      "h01-20200909-105604.csv\n",
      "125\n",
      "h01-20200909-110016.csv\n",
      "126\n",
      "h01-20200909-110229.csv\n",
      "127\n",
      "h01-20200909-110453.csv\n",
      "128\n",
      "h01-20200909-110722.csv\n",
      "129\n",
      "h01-20200909-110927.csv\n",
      "130\n",
      "h01-20200909-111138.csv\n",
      "131\n",
      "h01-20200909-111353.csv\n",
      "132\n",
      "h01-20200909-111608.csv\n",
      "133\n",
      "h01-20200909-111819.csv\n",
      "134\n",
      "h01-20200909-112028.csv\n",
      "135\n",
      "h01-20200909-112239.csv\n",
      "136\n",
      "h01-20200909-112444.csv\n",
      "137\n",
      "h01-20200909-112843.csv\n",
      "138\n",
      "h01-20200909-113037.csv\n",
      "139\n",
      "h01-20200909-113234.csv\n",
      "140\n",
      "h01-20200909-113426.csv\n",
      "141\n",
      "h01-20200909-113613.csv\n",
      "142\n",
      "h01-20200909-113758.csv\n",
      "143\n",
      "h01-20200909-113939.csv\n",
      "144\n",
      "h01-20200909-114119.csv\n",
      "145\n",
      "h01-20200909-114256.csv\n",
      "146\n",
      "h01-20200909-114430.csv\n",
      "147\n",
      "h01-20200909-114603.csv\n",
      "148\n",
      "h01-20200909-114742.csv\n",
      "149\n",
      "h01-20200909-114918.csv\n",
      "150\n",
      "h01-20200909-115056.csv\n",
      "151\n",
      "h01-20200909-115233.csv\n",
      "152\n",
      "h01-20200909-115625.csv\n",
      "153\n",
      "h01-20200909-115843.csv\n",
      "154\n",
      "h01-20200909-120055.csv\n",
      "155\n",
      "h01-20200909-120309.csv\n",
      "156\n",
      "h01-20200909-120522.csv\n",
      "157\n",
      "h01-20200909-120732.csv\n",
      "158\n",
      "h01-20200909-120939.csv\n",
      "159\n",
      "h01-20200909-121148.csv\n",
      "160\n",
      "h01-20200909-121358.csv\n",
      "161\n",
      "h01-20200909-121605.csv\n",
      "162\n",
      "h01-20200909-121810.csv\n",
      "163\n",
      "h01-20200909-122017.csv\n",
      "164\n",
      "h01-20200909-122224.csv\n",
      "165\n",
      "h01-20200909-122432.csv\n",
      "166\n",
      "h01-20200909-122643.csv\n",
      "167\n",
      "h01-20200909-123207.csv\n",
      "168\n",
      "h01-20200909-123525.csv\n",
      "169\n",
      "h01-20200909-123846.csv\n",
      "170\n",
      "h01-20200909-124204.csv\n",
      "171\n",
      "h01-20200909-124520.csv\n",
      "172\n",
      "h01-20200909-124838.csv\n",
      "173\n",
      "h01-20200909-125156.csv\n",
      "174\n",
      "h01-20200909-125516.csv\n",
      "175\n",
      "h01-20200909-125838.csv\n",
      "176\n",
      "h01-20200909-130151.csv\n",
      "177\n",
      "h01-20200909-130517.csv\n",
      "178\n",
      "h01-20200909-130847.csv\n",
      "179\n",
      "h01-20200909-131219.csv\n",
      "180\n",
      "h01-20200909-131542.csv\n",
      "181\n",
      "h01-20200909-131903.csv\n",
      "182\n",
      "h01-20200909-132544.csv\n",
      "183\n",
      "h01-20200909-132910.csv\n",
      "184\n",
      "h01-20200909-133229.csv\n",
      "185\n",
      "h01-20200909-133548.csv\n",
      "186\n",
      "h01-20200909-133910.csv\n",
      "187\n",
      "h01-20200909-134236.csv\n",
      "188\n",
      "h01-20200909-134559.csv\n",
      "189\n",
      "h01-20200909-134923.csv\n",
      "190\n",
      "h01-20200909-135617.csv\n",
      "191\n",
      "h01-20200909-140321.csv\n",
      "192\n",
      "h01-20200909-141046.csv\n",
      "193\n",
      "h01-20200909-141817.csv\n",
      "194\n",
      "h01-20200909-142608.csv\n",
      "195\n",
      "h01-20200909-143341.csv\n",
      "196\n",
      "h01-20200909-144126.csv\n",
      "197\n",
      "h01-20200909-152903.csv\n",
      "198\n",
      "h01-20200909-153120.csv\n",
      "199\n",
      "h01-20200909-153341.csv\n",
      "200\n",
      "h01-20200909-153651.csv\n",
      "201\n",
      "h01-20200909-154000.csv\n",
      "202\n",
      "h01-20200909-154310.csv\n",
      "203\n",
      "h01-20200909-154622.csv\n",
      "204\n",
      "h01-20200909-154941.csv\n",
      "205\n",
      "h01-20200909-155304.csv\n",
      "206\n",
      "h01-20200909-155627.csv\n",
      "207\n",
      "h01-20200909-155948.csv\n",
      "208\n",
      "h01-20200909-160255.csv\n",
      "209\n",
      "h01-20200909-160610.csv\n",
      "210\n",
      "h01-20200909-160925.csv\n",
      "211\n",
      "h01-20200909-161240.csv\n",
      "212\n",
      "h01-20200909-161913.csv\n",
      "213\n",
      "h01-20200909-162227.csv\n",
      "214\n",
      "h01-20200909-162542.csv\n",
      "215\n",
      "h01-20200909-162902.csv\n",
      "216\n",
      "h01-20200909-163220.csv\n",
      "217\n",
      "h01-20200909-163543.csv\n",
      "218\n",
      "h01-20200909-163905.csv\n",
      "219\n",
      "h01-20200909-164230.csv\n",
      "220\n",
      "h01-20200909-164553.csv\n",
      "221\n",
      "h01-20200909-165103.csv\n",
      "222\n",
      "h01-20200909-165615.csv\n",
      "223\n",
      "h01-20200909-170126.csv\n",
      "224\n",
      "h01-20200909-171229.csv\n",
      "225\n",
      "h01-20200909-172320.csv\n",
      "226\n",
      "h01-20200909-173350.csv\n",
      "227\n",
      "h01-20200909-175458.csv\n",
      "228\n",
      "h01-20200909-180602.csv\n",
      "229\n",
      "h01-20200909-181711.csv\n",
      "230\n",
      "h01-20200909-182814.csv\n",
      "231\n",
      "h01-20200909-183931.csv\n",
      "232\n",
      "h01-20200909-185111.csv\n",
      "233\n",
      "h01-20200909-190248.csv\n",
      "234\n",
      "h01-20200909-191501.csv\n",
      "235\n",
      "h01-20200909-191817.csv\n",
      "236\n",
      "h01-20200909-192117.csv\n",
      "237\n",
      "h01-20200909-192421.csv\n",
      "238\n",
      "h01-20200909-192723.csv\n",
      "239\n",
      "h01-20200909-193026.csv\n",
      "240\n",
      "h01-20200909-193334.csv\n",
      "241\n",
      "h01-20200909-193641.csv\n",
      "242\n",
      "h01-20200909-194406.csv\n",
      "243\n",
      "h01-20200909-194815.csv\n",
      "244\n",
      "h01-20200909-195221.csv\n",
      "245\n",
      "h01-20200909-195627.csv\n",
      "246\n",
      "h01-20200909-200031.csv\n",
      "247\n",
      "h01-20200909-200449.csv\n",
      "248\n",
      "h01-20200909-200951.csv\n",
      "249\n",
      "h01-20200909-201258.csv\n",
      "250\n",
      "h01-20200909-201606.csv\n",
      "251\n",
      "h01-20200909-201914.csv\n",
      "252\n",
      "h01-20200909-202219.csv\n",
      "253\n",
      "h01-20200909-202525.csv\n",
      "254\n",
      "h01-20200909-202933.csv\n",
      "255\n",
      "h01-20200909-203344.csv\n",
      "256\n",
      "h01-20200909-203749.csv\n",
      "257\n",
      "h01-20200909-204609.csv\n",
      "258\n",
      "h01-20200909-205021.csv\n",
      "259\n",
      "h01-20200909-205433.csv\n",
      "260\n",
      "h01-20200909-205843.csv\n",
      "261\n",
      "h01-20200909-210256.csv\n",
      "262\n",
      "h01-20200909-210713.csv\n",
      "263\n",
      "h01-20200909-211124.csv\n",
      "264\n",
      "h01-20200909-211524.csv\n",
      "265\n",
      "h01-20200909-211944.csv\n",
      "266\n",
      "h01-20200909-212357.csv\n",
      "267\n",
      "h01-20200909-212803.csv\n",
      "268\n",
      "h01-20200909-213201.csv\n",
      "269\n",
      "h01-20200909-213601.csv\n",
      "270\n",
      "h01-20200909-214000.csv\n",
      "271\n",
      "h01-20200909-214358.csv\n",
      "272\n",
      "h01-20200909-215208.csv\n",
      "273\n",
      "h01-20200909-215609.csv\n",
      "274\n",
      "h01-20200909-220010.csv\n",
      "275\n",
      "h01-20200909-220412.csv\n",
      "276\n",
      "h01-20200909-220821.csv\n",
      "277\n",
      "h01-20200909-221222.csv\n",
      "278\n",
      "h01-20200909-221621.csv\n",
      "279\n",
      "h01-20200909-222017.csv\n",
      "280\n",
      "h01-20200909-222422.csv\n",
      "281\n",
      "h01-20200909-222823.csv\n",
      "282\n",
      "h01-20200909-223227.csv\n",
      "283\n",
      "h01-20200909-223634.csv\n",
      "284\n",
      "h01-20200909-224043.csv\n",
      "285\n",
      "h01-20200909-224651.csv\n",
      "286\n",
      "h01-20200909-225305.csv\n",
      "287\n",
      "h01-20200909-225915.csv\n",
      "288\n",
      "h01-20200909-230532.csv\n",
      "289\n",
      "h01-20200909-231156.csv\n",
      "290\n",
      "h01-20200909-231818.csv\n",
      "291\n",
      "h01-20200909-232439.csv\n",
      "292\n",
      "h01-20200909-232939.csv\n",
      "293\n",
      "h01-20200909-233249.csv\n",
      "294\n",
      "h01-20200909-233608.csv\n",
      "295\n",
      "h01-20200909-233929.csv\n",
      "296\n",
      "h01-20200909-234244.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "297\n",
      "h01-20200909-234559.csv\n",
      "298\n",
      "h01-20200909-234916.csv\n",
      "299\n",
      "h01-20200909-235553.csv\n",
      "300\n",
      "300\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "concatenate(indir, outfile)\n",
    "\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time :  4189.667324542999 sec\n"
     ]
    }
   ],
   "source": [
    "print(\"Total time : \",(end-start),\"sec\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
