{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import csv\n",
    "import re\n",
    "import string\n",
    "import time\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/manikya_varshney/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/manikya_varshney/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/manikya_varshney/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/manikya_varshney/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import re\n",
    "import string\n",
    "import emoji\n",
    "\n",
    "import nltk\n",
    "from nltk import pos_tag, pos_tag_sents\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "stop_words = set(stopwords.words('english')) - set(['at', 'do', 'your', 'from', 'to', 'out', 'no', 'the'])\n",
    "#demoji.download_codes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "indir = \"/media/manikya_varshney/My Passport/Mani(Yale)/Data/h01-20200901-20200908/h01-20200903\"\n",
    "outfile = \"/media/manikya_varshney/My Passport/Mani(Yale)/Data/h01-20200901-20200908/(C&A)September3rd.csv\"\n",
    "\n",
    "#indir = \"/media/manikya_varshney/My Passport/Mani(Yale)/Data\"\n",
    "#outfile = \"/media/manikya_varshney/My Passport/Mani(Yale)/Data/New Final Data/(Final)h01-20200818-153021.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Data_Cleaning(data):\n",
    "    \n",
    "    cols = [0, 1, 3, 4, 5,6, 7, 8, 14, 15, 16, 17, 18, 19, 22, 30, 31, 32, 34, 35, 36, 41, 42, 43, 45, 51, 52, 53, 54, 55, 56, 58, 63, 64, 65, 67, 73, 74, 75, 76, 77]\n",
    "    \n",
    "    data = data[data.columns.values[cols]]\n",
    "\n",
    "    data_en = data[data['lang'] == 'en'].reset_index(drop=True)\n",
    "\n",
    "    # RT flag\n",
    "\n",
    "    data_en['RT_Flag'] = data_en['text'].str[:2]=='RT'\n",
    "\n",
    "    # Date check\n",
    "\n",
    "    #data_en['Date_Flag'] = data_en['created_at'].str[:10] == 'Tue Aug 18'\n",
    "\n",
    "    #data_en = data_en[np.where((data_en['Date_Flag'] == True),True,False)].reset_index(drop=True)\n",
    "\n",
    "    # Tree\n",
    "\n",
    "    ############### is_quote_tweet --> False --> Non RT ###############\n",
    "\n",
    "    interim = data_en[np.where((data_en['is_quote_tweet'] == False) & (data_en['RT_Flag']==False),True,False)].reset_index(drop=True)\n",
    "\n",
    "    interim['extended_tweet_full_text'] = np.where(~interim['truncated'], interim['text'], interim['extended_tweet_full_text'])\n",
    "\n",
    "    non_rep = interim[['created_at', 'id','user_id', 'coordinates', 'location', 'extended_tweet_full_text']]\n",
    "\n",
    "    non_rep = non_rep.drop_duplicates(['id']).reset_index(drop=True)\n",
    "\n",
    "    ############### is_quote_tweet --> False --> RT ###############\n",
    "\n",
    "    interim = data_en[np.where((data_en['is_quote_tweet'] == False) & (data_en['RT_Flag']== True),True,False)].reset_index(drop=True)\n",
    "\n",
    "    interim['RT_full_text'] = np.where(interim['RT_full_text'].isna(),interim['RT_text'],interim['RT_full_text'])\n",
    "\n",
    "    interim = interim[['created_at', 'RT_id','RT_user_id', 'RT_coordinates', 'RT_location','RT_full_text']]\n",
    "\n",
    "    interim = interim.drop_duplicates(['RT_id']).reset_index(drop=True)\n",
    "\n",
    "    interim.columns = non_rep.columns\n",
    "\n",
    "    non_rep = non_rep.append(interim,ignore_index=True)\n",
    "\n",
    "    non_rep = non_rep.drop_duplicates(['id']).reset_index(drop=True)\n",
    "\n",
    "    ############### is_quote_tweet --> True --> Non RT ###############\n",
    "\n",
    "    interim = data_en[np.where((data_en['is_quote_tweet']== True) & (data_en['RT_Flag']== False),True,False)].reset_index(drop=True)\n",
    "\n",
    "    interim['extended_tweet_full_text'] = np.where(~interim['truncated'],interim['text'],interim['extended_tweet_full_text'])\n",
    "\n",
    "    interim = interim[['created_at', 'id','user_id', 'coordinates', 'location', 'extended_tweet_full_text']]\n",
    "\n",
    "    interim.columns = non_rep.columns\n",
    "\n",
    "    interim = interim.drop_duplicates(['id']).reset_index(drop=True)\n",
    "\n",
    "    non_rep = non_rep.append(interim,ignore_index=True)\n",
    "\n",
    "    non_rep = non_rep.drop_duplicates(['id']).reset_index(drop=True)\n",
    "\n",
    "    interim = data_en[np.where((data_en['is_quote_tweet'] == True) & (data_en['RT_Flag']== False),True,False)].reset_index(drop=True)\n",
    "\n",
    "    interim['QT_full_text'] = np.where(interim['QT_full_text'].isna(),interim['QT_text'],interim['QT_full_text'])\n",
    "\n",
    "    interim = interim[['created_at','QT_id','QT_user_id', 'QT_coordinates', 'QT_location', 'QT_full_text']]\n",
    "\n",
    "    interim = interim.drop_duplicates(['QT_id']).reset_index(drop=True)\n",
    "\n",
    "    interim.columns = non_rep.columns\n",
    "\n",
    "    non_rep = non_rep.append(interim,ignore_index=True)\n",
    "\n",
    "    non_rep = non_rep.drop_duplicates(['id']).reset_index(drop=True)\n",
    "\n",
    "    ############### is_quote_tweet --> True --> RT ###############\n",
    "\n",
    "    interim = data_en[np.where((data_en['is_quote_tweet'] == True) & (data_en['RT_Flag']== True),True,False)].reset_index(drop=True)\n",
    "\n",
    "    interim['RT_full_text'] = np.where(interim['RT_full_text'].isna(),interim['RT_text'],interim['RT_full_text'])\n",
    "    \n",
    "    interim = interim[['created_at', 'RT_id','RT_user_id', 'RT_coordinates', 'RT_location','RT_full_text']]\n",
    "\n",
    "    interim = interim.drop_duplicates(['RT_id']).reset_index(drop=True)\n",
    "\n",
    "    interim.columns = non_rep.columns\n",
    "\n",
    "    non_rep = non_rep.append(interim,ignore_index=True)\n",
    "\n",
    "    non_rep = non_rep.drop_duplicates(['id']).reset_index(drop=True)\n",
    "\n",
    "    interim = data_en[np.where((data_en['is_quote_tweet']== True ) & (data_en['RT_Flag']== True),True,False)].reset_index(drop=True)\n",
    "\n",
    "    interim['QT_full_text'] = np.where(interim['QT_full_text'].isna(),interim['QT_text'],interim['QT_full_text'])\n",
    "\n",
    "    interim = interim[['created_at', 'QT_id','QT_user_id', 'QT_coordinates', 'QT_location', 'QT_full_text']]\n",
    "\n",
    "    interim = interim.drop_duplicates(['QT_id']).reset_index(drop=True)\n",
    "\n",
    "    interim.columns = non_rep.columns\n",
    "\n",
    "    non_rep = non_rep.append(interim,ignore_index=True)\n",
    "\n",
    "    non_rep = non_rep.drop_duplicates(['id']).reset_index(drop=True)\n",
    "    \n",
    "    return non_rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Analysis(cleaned_data):\n",
    "    \n",
    "    #cleaned_data = non_rep.copy()\n",
    "\n",
    "    cleaned_data['extended_tweet_full_text_duplicate'] = cleaned_data['extended_tweet_full_text']\n",
    "\n",
    "    #Convert to lower case\n",
    "    cleaned_data['extended_tweet_full_text'] = cleaned_data['extended_tweet_full_text'].str.lower()\n",
    "\n",
    "    #Removing emojis\n",
    "    def demoji(text):\n",
    "        emoji_pattern = re.compile(\"[\"\n",
    "            u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "            u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "            u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "            u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "            u\"\\U00002702-\\U000027B0\"\n",
    "            u\"\\U000024C2-\\U0001F251\"\n",
    "            u\"\\U00010000-\\U0010ffff\"\n",
    "                                   \"]+\", flags=re.UNICODE)\n",
    "        return(emoji_pattern.sub(r'', text))\n",
    "\n",
    "    cleaned_data[u'extended_tweet_full_text'] = cleaned_data[u'extended_tweet_full_text'].astype(str)\n",
    "    cleaned_data[u'extended_tweet_full_text'] = cleaned_data[u'extended_tweet_full_text'].apply(lambda x:demoji(x))\n",
    "\n",
    "    #Remove URLs\n",
    "    cleaned_data['extended_tweet_full_text'] = cleaned_data['extended_tweet_full_text'].str.replace(r\"http\\S+| www\\S+| https\\S+| \\S+\\.com\\S+| \\S+\\.com\", \"\", regex=True)\n",
    "\n",
    "    #Remove user @\n",
    "    cleaned_data['extended_tweet_full_text'] = cleaned_data['extended_tweet_full_text'].str.replace(r'\\@[\\w]+', \"\", regex=True)\n",
    "\n",
    "    #Remove punctuations\n",
    "    cleaned_data['extended_tweet_full_text'] = cleaned_data['extended_tweet_full_text'].str.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "\n",
    "    #More Cleaning\n",
    "    cleaned_data['extended_tweet_full_text']=cleaned_data['extended_tweet_full_text'].astype(str).str.replace('/[^a-zA-Z0-9 ]/g', '', regex=True).str.replace('\\n',' ', regex=True).str.replace('—',' ', regex=True).str.strip('“').str.strip('”').str.strip('’').str.lstrip(' ').str.rstrip(' ')\n",
    "\n",
    "    #Tokenizing\n",
    "    cleaned_data.extended_tweet_full_text = cleaned_data.extended_tweet_full_text.astype(str)\n",
    "    cleaned_data['tokenized_extended_tweet_full_text'] = cleaned_data.apply(lambda row: nltk.word_tokenize(row.extended_tweet_full_text), axis=1)\n",
    "\n",
    "    # remove stopwords\n",
    "    cleaned_data['filtered_extended_tweet_full_text'] = cleaned_data['tokenized_extended_tweet_full_text'].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "\n",
    "    #Stemming\n",
    "    ps = PorterStemmer()\n",
    "    cleaned_data['stemmed_extended_tweet_full_text'] = cleaned_data['filtered_extended_tweet_full_text'].apply(lambda x: [ps.stem(y) for y in x])\n",
    "\n",
    "    #POSTags\n",
    "    def get_wordnet_pos(word):\n",
    "        \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "        tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "        tag_dict = {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV}\n",
    "        return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "    #Lemmatizing\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    cleaned_data['lemmatized_extended_tweet_full_text'] = cleaned_data['filtered_extended_tweet_full_text'].apply(lambda x: [lemmatizer.lemmatize(y, get_wordnet_pos(y)) for y in x])\n",
    "\n",
    "    #Joining the lemmetized tokens to form string\n",
    "    cleaned_data['final'] = cleaned_data['lemmatized_extended_tweet_full_text'].apply(lambda x: \" \".join([word for word in x]))\n",
    "\n",
    "    #Remove punctuations\n",
    "    cleaned_data['final'] = cleaned_data['final'].str.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    cleaned_data['final'] = cleaned_data['final'].str.replace(\"’\", '').str.replace(\"“\", '').str.replace(\"”\", '')\n",
    "    cleaned_data['extended_tweet_full_text']=cleaned_data['extended_tweet_full_text'].astype(str).str.replace('/[^a-zA-Z0-9 ]/g', '', regex=True).str.replace('\\n',' ', regex=True).str.replace('—',' ', regex=True).str.strip('“').str.strip('”').str.strip('’').str.lstrip(' ').str.rstrip(' ')\n",
    "\n",
    "    cols_2 = [0, 1, 2, 3 ,4 ,6, 11]\n",
    "    data_final = cleaned_data[cleaned_data.columns.values[cols_2]]\n",
    "\n",
    "    return data_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate(indir, outfile):\n",
    "    os.chdir(indir)\n",
    "    fileList = glob.glob(\"*.csv\")\n",
    "    dfList = []\n",
    "    for filename in fileList:\n",
    "        print(filename)\n",
    "        df = pd.read_csv(filename, header = 0, low_memory=False)\n",
    "        cleaned_df = Data_Cleaning(df)\n",
    "        final_df = Analysis(cleaned_df)\n",
    "        dfList.append(final_df)\n",
    "    print(len(dfList))\n",
    "    concatDf = pd.concat(dfList, axis = 0)\n",
    "    concatDf.to_csv(outfile, index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h01-20200902-000407.csv\n",
      "h01-20200902-001021.csv\n",
      "h01-20200902-001635.csv\n",
      "h01-20200902-002300.csv\n",
      "h01-20200902-002938.csv\n",
      "h01-20200902-003522.csv\n",
      "h01-20200902-004025.csv\n",
      "h01-20200902-004525.csv\n",
      "h01-20200902-005032.csv\n",
      "h01-20200902-005542.csv\n",
      "h01-20200902-010057.csv\n",
      "h01-20200902-010619.csv\n",
      "h01-20200902-011135.csv\n",
      "h01-20200902-011656.csv\n",
      "h01-20200902-012227.csv\n",
      "h01-20200902-013335.csv\n",
      "h01-20200902-013918.csv\n",
      "h01-20200902-014502.csv\n",
      "h01-20200902-015044.csv\n",
      "h01-20200902-015637.csv\n",
      "h01-20200902-020217.csv\n",
      "h01-20200902-020744.csv\n",
      "h01-20200902-021321.csv\n",
      "h01-20200902-021907.csv\n",
      "h01-20200902-022453.csv\n",
      "h01-20200902-023038.csv\n",
      "h01-20200902-023637.csv\n",
      "h01-20200902-024239.csv\n",
      "h01-20200902-024847.csv\n",
      "h01-20200902-025453.csv\n",
      "h01-20200902-030648.csv\n",
      "h01-20200902-031241.csv\n",
      "h01-20200902-031838.csv\n",
      "h01-20200902-032441.csv\n",
      "h01-20200902-033045.csv\n",
      "h01-20200902-033650.csv\n",
      "h01-20200902-034305.csv\n",
      "h01-20200902-034913.csv\n",
      "h01-20200902-035523.csv\n",
      "h01-20200902-040114.csv\n",
      "h01-20200902-040933.csv\n",
      "h01-20200902-041748.csv\n",
      "h01-20200902-042602.csv\n",
      "h01-20200902-043357.csv\n",
      "h01-20200902-044213.csv\n",
      "h01-20200902-045817.csv\n",
      "h01-20200902-050416.csv\n",
      "h01-20200902-051011.csv\n",
      "h01-20200902-051559.csv\n",
      "h01-20200902-052142.csv\n",
      "h01-20200902-052723.csv\n",
      "h01-20200902-053257.csv\n",
      "h01-20200902-053830.csv\n",
      "h01-20200902-054409.csv\n",
      "h01-20200902-054942.csv\n",
      "h01-20200902-055518.csv\n",
      "h01-20200902-060036.csv\n",
      "h01-20200902-060557.csv\n",
      "h01-20200902-061123.csv\n",
      "h01-20200902-061642.csv\n",
      "h01-20200902-012800.csv\n",
      "h01-20200902-030052.csv\n",
      "h01-20200902-045021.csv\n",
      "h01-20200902-062157.csv\n",
      "h01-20200902-075706.csv\n",
      "h01-20200902-090446.csv\n",
      "h01-20200902-100100.csv\n",
      "h01-20200902-104759.csv\n",
      "h01-20200902-113559.csv\n",
      "h01-20200902-125750.csv\n",
      "h01-20200902-150429.csv\n",
      "h01-20200902-162531.csv\n",
      "h01-20200902-183534.csv\n",
      "h01-20200902-193948.csv\n",
      "h01-20200902-204304.csv\n",
      "h01-20200902-062656.csv\n",
      "h01-20200902-063144.csv\n",
      "h01-20200902-064014.csv\n",
      "h01-20200902-064938.csv\n",
      "h01-20200902-065854.csv\n",
      "h01-20200902-070746.csv\n",
      "h01-20200902-071643.csv\n",
      "h01-20200902-072133.csv\n",
      "h01-20200902-072550.csv\n",
      "h01-20200902-073114.csv\n",
      "h01-20200902-073641.csv\n",
      "h01-20200902-074111.csv\n",
      "h01-20200902-074514.csv\n",
      "h01-20200902-074911.csv\n",
      "h01-20200902-075308.csv\n",
      "h01-20200902-080056.csv\n",
      "h01-20200902-080447.csv\n",
      "h01-20200902-080841.csv\n",
      "h01-20200902-081231.csv\n",
      "h01-20200902-081610.csv\n",
      "h01-20200902-081952.csv\n",
      "h01-20200902-082328.csv\n",
      "h01-20200902-082708.csv\n",
      "h01-20200902-083035.csv\n",
      "h01-20200902-083405.csv\n",
      "h01-20200902-083846.csv\n",
      "h01-20200902-084319.csv\n",
      "h01-20200902-084737.csv\n",
      "h01-20200902-085158.csv\n",
      "h01-20200902-085827.csv\n",
      "h01-20200902-091121.csv\n",
      "h01-20200902-091746.csv\n",
      "h01-20200902-092113.csv\n",
      "h01-20200902-092429.csv\n",
      "h01-20200902-092745.csv\n",
      "h01-20200902-093051.csv\n",
      "h01-20200902-093406.csv\n",
      "h01-20200902-093715.csv\n",
      "h01-20200902-094017.csv\n",
      "h01-20200902-094321.csv\n",
      "h01-20200902-094619.csv\n",
      "h01-20200902-094919.csv\n",
      "h01-20200902-095217.csv\n",
      "h01-20200902-095516.csv\n",
      "h01-20200902-095813.csv\n",
      "h01-20200902-100355.csv\n",
      "h01-20200902-100654.csv\n",
      "h01-20200902-100952.csv\n",
      "h01-20200902-101254.csv\n",
      "h01-20200902-101552.csv\n",
      "h01-20200902-101851.csv\n",
      "h01-20200902-102144.csv\n",
      "h01-20200902-102442.csv\n",
      "h01-20200902-102742.csv\n",
      "h01-20200902-103036.csv\n",
      "h01-20200902-103328.csv\n",
      "h01-20200902-103622.csv\n",
      "h01-20200902-103916.csv\n",
      "h01-20200902-104210.csv\n",
      "h01-20200902-104505.csv\n",
      "h01-20200902-105056.csv\n",
      "h01-20200902-105354.csv\n",
      "h01-20200902-105652.csv\n",
      "h01-20200902-105950.csv\n",
      "h01-20200902-110245.csv\n",
      "h01-20200902-110552.csv\n",
      "h01-20200902-110836.csv\n",
      "h01-20200902-111131.csv\n",
      "h01-20200902-111419.csv\n",
      "h01-20200902-111710.csv\n",
      "h01-20200902-112004.csv\n",
      "h01-20200902-112300.csv\n",
      "h01-20200902-112557.csv\n",
      "h01-20200902-112859.csv\n",
      "h01-20200902-113155.csv\n",
      "h01-20200902-114008.csv\n",
      "h01-20200902-114418.csv\n",
      "h01-20200902-114825.csv\n",
      "h01-20200902-115229.csv\n",
      "h01-20200902-115638.csv\n",
      "h01-20200902-120044.csv\n",
      "h01-20200902-120451.csv\n",
      "h01-20200902-120905.csv\n",
      "h01-20200902-121316.csv\n",
      "h01-20200902-121727.csv\n",
      "h01-20200902-122144.csv\n",
      "h01-20200902-122557.csv\n",
      "h01-20200902-123207.csv\n",
      "h01-20200902-123824.csv\n",
      "h01-20200902-124518.csv\n",
      "h01-20200902-131027.csv\n",
      "h01-20200902-132329.csv\n",
      "h01-20200902-133617.csv\n",
      "h01-20200902-134953.csv\n",
      "h01-20200902-140257.csv\n",
      "h01-20200902-141546.csv\n",
      "h01-20200902-142716.csv\n",
      "h01-20200902-143038.csv\n",
      "h01-20200902-143400.csv\n",
      "h01-20200902-143728.csv\n",
      "h01-20200902-144058.csv\n",
      "h01-20200902-144426.csv\n",
      "h01-20200902-144825.csv\n",
      "h01-20200902-145302.csv\n",
      "h01-20200902-145737.csv\n",
      "h01-20200902-151134.csv\n",
      "h01-20200902-151748.csv\n",
      "h01-20200902-152106.csv\n",
      "h01-20200902-152427.csv\n",
      "h01-20200902-152751.csv\n",
      "h01-20200902-153112.csv\n",
      "h01-20200902-153438.csv\n",
      "h01-20200902-153808.csv\n",
      "h01-20200902-154140.csv\n",
      "h01-20200902-154512.csv\n",
      "h01-20200902-155219.csv\n",
      "h01-20200902-155916.csv\n",
      "h01-20200902-160617.csv\n",
      "h01-20200902-161317.csv\n",
      "h01-20200902-162005.csv\n",
      "h01-20200902-162904.csv\n",
      "h01-20200902-163224.csv\n",
      "h01-20200902-163552.csv\n",
      "h01-20200902-163921.csv\n",
      "h01-20200902-164356.csv\n",
      "h01-20200902-164904.csv\n",
      "h01-20200902-165541.csv\n",
      "h01-20200902-170206.csv\n",
      "h01-20200902-170853.csv\n",
      "h01-20200902-171539.csv\n",
      "h01-20200902-172338.csv\n",
      "h01-20200902-173723.csv\n",
      "h01-20200902-175119.csv\n",
      "h01-20200902-180529.csv\n",
      "h01-20200902-182026.csv\n",
      "h01-20200902-184039.csv\n",
      "h01-20200902-184428.csv\n",
      "h01-20200902-184819.csv\n",
      "h01-20200902-185208.csv\n",
      "h01-20200902-185602.csv\n",
      "h01-20200902-185954.csv\n",
      "h01-20200902-190343.csv\n",
      "h01-20200902-190741.csv\n",
      "h01-20200902-191138.csv\n",
      "h01-20200902-191534.csv\n",
      "h01-20200902-191937.csv\n",
      "h01-20200902-192341.csv\n",
      "h01-20200902-192745.csv\n",
      "h01-20200902-193144.csv\n",
      "h01-20200902-193544.csv\n",
      "h01-20200902-194349.csv\n",
      "h01-20200902-194739.csv\n",
      "h01-20200902-195138.csv\n",
      "h01-20200902-195535.csv\n",
      "h01-20200902-195931.csv\n",
      "h01-20200902-200325.csv\n",
      "h01-20200902-200733.csv\n",
      "h01-20200902-201139.csv\n",
      "h01-20200902-201539.csv\n",
      "h01-20200902-201936.csv\n",
      "h01-20200902-202335.csv\n",
      "h01-20200902-202731.csv\n",
      "h01-20200902-203119.csv\n",
      "h01-20200902-203514.csv\n",
      "h01-20200902-203911.csv\n",
      "h01-20200902-204658.csv\n",
      "h01-20200902-205206.csv\n",
      "h01-20200902-205710.csv\n",
      "h01-20200902-210438.csv\n",
      "h01-20200902-211209.csv\n",
      "h01-20200902-211933.csv\n",
      "h01-20200902-212642.csv\n",
      "h01-20200902-213351.csv\n",
      "h01-20200902-214120.csv\n",
      "h01-20200902-214845.csv\n",
      "h01-20200902-215617.csv\n",
      "h01-20200902-220336.csv\n",
      "h01-20200902-221050.csv\n",
      "h01-20200902-221805.csv\n",
      "h01-20200902-222512.csv\n",
      "h01-20200902-223217.csv\n",
      "h01-20200902-223927.csv\n",
      "h01-20200902-224632.csv\n",
      "h01-20200902-225342.csv\n",
      "h01-20200902-230055.csv\n",
      "h01-20200902-230607.csv\n",
      "h01-20200902-230949.csv\n",
      "h01-20200902-231328.csv\n",
      "h01-20200902-231705.csv\n",
      "h01-20200902-232044.csv\n",
      "h01-20200902-232424.csv\n",
      "h01-20200902-232806.csv\n",
      "h01-20200902-233146.csv\n",
      "h01-20200902-233528.csv\n",
      "269\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "concatenate(indir, outfile)\n",
    "\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time :  3731.509759426117 sec\n"
     ]
    }
   ],
   "source": [
    "print(\"Total time : \",(end-start),\"sec\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "path = \"/media/manikya_varshney/My Passport/Mani(Yale)/Data/New Final Data/(Final)h01-20200818-153021.csv\"\n",
    "data = pd.read_csv(path, header = 0, low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
