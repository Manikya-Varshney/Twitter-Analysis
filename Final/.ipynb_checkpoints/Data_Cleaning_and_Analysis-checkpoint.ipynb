{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import csv\n",
    "import re\n",
    "import string\n",
    "import time\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/manikya_varshney/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/manikya_varshney/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/manikya_varshney/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/manikya_varshney/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import re\n",
    "import string\n",
    "import emoji\n",
    "\n",
    "import nltk\n",
    "from nltk import pos_tag, pos_tag_sents\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "stop_words = set(stopwords.words('english')) - set(['at', 'do', 'your', 'from', 'to', 'out', 'no', 'the'])\n",
    "#demoji.download_codes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/manikya_varshney/Documents/Python/Yale/h01-20200818-10files/h01-20200818-153021.csv\"\n",
    "data = pd.read_csv(path, header = 0, low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [0, 1, 3, 4, 5,6, 7, 8, 14, 15, 16, 17, 18, 19, 22, 30, 31, 32, 34, 35, 36, 41, 42, 43, 45, 51, 52, 53, 54, 55, 56, 58, 63, 64, 65, 67, 73, 74, 75, 76, 77]\n",
    "\n",
    "data = data[data.columns.values[cols]]\n",
    "\n",
    "data_en = data[data['lang'] == 'en'].reset_index(drop=True)\n",
    "\n",
    "# RT flag\n",
    "\n",
    "data_en['RT_Flag'] = data_en['text'].str[:2]=='RT'\n",
    "\n",
    "# Date check\n",
    "\n",
    "data_en['Date_Flag'] = data_en['created_at'].str[:10] == 'Tue Aug 18'\n",
    "\n",
    "data_en = data_en[np.where((data_en['Date_Flag'] == True),True,False)].reset_index(drop=True)\n",
    "\n",
    "# Tree\n",
    "\n",
    "############### is_quote_tweet --> False --> Non RT ###############\n",
    "\n",
    "interim = data_en[np.where((data_en['is_quote_tweet'] == False) & (data_en['RT_Flag']==False),True,False)].reset_index(drop=True)\n",
    "\n",
    "interim['extended_tweet_full_text'] = np.where(~interim['truncated'], interim['text'], interim['extended_tweet_full_text'])\n",
    "\n",
    "non_rep = interim[['created_at', 'id','user_id', 'coordinates', 'location', 'extended_tweet_full_text']]\n",
    "\n",
    "non_rep = non_rep.drop_duplicates(['id']).reset_index(drop=True)\n",
    "\n",
    "############### is_quote_tweet --> False --> RT ###############\n",
    "\n",
    "interim = data_en[np.where((data_en['is_quote_tweet'] == False) & (data_en['RT_Flag']== True),True,False)].reset_index(drop=True)\n",
    "\n",
    "interim['RT_full_text'] = np.where(interim['RT_full_text'].isna(),interim['RT_text'],interim['RT_full_text'])\n",
    "\n",
    "interim = interim[['created_at', 'RT_id','RT_user_id', 'RT_coordinates', 'RT_location','RT_full_text']]\n",
    "\n",
    "interim = interim.drop_duplicates(['RT_id']).reset_index(drop=True)\n",
    "\n",
    "interim.columns = non_rep.columns\n",
    "\n",
    "non_rep = non_rep.append(interim,ignore_index=True)\n",
    "\n",
    "non_rep = non_rep.drop_duplicates(['id']).reset_index(drop=True)\n",
    "\n",
    "############### is_quote_tweet --> True --> Non RT ###############\n",
    "\n",
    "interim = data_en[np.where((data_en['is_quote_tweet']== True) & (data_en['RT_Flag']== False),True,False)].reset_index(drop=True)\n",
    "\n",
    "interim['extended_tweet_full_text'] = np.where(~interim['truncated'],interim['text'],interim['extended_tweet_full_text'])\n",
    "\n",
    "interim = interim[['created_at', 'id','user_id', 'coordinates', 'location', 'extended_tweet_full_text']]\n",
    "\n",
    "interim.columns = non_rep.columns\n",
    "\n",
    "interim = interim.drop_duplicates(['id']).reset_index(drop=True)\n",
    "\n",
    "non_rep = non_rep.append(interim,ignore_index=True)\n",
    "\n",
    "non_rep = non_rep.drop_duplicates(['id']).reset_index(drop=True)\n",
    "\n",
    "interim = data_en[np.where((data_en['is_quote_tweet'] == True) & (data_en['RT_Flag']== False),True,False)].reset_index(drop=True)\n",
    "\n",
    "interim['QT_full_text'] = np.where(interim['QT_full_text'].isna(),interim['QT_text'],interim['QT_full_text'])\n",
    "\n",
    "interim = interim[['created_at','QT_id','QT_user_id', 'QT_coordinates', 'QT_location', 'QT_full_text']]\n",
    "\n",
    "interim = interim.drop_duplicates(['QT_id']).reset_index(drop=True)\n",
    "\n",
    "interim.columns = non_rep.columns\n",
    "\n",
    "non_rep = non_rep.append(interim,ignore_index=True)\n",
    "\n",
    "non_rep = non_rep.drop_duplicates(['id']).reset_index(drop=True)\n",
    "\n",
    "############### is_quote_tweet --> True --> RT ###############\n",
    "\n",
    "interim = data_en[np.where((data_en['is_quote_tweet'] == True) & (data_en['RT_Flag']== True),True,False)].reset_index(drop=True)\n",
    "\n",
    "interim['RT_full_text'] = np.where(interim['RT_full_text'].isna(),interim['RT_text'],interim['RT_full_text'])\n",
    "\n",
    "interim = interim[['created_at', 'RT_id','RT_user_id', 'RT_coordinates', 'RT_location','RT_full_text']]\n",
    "\n",
    "interim = interim.drop_duplicates(['RT_id']).reset_index(drop=True)\n",
    "\n",
    "interim.columns = non_rep.columns\n",
    "\n",
    "non_rep = non_rep.append(interim,ignore_index=True)\n",
    "\n",
    "non_rep = non_rep.drop_duplicates(['id']).reset_index(drop=True)\n",
    "\n",
    "interim = data_en[np.where((data_en['is_quote_tweet']== True ) & (data_en['RT_Flag']== True),True,False)].reset_index(drop=True)\n",
    "\n",
    "interim['QT_full_text'] = np.where(interim['QT_full_text'].isna(),interim['QT_text'],interim['QT_full_text'])\n",
    "\n",
    "interim = interim[['created_at', 'QT_id','QT_user_id', 'QT_coordinates', 'QT_location', 'QT_full_text']]\n",
    "\n",
    "interim = interim.drop_duplicates(['QT_id']).reset_index(drop=True)\n",
    "\n",
    "interim.columns = non_rep.columns\n",
    "\n",
    "non_rep = non_rep.append(interim,ignore_index=True)\n",
    "\n",
    "non_rep = non_rep.drop_duplicates(['id']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  1. Casing (Upper or lower case)\n",
    "##### 2. Noise Removal (Removal of punctuation, white spaces, special characters, HTML tags)\n",
    "##### 3. Tokenization (Tweets to tokens i.e. words seprated by spaces)\n",
    "##### 4. Stopword Removal\n",
    "##### 5. Text Normalization (Stemming and Lemmatization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data = non_rep.copy()\n",
    "\n",
    "cleaned_data['extended_tweet_full_text_duplicate'] = cleaned_data['extended_tweet_full_text']\n",
    "\n",
    "#Convert to lower case\n",
    "cleaned_data['extended_tweet_full_text'] = cleaned_data['extended_tweet_full_text'].str.lower()\n",
    "\n",
    "#Removing emojis\n",
    "def demoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U00010000-\\U0010ffff\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    return(emoji_pattern.sub(r'', text))\n",
    "\n",
    "cleaned_data[u'extended_tweet_full_text'] = cleaned_data[u'extended_tweet_full_text'].astype(str)\n",
    "cleaned_data[u'extended_tweet_full_text'] = cleaned_data[u'extended_tweet_full_text'].apply(lambda x:demoji(x))\n",
    "\n",
    "#Remove URLs\n",
    "cleaned_data['extended_tweet_full_text'] = cleaned_data['extended_tweet_full_text'].str.replace(r\"http\\S+| www\\S+| https\\S+| \\S+\\.com\\S+| \\S+\\.com\", \"\", regex=True)\n",
    "\n",
    "#Remove user @\n",
    "cleaned_data['extended_tweet_full_text'] = cleaned_data['extended_tweet_full_text'].str.replace(r'\\@[\\w]+', \"\", regex=True)\n",
    "\n",
    "#Remove punctuations\n",
    "cleaned_data['extended_tweet_full_text'] = cleaned_data['extended_tweet_full_text'].str.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "\n",
    "#More Cleaning\n",
    "cleaned_data['extended_tweet_full_text']=cleaned_data['extended_tweet_full_text'].astype(str).str.replace('/[^a-zA-Z0-9 ]/g', '', regex=True).str.replace('\\n',' ', regex=True).str.replace('—',' ', regex=True).str.strip('“').str.strip('”').str.strip('’').str.lstrip(' ').str.rstrip(' ')\n",
    "\n",
    "#Tokenizing\n",
    "cleaned_data.extended_tweet_full_text = cleaned_data.extended_tweet_full_text.astype(str)\n",
    "cleaned_data['tokenized_extended_tweet_full_text'] = cleaned_data.apply(lambda row: nltk.word_tokenize(row.extended_tweet_full_text), axis=1)\n",
    "\n",
    "# remove stopwords\n",
    "cleaned_data['filtered_extended_tweet_full_text'] = cleaned_data['tokenized_extended_tweet_full_text'].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "\n",
    "#Stemming\n",
    "ps = PorterStemmer()\n",
    "cleaned_data['stemmed_extended_tweet_full_text'] = cleaned_data['filtered_extended_tweet_full_text'].apply(lambda x: [ps.stem(y) for y in x])\n",
    "\n",
    "#POSTags\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "#Lemmatizing\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "cleaned_data['lemmatized_extended_tweet_full_text'] = cleaned_data['filtered_extended_tweet_full_text'].apply(lambda x: [lemmatizer.lemmatize(y, get_wordnet_pos(y)) for y in x])\n",
    "\n",
    "#Joining the lemmetized tokens to form string\n",
    "cleaned_data['final'] = cleaned_data['lemmatized_extended_tweet_full_text'].apply(lambda x: \" \".join([word for word in x]))\n",
    "\n",
    "#Remove punctuations\n",
    "cleaned_data['final'] = cleaned_data['final'].str.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "cleaned_data['final'] = cleaned_data['final'].str.replace(\"’\", '').str.replace(\"“\", '').str.replace(\"”\", '')\n",
    "cleaned_data['extended_tweet_full_text']=cleaned_data['extended_tweet_full_text'].astype(str).str.replace('/[^a-zA-Z0-9 ]/g', '', regex=True).str.replace('\\n',' ', regex=True).str.replace('—',' ', regex=True).str.strip('“').str.strip('”').str.strip('’').str.lstrip(' ').str.rstrip(' ')\n",
    "\n",
    "cols_2 = [0, 1, 2, 3 ,4 ,6, 11]\n",
    "data_final = cleaned_data[cleaned_data.columns.values[cols_2]]\n",
    "\n",
    "\n",
    "#data_final.to_csv('/home/manikya_varshney/Documents/Python/Yale/Final Data/(Sample_Cleaned&Analysed)h01-20200818-153021.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
