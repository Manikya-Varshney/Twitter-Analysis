{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import csv\n",
    "import re\n",
    "import string\n",
    "import time\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/manikya_varshney/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/manikya_varshney/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/manikya_varshney/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/manikya_varshney/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import re\n",
    "import string\n",
    "import emoji\n",
    "\n",
    "import nltk\n",
    "from nltk import pos_tag, pos_tag_sents\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "stop_words = set(stopwords.words('english')) - set(['at', 'do', 'your', 'from', 'to', 'out', 'no', 'the'])\n",
    "#demoji.download_codes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/manikya_varshney/Documents/Python/Yale/h01-20200818-10files/h01-20200818-153021.csv\"\n",
    "data = pd.read_csv(path, header = 0, low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [0, 1, 3, 4, 5,6, 7, 8, 14, 15, 16, 17, 18, 19, 22, 30, 31, 32, 34, 35, 36, 41, 42, 43, 45, 51, 52, 53, 54, 55, 56, 58, 63, 64, 65, 67, 73, 74, 75, 76, 77]\n",
    "\n",
    "data = data[data.columns.values[cols]]\n",
    "\n",
    "data_en = data[data['lang'] == 'en'].reset_index(drop=True)\n",
    "\n",
    "# RT flag\n",
    "\n",
    "data_en['RT_Flag'] = data_en['text'].str[:2]=='RT'\n",
    "\n",
    "# Date check\n",
    "\n",
    "data_en['Date_Flag'] = data_en['created_at'].str[:10] == 'Tue Aug 18'\n",
    "\n",
    "data_en = data_en[np.where((data_en['Date_Flag'] == True),True,False)].reset_index(drop=True)\n",
    "\n",
    "# Tree\n",
    "\n",
    "############### is_quote_tweet --> False --> Non RT ###############\n",
    "\n",
    "interim = data_en[np.where((data_en['is_quote_tweet'] == False) & (data_en['RT_Flag']==False),True,False)].reset_index(drop=True)\n",
    "\n",
    "interim['extended_tweet_full_text'] = np.where(~interim['truncated'], interim['text'], interim['extended_tweet_full_text'])\n",
    "\n",
    "non_rep = interim[['created_at', 'id','user_id', 'coordinates', 'location', 'extended_tweet_full_text']]\n",
    "\n",
    "non_rep = non_rep.drop_duplicates(['id']).reset_index(drop=True)\n",
    "\n",
    "############### is_quote_tweet --> False --> RT ###############\n",
    "\n",
    "interim = data_en[np.where((data_en['is_quote_tweet'] == False) & (data_en['RT_Flag']== True),True,False)].reset_index(drop=True)\n",
    "\n",
    "interim['RT_full_text'] = np.where(interim['RT_full_text'].isna(),interim['RT_text'],interim['RT_full_text'])\n",
    "\n",
    "interim = interim[['created_at', 'RT_id','RT_user_id', 'RT_coordinates', 'RT_location','RT_full_text']]\n",
    "\n",
    "interim = interim.drop_duplicates(['RT_id']).reset_index(drop=True)\n",
    "\n",
    "interim.columns = non_rep.columns\n",
    "\n",
    "non_rep = non_rep.append(interim,ignore_index=True)\n",
    "\n",
    "non_rep = non_rep.drop_duplicates(['id']).reset_index(drop=True)\n",
    "\n",
    "############### is_quote_tweet --> True --> Non RT ###############\n",
    "\n",
    "interim = data_en[np.where((data_en['is_quote_tweet']== True) & (data_en['RT_Flag']== False),True,False)].reset_index(drop=True)\n",
    "\n",
    "interim['extended_tweet_full_text'] = np.where(~interim['truncated'],interim['text'],interim['extended_tweet_full_text'])\n",
    "\n",
    "interim = interim[['created_at', 'id','user_id', 'coordinates', 'location', 'extended_tweet_full_text']]\n",
    "\n",
    "interim.columns = non_rep.columns\n",
    "\n",
    "interim = interim.drop_duplicates(['id']).reset_index(drop=True)\n",
    "\n",
    "non_rep = non_rep.append(interim,ignore_index=True)\n",
    "\n",
    "non_rep = non_rep.drop_duplicates(['id']).reset_index(drop=True)\n",
    "\n",
    "interim = data_en[np.where((data_en['is_quote_tweet'] == True) & (data_en['RT_Flag']== False),True,False)].reset_index(drop=True)\n",
    "\n",
    "interim['QT_full_text'] = np.where(interim['QT_full_text'].isna(),interim['QT_text'],interim['QT_full_text'])\n",
    "\n",
    "interim = interim[['created_at','QT_id','QT_user_id', 'QT_coordinates', 'QT_location', 'QT_full_text']]\n",
    "\n",
    "interim = interim.drop_duplicates(['QT_id']).reset_index(drop=True)\n",
    "\n",
    "interim.columns = non_rep.columns\n",
    "\n",
    "non_rep = non_rep.append(interim,ignore_index=True)\n",
    "\n",
    "non_rep = non_rep.drop_duplicates(['id']).reset_index(drop=True)\n",
    "\n",
    "############### is_quote_tweet --> True --> RT ###############\n",
    "\n",
    "interim = data_en[np.where((data_en['is_quote_tweet'] == True) & (data_en['RT_Flag']== True),True,False)].reset_index(drop=True)\n",
    "\n",
    "interim['RT_full_text'] = np.where(interim['RT_full_text'].isna(),interim['RT_text'],interim['RT_full_text'])\n",
    "\n",
    "interim = interim[['created_at', 'RT_id','RT_user_id', 'RT_coordinates', 'RT_location','RT_full_text']]\n",
    "\n",
    "interim = interim.drop_duplicates(['RT_id']).reset_index(drop=True)\n",
    "\n",
    "interim.columns = non_rep.columns\n",
    "\n",
    "non_rep = non_rep.append(interim,ignore_index=True)\n",
    "\n",
    "non_rep = non_rep.drop_duplicates(['id']).reset_index(drop=True)\n",
    "\n",
    "interim = data_en[np.where((data_en['is_quote_tweet']== True ) & (data_en['RT_Flag']== True),True,False)].reset_index(drop=True)\n",
    "\n",
    "interim['QT_full_text'] = np.where(interim['QT_full_text'].isna(),interim['QT_text'],interim['QT_full_text'])\n",
    "\n",
    "interim = interim[['created_at', 'QT_id','QT_user_id', 'QT_coordinates', 'QT_location', 'QT_full_text']]\n",
    "\n",
    "interim = interim.drop_duplicates(['QT_id']).reset_index(drop=True)\n",
    "\n",
    "interim.columns = non_rep.columns\n",
    "\n",
    "non_rep = non_rep.append(interim,ignore_index=True)\n",
    "\n",
    "non_rep = non_rep.drop_duplicates(['id']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  1. Casing (Upper or lower case)\n",
    "##### 2. Noise Removal (Removal of punctuation, white spaces, special characters, HTML tags)\n",
    "##### 3. Tokenization (Tweets to tokens i.e. words seprated by spaces)\n",
    "##### 4. Stopword Removal\n",
    "##### 5. Text Normalization (Stemming and Lemmatization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data = non_rep.copy()\n",
    "\n",
    "cleaned_data['extended_tweet_full_text_duplicate'] = cleaned_data['extended_tweet_full_text']\n",
    "\n",
    "#Convert to lower case\n",
    "cleaned_data['extended_tweet_full_text'] = cleaned_data['extended_tweet_full_text'].str.lower()\n",
    "\n",
    "#Removing emojis\n",
    "def demoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U00010000-\\U0010ffff\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    return(emoji_pattern.sub(r'', text))\n",
    "\n",
    "cleaned_data[u'extended_tweet_full_text'] = cleaned_data[u'extended_tweet_full_text'].astype(str)\n",
    "cleaned_data[u'extended_tweet_full_text'] = cleaned_data[u'extended_tweet_full_text'].apply(lambda x:demoji(x))\n",
    "\n",
    "#Remove URLs\n",
    "cleaned_data['extended_tweet_full_text'] = cleaned_data['extended_tweet_full_text'].str.replace(r\"http\\S+| www\\S+| https\\S+| \\S+\\.com\\S+| \\S+\\.com\", \"\", regex=True)\n",
    "\n",
    "#Remove user @\n",
    "cleaned_data['extended_tweet_full_text'] = cleaned_data['extended_tweet_full_text'].str.replace(r'\\@[\\w]+', \"\", regex=True)\n",
    "\n",
    "#Remove punctuations\n",
    "cleaned_data['extended_tweet_full_text'] = cleaned_data['extended_tweet_full_text'].str.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "\n",
    "#More Cleaning\n",
    "cleaned_data['extended_tweet_full_text']=cleaned_data['extended_tweet_full_text'].astype(str).str.replace('/[^a-zA-Z0-9 ]/g', '', regex=True).str.replace('\\n',' ', regex=True).str.replace('—',' ', regex=True).str.strip('“').str.strip('”').str.strip('’').str.lstrip(' ').str.rstrip(' ')\n",
    "\n",
    "#Tokenizing\n",
    "cleaned_data.extended_tweet_full_text = cleaned_data.extended_tweet_full_text.astype(str)\n",
    "cleaned_data['tokenized_extended_tweet_full_text'] = cleaned_data.apply(lambda row: nltk.word_tokenize(row.extended_tweet_full_text), axis=1)\n",
    "\n",
    "# remove stopwords\n",
    "cleaned_data['filtered_extended_tweet_full_text'] = cleaned_data['tokenized_extended_tweet_full_text'].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "\n",
    "#Stemming\n",
    "ps = PorterStemmer()\n",
    "cleaned_data['stemmed_extended_tweet_full_text'] = cleaned_data['filtered_extended_tweet_full_text'].apply(lambda x: [ps.stem(y) for y in x])\n",
    "\n",
    "#POSTags\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "#Lemmatizing\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "cleaned_data['lemmatized_extended_tweet_full_text'] = cleaned_data['filtered_extended_tweet_full_text'].apply(lambda x: [lemmatizer.lemmatize(y, get_wordnet_pos(y)) for y in x])\n",
    "\n",
    "#Joining the lemmetized tokens to form string\n",
    "cleaned_data['final'] = cleaned_data['lemmatized_extended_tweet_full_text'].apply(lambda x: \" \".join([word for word in x]))\n",
    "\n",
    "#Remove punctuations\n",
    "cleaned_data['final'] = cleaned_data['final'].str.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "cleaned_data['final'] = cleaned_data['final'].str.replace(\"’\", '').str.replace(\"“\", '').str.replace(\"”\", '')\n",
    "cleaned_data['extended_tweet_full_text']=cleaned_data['extended_tweet_full_text'].astype(str).str.replace('/[^a-zA-Z0-9 ]/g', '', regex=True).str.replace('\\n',' ', regex=True).str.replace('—',' ', regex=True).str.strip('“').str.strip('”').str.strip('’').str.lstrip(' ').str.rstrip(' ')\n",
    "\n",
    "cols_2 = [0, 1, 2, 3 ,4 ,6, 11]\n",
    "data_final = cleaned_data[cleaned_data.columns.values[cols_2]]\n",
    "\n",
    "\n",
    "#data_final.to_csv('/home/manikya_varshney/Documents/Python/Yale/Final Data/(Sample_Cleaned&Analysed)h01-20200818-153021.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>coordinates</th>\n",
       "      <th>location</th>\n",
       "      <th>extended_tweet_full_text</th>\n",
       "      <th>extended_tweet_full_text_duplicate</th>\n",
       "      <th>tokenized_extended_tweet_full_text</th>\n",
       "      <th>filtered_extended_tweet_full_text</th>\n",
       "      <th>stemmed_extended_tweet_full_text</th>\n",
       "      <th>lemmatized_extended_tweet_full_text</th>\n",
       "      <th>final</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tue Aug 18 19:30:11 +0000 2020</td>\n",
       "      <td>1.295805e+18</td>\n",
       "      <td>1.278716e+18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>can we wear clone helmets instead of masks to ...</td>\n",
       "      <td>can we wear clone helmets instead of masks to ...</td>\n",
       "      <td>[can, we, wear, clone, helmets, instead, of, m...</td>\n",
       "      <td>[wear, clone, helmets, instead, masks, to, sch...</td>\n",
       "      <td>[wear, clone, helmet, instead, mask, to, school]</td>\n",
       "      <td>[wear, clone, helmet, instead, mask, to, school]</td>\n",
       "      <td>wear clone helmet instead mask to school</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tue Aug 18 19:30:11 +0000 2020</td>\n",
       "      <td>1.295805e+18</td>\n",
       "      <td>9.191072e+08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Myrtle Beach, SC</td>\n",
       "      <td>sc senate to reconvene in sept to discuss covi...</td>\n",
       "      <td>SC Senate to reconvene in Sept. to discuss COV...</td>\n",
       "      <td>[sc, senate, to, reconvene, in, sept, to, disc...</td>\n",
       "      <td>[sc, senate, to, reconvene, sept, to, discuss,...</td>\n",
       "      <td>[sc, senat, to, reconven, sept, to, discuss, c...</td>\n",
       "      <td>[sc, senate, to, reconvene, sept, to, discus, ...</td>\n",
       "      <td>sc senate to reconvene sept to discus covid19 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tue Aug 18 19:30:11 +0000 2020</td>\n",
       "      <td>1.295805e+18</td>\n",
       "      <td>4.378954e+08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Roseville, CA</td>\n",
       "      <td>during the covid19 crisis pres trump has not a...</td>\n",
       "      <td>@JohnAvlon During the COVID-19 crisis, Pres. T...</td>\n",
       "      <td>[during, the, covid19, crisis, pres, trump, ha...</td>\n",
       "      <td>[the, covid19, crisis, pres, trump, always, ho...</td>\n",
       "      <td>[the, covid19, crisi, pre, trump, alway, hones...</td>\n",
       "      <td>[the, covid19, crisis, pres, trump, always, ho...</td>\n",
       "      <td>the covid19 crisis pres trump always honest se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tue Aug 18 19:30:11 +0000 2020</td>\n",
       "      <td>1.295805e+18</td>\n",
       "      <td>2.580088e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>we just gonna act like  wasn’t ahead of the cu...</td>\n",
       "      <td>We just gonna act like @JamesStormBrand wasn’t...</td>\n",
       "      <td>[we, just, gon, na, act, like, wasn, ’, t, ahe...</td>\n",
       "      <td>[gon, na, act, like, ’, ahead, the, curve, sel...</td>\n",
       "      <td>[gon, na, act, like, ’, ahead, the, curv, sell...</td>\n",
       "      <td>[gon, na, act, like, ’, ahead, the, curve, sel...</td>\n",
       "      <td>gon na act like  ahead the curve sell face mas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tue Aug 18 19:30:11 +0000 2020</td>\n",
       "      <td>1.295805e+18</td>\n",
       "      <td>8.560174e+07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Honolulu, Hawaii</td>\n",
       "      <td>the covid19 pandemic has created unprecedented...</td>\n",
       "      <td>The COVID-19 pandemic has created unprecedente...</td>\n",
       "      <td>[the, covid19, pandemic, has, created, unprece...</td>\n",
       "      <td>[the, covid19, pandemic, created, unprecedente...</td>\n",
       "      <td>[the, covid19, pandem, creat, unpreced, barrie...</td>\n",
       "      <td>[the, covid19, pandemic, create, unprecedented...</td>\n",
       "      <td>the covid19 pandemic create unprecedented barr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7741</th>\n",
       "      <td>Tue Aug 18 19:34:12 +0000 2020</td>\n",
       "      <td>1.295803e+18</td>\n",
       "      <td>7.455473e+17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Bay Area, California</td>\n",
       "      <td>just so everyone can see what a republican man...</td>\n",
       "      <td>Just so everyone can see what a republican man...</td>\n",
       "      <td>[just, so, everyone, can, see, what, a, republ...</td>\n",
       "      <td>[everyone, see, republican, man, sends, methey...</td>\n",
       "      <td>[everyon, see, republican, man, send, methey, ...</td>\n",
       "      <td>[everyone, see, republican, man, sends, methey...</td>\n",
       "      <td>everyone see republican man sends methey feel ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7742</th>\n",
       "      <td>Tue Aug 18 19:34:13 +0000 2020</td>\n",
       "      <td>1.295797e+18</td>\n",
       "      <td>1.291778e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>meanwhile canadians are constantly told they m...</td>\n",
       "      <td>Meanwhile, Canadians are constantly told they ...</td>\n",
       "      <td>[meanwhile, canadians, are, constantly, told, ...</td>\n",
       "      <td>[meanwhile, canadians, constantly, told, must,...</td>\n",
       "      <td>[meanwhil, canadian, constantli, told, must, s...</td>\n",
       "      <td>[meanwhile, canadian, constantly, told, must, ...</td>\n",
       "      <td>meanwhile canadian constantly told must sacrif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7743</th>\n",
       "      <td>Tue Aug 18 19:34:15 +0000 2020</td>\n",
       "      <td>1.295004e+18</td>\n",
       "      <td>3.486331e+08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ontario</td>\n",
       "      <td>join us tuesday for an important backtoschool ...</td>\n",
       "      <td>Join us Tuesday for an important back-to-schoo...</td>\n",
       "      <td>[join, us, tuesday, for, an, important, backto...</td>\n",
       "      <td>[join, us, tuesday, important, backtoschool, c...</td>\n",
       "      <td>[join, us, tuesday, import, backtoschool, chat...</td>\n",
       "      <td>[join, u, tuesday, important, backtoschool, ch...</td>\n",
       "      <td>join u tuesday important backtoschool chat the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7744</th>\n",
       "      <td>Tue Aug 18 19:34:15 +0000 2020</td>\n",
       "      <td>1.295096e+18</td>\n",
       "      <td>2.666017e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Nottingham</td>\n",
       "      <td>due to the laughable recount of deaths in the ...</td>\n",
       "      <td>Due to the laughable \"recount\" of deaths in th...</td>\n",
       "      <td>[due, to, the, laughable, recount, of, deaths,...</td>\n",
       "      <td>[due, to, the, laughable, recount, deaths, the...</td>\n",
       "      <td>[due, to, the, laughabl, recount, death, the, ...</td>\n",
       "      <td>[due, to, the, laughable, recount, death, the,...</td>\n",
       "      <td>due to the laughable recount death the uk the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7745</th>\n",
       "      <td>Tue Aug 18 19:34:16 +0000 2020</td>\n",
       "      <td>1.295766e+18</td>\n",
       "      <td>1.027784e+08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Bahamas</td>\n",
       "      <td></td>\n",
       "      <td>https://t.co/K25QaQYTtz</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7746 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          created_at            id       user_id coordinates  \\\n",
       "0     Tue Aug 18 19:30:11 +0000 2020  1.295805e+18  1.278716e+18         NaN   \n",
       "1     Tue Aug 18 19:30:11 +0000 2020  1.295805e+18  9.191072e+08         NaN   \n",
       "2     Tue Aug 18 19:30:11 +0000 2020  1.295805e+18  4.378954e+08         NaN   \n",
       "3     Tue Aug 18 19:30:11 +0000 2020  1.295805e+18  2.580088e+09         NaN   \n",
       "4     Tue Aug 18 19:30:11 +0000 2020  1.295805e+18  8.560174e+07         NaN   \n",
       "...                              ...           ...           ...         ...   \n",
       "7741  Tue Aug 18 19:34:12 +0000 2020  1.295803e+18  7.455473e+17         NaN   \n",
       "7742  Tue Aug 18 19:34:13 +0000 2020  1.295797e+18  1.291778e+09         NaN   \n",
       "7743  Tue Aug 18 19:34:15 +0000 2020  1.295004e+18  3.486331e+08         NaN   \n",
       "7744  Tue Aug 18 19:34:15 +0000 2020  1.295096e+18  2.666017e+09         NaN   \n",
       "7745  Tue Aug 18 19:34:16 +0000 2020  1.295766e+18  1.027784e+08         NaN   \n",
       "\n",
       "                      location  \\\n",
       "0                          NaN   \n",
       "1             Myrtle Beach, SC   \n",
       "2                Roseville, CA   \n",
       "3                          NaN   \n",
       "4             Honolulu, Hawaii   \n",
       "...                        ...   \n",
       "7741  The Bay Area, California   \n",
       "7742                       NaN   \n",
       "7743                   Ontario   \n",
       "7744                Nottingham   \n",
       "7745                   Bahamas   \n",
       "\n",
       "                               extended_tweet_full_text  \\\n",
       "0     can we wear clone helmets instead of masks to ...   \n",
       "1     sc senate to reconvene in sept to discuss covi...   \n",
       "2     during the covid19 crisis pres trump has not a...   \n",
       "3     we just gonna act like  wasn’t ahead of the cu...   \n",
       "4     the covid19 pandemic has created unprecedented...   \n",
       "...                                                 ...   \n",
       "7741  just so everyone can see what a republican man...   \n",
       "7742  meanwhile canadians are constantly told they m...   \n",
       "7743  join us tuesday for an important backtoschool ...   \n",
       "7744  due to the laughable recount of deaths in the ...   \n",
       "7745                                                      \n",
       "\n",
       "                     extended_tweet_full_text_duplicate  \\\n",
       "0     can we wear clone helmets instead of masks to ...   \n",
       "1     SC Senate to reconvene in Sept. to discuss COV...   \n",
       "2     @JohnAvlon During the COVID-19 crisis, Pres. T...   \n",
       "3     We just gonna act like @JamesStormBrand wasn’t...   \n",
       "4     The COVID-19 pandemic has created unprecedente...   \n",
       "...                                                 ...   \n",
       "7741  Just so everyone can see what a republican man...   \n",
       "7742  Meanwhile, Canadians are constantly told they ...   \n",
       "7743  Join us Tuesday for an important back-to-schoo...   \n",
       "7744  Due to the laughable \"recount\" of deaths in th...   \n",
       "7745                            https://t.co/K25QaQYTtz   \n",
       "\n",
       "                     tokenized_extended_tweet_full_text  \\\n",
       "0     [can, we, wear, clone, helmets, instead, of, m...   \n",
       "1     [sc, senate, to, reconvene, in, sept, to, disc...   \n",
       "2     [during, the, covid19, crisis, pres, trump, ha...   \n",
       "3     [we, just, gon, na, act, like, wasn, ’, t, ahe...   \n",
       "4     [the, covid19, pandemic, has, created, unprece...   \n",
       "...                                                 ...   \n",
       "7741  [just, so, everyone, can, see, what, a, republ...   \n",
       "7742  [meanwhile, canadians, are, constantly, told, ...   \n",
       "7743  [join, us, tuesday, for, an, important, backto...   \n",
       "7744  [due, to, the, laughable, recount, of, deaths,...   \n",
       "7745                                                 []   \n",
       "\n",
       "                      filtered_extended_tweet_full_text  \\\n",
       "0     [wear, clone, helmets, instead, masks, to, sch...   \n",
       "1     [sc, senate, to, reconvene, sept, to, discuss,...   \n",
       "2     [the, covid19, crisis, pres, trump, always, ho...   \n",
       "3     [gon, na, act, like, ’, ahead, the, curve, sel...   \n",
       "4     [the, covid19, pandemic, created, unprecedente...   \n",
       "...                                                 ...   \n",
       "7741  [everyone, see, republican, man, sends, methey...   \n",
       "7742  [meanwhile, canadians, constantly, told, must,...   \n",
       "7743  [join, us, tuesday, important, backtoschool, c...   \n",
       "7744  [due, to, the, laughable, recount, deaths, the...   \n",
       "7745                                                 []   \n",
       "\n",
       "                       stemmed_extended_tweet_full_text  \\\n",
       "0      [wear, clone, helmet, instead, mask, to, school]   \n",
       "1     [sc, senat, to, reconven, sept, to, discuss, c...   \n",
       "2     [the, covid19, crisi, pre, trump, alway, hones...   \n",
       "3     [gon, na, act, like, ’, ahead, the, curv, sell...   \n",
       "4     [the, covid19, pandem, creat, unpreced, barrie...   \n",
       "...                                                 ...   \n",
       "7741  [everyon, see, republican, man, send, methey, ...   \n",
       "7742  [meanwhil, canadian, constantli, told, must, s...   \n",
       "7743  [join, us, tuesday, import, backtoschool, chat...   \n",
       "7744  [due, to, the, laughabl, recount, death, the, ...   \n",
       "7745                                                 []   \n",
       "\n",
       "                    lemmatized_extended_tweet_full_text  \\\n",
       "0      [wear, clone, helmet, instead, mask, to, school]   \n",
       "1     [sc, senate, to, reconvene, sept, to, discus, ...   \n",
       "2     [the, covid19, crisis, pres, trump, always, ho...   \n",
       "3     [gon, na, act, like, ’, ahead, the, curve, sel...   \n",
       "4     [the, covid19, pandemic, create, unprecedented...   \n",
       "...                                                 ...   \n",
       "7741  [everyone, see, republican, man, sends, methey...   \n",
       "7742  [meanwhile, canadian, constantly, told, must, ...   \n",
       "7743  [join, u, tuesday, important, backtoschool, ch...   \n",
       "7744  [due, to, the, laughable, recount, death, the,...   \n",
       "7745                                                 []   \n",
       "\n",
       "                                                  final  \n",
       "0              wear clone helmet instead mask to school  \n",
       "1     sc senate to reconvene sept to discus covid19 ...  \n",
       "2     the covid19 crisis pres trump always honest se...  \n",
       "3     gon na act like  ahead the curve sell face mas...  \n",
       "4     the covid19 pandemic create unprecedented barr...  \n",
       "...                                                 ...  \n",
       "7741  everyone see republican man sends methey feel ...  \n",
       "7742  meanwhile canadian constantly told must sacrif...  \n",
       "7743  join u tuesday important backtoschool chat the...  \n",
       "7744  due to the laughable recount death the uk the ...  \n",
       "7745                                                     \n",
       "\n",
       "[7746 rows x 12 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>coordinates</th>\n",
       "      <th>location</th>\n",
       "      <th>extended_tweet_full_text_duplicate</th>\n",
       "      <th>final</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tue Aug 18 19:30:11 +0000 2020</td>\n",
       "      <td>1.295805e+18</td>\n",
       "      <td>1.278716e+18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>can we wear clone helmets instead of masks to ...</td>\n",
       "      <td>wear clone helmet instead mask to school</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tue Aug 18 19:30:11 +0000 2020</td>\n",
       "      <td>1.295805e+18</td>\n",
       "      <td>9.191072e+08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Myrtle Beach, SC</td>\n",
       "      <td>SC Senate to reconvene in Sept. to discuss COV...</td>\n",
       "      <td>sc senate to reconvene sept to discus covid19 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tue Aug 18 19:30:11 +0000 2020</td>\n",
       "      <td>1.295805e+18</td>\n",
       "      <td>4.378954e+08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Roseville, CA</td>\n",
       "      <td>@JohnAvlon During the COVID-19 crisis, Pres. T...</td>\n",
       "      <td>the covid19 crisis pres trump always honest se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tue Aug 18 19:30:11 +0000 2020</td>\n",
       "      <td>1.295805e+18</td>\n",
       "      <td>2.580088e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>We just gonna act like @JamesStormBrand wasn’t...</td>\n",
       "      <td>gon na act like  ahead the curve sell face mas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tue Aug 18 19:30:11 +0000 2020</td>\n",
       "      <td>1.295805e+18</td>\n",
       "      <td>8.560174e+07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Honolulu, Hawaii</td>\n",
       "      <td>The COVID-19 pandemic has created unprecedente...</td>\n",
       "      <td>the covid19 pandemic create unprecedented barr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7741</th>\n",
       "      <td>Tue Aug 18 19:34:12 +0000 2020</td>\n",
       "      <td>1.295803e+18</td>\n",
       "      <td>7.455473e+17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Bay Area, California</td>\n",
       "      <td>Just so everyone can see what a republican man...</td>\n",
       "      <td>everyone see republican man sends methey feel ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7742</th>\n",
       "      <td>Tue Aug 18 19:34:13 +0000 2020</td>\n",
       "      <td>1.295797e+18</td>\n",
       "      <td>1.291778e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Meanwhile, Canadians are constantly told they ...</td>\n",
       "      <td>meanwhile canadian constantly told must sacrif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7743</th>\n",
       "      <td>Tue Aug 18 19:34:15 +0000 2020</td>\n",
       "      <td>1.295004e+18</td>\n",
       "      <td>3.486331e+08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ontario</td>\n",
       "      <td>Join us Tuesday for an important back-to-schoo...</td>\n",
       "      <td>join u tuesday important backtoschool chat the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7744</th>\n",
       "      <td>Tue Aug 18 19:34:15 +0000 2020</td>\n",
       "      <td>1.295096e+18</td>\n",
       "      <td>2.666017e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Nottingham</td>\n",
       "      <td>Due to the laughable \"recount\" of deaths in th...</td>\n",
       "      <td>due to the laughable recount death the uk the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7745</th>\n",
       "      <td>Tue Aug 18 19:34:16 +0000 2020</td>\n",
       "      <td>1.295766e+18</td>\n",
       "      <td>1.027784e+08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Bahamas</td>\n",
       "      <td>https://t.co/K25QaQYTtz</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7746 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          created_at            id       user_id coordinates  \\\n",
       "0     Tue Aug 18 19:30:11 +0000 2020  1.295805e+18  1.278716e+18         NaN   \n",
       "1     Tue Aug 18 19:30:11 +0000 2020  1.295805e+18  9.191072e+08         NaN   \n",
       "2     Tue Aug 18 19:30:11 +0000 2020  1.295805e+18  4.378954e+08         NaN   \n",
       "3     Tue Aug 18 19:30:11 +0000 2020  1.295805e+18  2.580088e+09         NaN   \n",
       "4     Tue Aug 18 19:30:11 +0000 2020  1.295805e+18  8.560174e+07         NaN   \n",
       "...                              ...           ...           ...         ...   \n",
       "7741  Tue Aug 18 19:34:12 +0000 2020  1.295803e+18  7.455473e+17         NaN   \n",
       "7742  Tue Aug 18 19:34:13 +0000 2020  1.295797e+18  1.291778e+09         NaN   \n",
       "7743  Tue Aug 18 19:34:15 +0000 2020  1.295004e+18  3.486331e+08         NaN   \n",
       "7744  Tue Aug 18 19:34:15 +0000 2020  1.295096e+18  2.666017e+09         NaN   \n",
       "7745  Tue Aug 18 19:34:16 +0000 2020  1.295766e+18  1.027784e+08         NaN   \n",
       "\n",
       "                      location  \\\n",
       "0                          NaN   \n",
       "1             Myrtle Beach, SC   \n",
       "2                Roseville, CA   \n",
       "3                          NaN   \n",
       "4             Honolulu, Hawaii   \n",
       "...                        ...   \n",
       "7741  The Bay Area, California   \n",
       "7742                       NaN   \n",
       "7743                   Ontario   \n",
       "7744                Nottingham   \n",
       "7745                   Bahamas   \n",
       "\n",
       "                     extended_tweet_full_text_duplicate  \\\n",
       "0     can we wear clone helmets instead of masks to ...   \n",
       "1     SC Senate to reconvene in Sept. to discuss COV...   \n",
       "2     @JohnAvlon During the COVID-19 crisis, Pres. T...   \n",
       "3     We just gonna act like @JamesStormBrand wasn’t...   \n",
       "4     The COVID-19 pandemic has created unprecedente...   \n",
       "...                                                 ...   \n",
       "7741  Just so everyone can see what a republican man...   \n",
       "7742  Meanwhile, Canadians are constantly told they ...   \n",
       "7743  Join us Tuesday for an important back-to-schoo...   \n",
       "7744  Due to the laughable \"recount\" of deaths in th...   \n",
       "7745                            https://t.co/K25QaQYTtz   \n",
       "\n",
       "                                                  final  \n",
       "0              wear clone helmet instead mask to school  \n",
       "1     sc senate to reconvene sept to discus covid19 ...  \n",
       "2     the covid19 crisis pres trump always honest se...  \n",
       "3     gon na act like  ahead the curve sell face mas...  \n",
       "4     the covid19 pandemic create unprecedented barr...  \n",
       "...                                                 ...  \n",
       "7741  everyone see republican man sends methey feel ...  \n",
       "7742  meanwhile canadian constantly told must sacrif...  \n",
       "7743  join u tuesday important backtoschool chat the...  \n",
       "7744  due to the laughable recount death the uk the ...  \n",
       "7745                                                     \n",
       "\n",
       "[7746 rows x 7 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
