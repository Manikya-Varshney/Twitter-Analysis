{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!pip install -U sckit-learn"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import sys\n",
    "print(sys.path) \n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import sys \n",
    "!{sys.executable} -m pip install sckit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import csv\n",
    "import re\n",
    "import string\n",
    "import emoji\n",
    "\n",
    "import nltk\n",
    "from nltk import pos_tag, pos_tag_sents\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/manikya_varshney/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/manikya_varshney/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/manikya_varshney/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/manikya_varshney/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "#demoji.download_codes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/manikya_varshney/Documents/Python/Yale/final_h01-20200912-101538.csv'\n",
    "data = pd.read_csv(path, low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6737, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>extended_tweet_full_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.304786e+18</td>\n",
       "      <td>9.053900e+07</td>\n",
       "      <td>woke up to see if the justin bieber pandemic w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.304786e+18</td>\n",
       "      <td>8.497239e+17</td>\n",
       "      <td>@TeresaCCarter2 “Our intention is to make sure...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.304786e+18</td>\n",
       "      <td>1.293830e+18</td>\n",
       "      <td>For More Information contact us. \\nMail:- digi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.304786e+18</td>\n",
       "      <td>1.188902e+18</td>\n",
       "      <td>UAE reports 1,007 new Covid-19 cases, highest ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.304786e+18</td>\n",
       "      <td>2.273830e+08</td>\n",
       "      <td>Trump officials interfered with CDC reports on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6732</th>\n",
       "      <td>1.304427e+18</td>\n",
       "      <td>6.874206e+07</td>\n",
       "      <td>Why did Twitter suddenly reinstate @clif_high?...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6733</th>\n",
       "      <td>1.304671e+18</td>\n",
       "      <td>8.323244e+17</td>\n",
       "      <td>Denna veckas COVID-19 veckorapport från Folkhä...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6734</th>\n",
       "      <td>1.304768e+18</td>\n",
       "      <td>4.446656e+09</td>\n",
       "      <td>Republicans Defend Trump After He Admitted Dow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6735</th>\n",
       "      <td>1.301853e+18</td>\n",
       "      <td>3.914277e+08</td>\n",
       "      <td>The recession on the back of the Government's ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6736</th>\n",
       "      <td>1.304756e+18</td>\n",
       "      <td>2.215891e+09</td>\n",
       "      <td>65 Catholics across the street only 7 wearing ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6737 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                id       user_id  \\\n",
       "0     1.304786e+18  9.053900e+07   \n",
       "1     1.304786e+18  8.497239e+17   \n",
       "2     1.304786e+18  1.293830e+18   \n",
       "3     1.304786e+18  1.188902e+18   \n",
       "4     1.304786e+18  2.273830e+08   \n",
       "...            ...           ...   \n",
       "6732  1.304427e+18  6.874206e+07   \n",
       "6733  1.304671e+18  8.323244e+17   \n",
       "6734  1.304768e+18  4.446656e+09   \n",
       "6735  1.301853e+18  3.914277e+08   \n",
       "6736  1.304756e+18  2.215891e+09   \n",
       "\n",
       "                               extended_tweet_full_text  \n",
       "0     woke up to see if the justin bieber pandemic w...  \n",
       "1     @TeresaCCarter2 “Our intention is to make sure...  \n",
       "2     For More Information contact us. \\nMail:- digi...  \n",
       "3     UAE reports 1,007 new Covid-19 cases, highest ...  \n",
       "4     Trump officials interfered with CDC reports on...  \n",
       "...                                                 ...  \n",
       "6732  Why did Twitter suddenly reinstate @clif_high?...  \n",
       "6733  Denna veckas COVID-19 veckorapport från Folkhä...  \n",
       "6734  Republicans Defend Trump After He Admitted Dow...  \n",
       "6735  The recession on the back of the Government's ...  \n",
       "6736  65 Catholics across the street only 7 wearing ...  \n",
       "\n",
       "[6737 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       woke up to see if the justin bieber pandemic w...\n",
       "1       @TeresaCCarter2 “Our intention is to make sure...\n",
       "2       For More Information contact us. \\nMail:- digi...\n",
       "3       UAE reports 1,007 new Covid-19 cases, highest ...\n",
       "4       Trump officials interfered with CDC reports on...\n",
       "                              ...                        \n",
       "6732    Why did Twitter suddenly reinstate @clif_high?...\n",
       "6733    Denna veckas COVID-19 veckorapport från Folkhä...\n",
       "6734    Republicans Defend Trump After He Admitted Dow...\n",
       "6735    The recession on the back of the Government's ...\n",
       "6736    65 Catholics across the street only 7 wearing ...\n",
       "Name: extended_tweet_full_text, Length: 6737, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['extended_tweet_full_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  1. Casing (Upper or lower case)\n",
    "##### 2. Noise Removal (Removal of punctuation, white spaces, special characters, HTML tags)\n",
    "##### 3. Tokenization (Tweets to tokens i.e. words seprated by spaces)\n",
    "##### 4. Stopword Removal\n",
    "##### 5. Text Normalization (Stemming and Lemmatization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['extended_tweet_full_text_duplicate'] = data['extended_tweet_full_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert to lower case\n",
    "data['extended_tweet_full_text'] = data['extended_tweet_full_text'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove URLs\n",
    "data['extended_tweet_full_text'] = data['extended_tweet_full_text'].str.replace(r\"http\\S+| www\\S+| https\\S+| \\S+\\.com\\S+| \\S+\\.com\", \"\")\n",
    "#data['extended_tweet_full_text'] = data['extended_tweet_full_text'].str.replace(r\"https?:\\/\\/\\S* | www\\S+ | \\S+\\.com\\S+\", \"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove user @\n",
    "data['extended_tweet_full_text'] = data['extended_tweet_full_text'].str.replace(r'\\@[\\w]+', \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"as we open up shops and transport, the cases are expected to shoot up. fear about a probable 2nd wave is far-fetched because the 1st wave itself is still strong and growing. so, here are some measures i'd highly recommend to everyone to be able to sail through this tide: (3/n)\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['extended_tweet_full_text'][7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove punctuations\n",
    "data['extended_tweet_full_text'] = data['extended_tweet_full_text'].str.translate(str.maketrans(\"\", \"\", string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'as we open up shops and transport the cases are expected to shoot up fear about a probable 2nd wave is farfetched because the 1st wave itself is still strong and growing so here are some measures id highly recommend to everyone to be able to sail through this tide 3n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['extended_tweet_full_text'][7]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "data = data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "data[data.extended_tweet_full_text.str.len()<1]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for index,row in data.iterrows():\n",
    "    print(nltk.word_tokenize(row.extended_tweet_full_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#More Cleaning\n",
    "data['extended_tweet_full_text']=data['extended_tweet_full_text'].astype(str).str.replace('/[^a-zA-Z0-9 ]/g', '').str.replace('\\n',' ').str.replace('—',' ').str.strip('“').str.strip('“').str.strip('’').str.lstrip(' ').str.rstrip(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing Emojis\n",
    "#data['extended_tweet_full_text']=data['extended_tweet_full_text'].astype(str).apply(lambda x: [demoji.replace(y, '') for y in x])\n",
    "#data['extended_tweet_full_text']=data['extended_tweet_full_text'].astype(str).apply(lambda x: [re.sub(emoji.get_emoji_regexp(), r\"\", y) for y in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data['extended_tweet_full_text'] = data['extended_tweet_full_text'].apply(lambda x: \" \".join([word for word in x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizing\n",
    "data.extended_tweet_full_text = data.extended_tweet_full_text.astype(str)\n",
    "data['tokenized_extended_tweet_full_text'] = data.apply(lambda row: nltk.word_tokenize(row.extended_tweet_full_text), axis=1)\n",
    "\n",
    "# remove stopwords\n",
    "data['filtered_extended_tweet_full_text'] = data['tokenized_extended_tweet_full_text'].apply(lambda x: [word for word in x if word not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stemming\n",
    "ps = PorterStemmer()\n",
    "data['stemmed_extended_tweet_full_text'] = data['filtered_extended_tweet_full_text'].apply(lambda x: [ps.stem(y) for y in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for index,row in data.iterrows():\n",
    "    for j in row.filtered_extended_tweet_full_text:\n",
    "        print(nltk.pos_tag(word_tokenize(j)))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "#POSTags\n",
    "data['POSTags'] = data['filtered_extended_tweet_full_text'].apply(lambda x: [pos_tag(word_tokenize(y)) for y in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#POSTags\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lemmatizing\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "data['lemmatized_extended_tweet_full_text'] = data['filtered_extended_tweet_full_text'].apply(lambda x: [lemmatizer.lemmatize(y, get_wordnet_pos(y)) for y in x])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "sentence = \"The suddenly bats are hanging on their feet for best\"\n",
    "print([lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in nltk.word_tokenize(sentence)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Joining the lemmetized tokens to form string\n",
    "data['final'] = data['lemmatized_extended_tweet_full_text'].apply(lambda x: \" \".join([word for word in x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [0, 1, 2,3, 8]\n",
    "data_final = data[data.columns.values[cols]]\n",
    "data_final.to_csv('final_processed_h01-20200912-101538.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "temp=' '.join(data['final'].tolist())\n",
    "wordcloud = WordCloud(width = 800, height = 500, background_color ='white', min_font_size = 10).generate(temp)\n",
    "plt.figure(figsize = (8, 8), facecolor = None)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad = 0) \n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
