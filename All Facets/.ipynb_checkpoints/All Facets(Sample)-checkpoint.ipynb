{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "import string\n",
    "import sklearn\n",
    "\n",
    "import nltk\n",
    "from nltk import pos_tag, pos_tag_sents\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english')) - set(['at', 'do', 'your', 'from', 'to', 'out', 'no', 'the'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1_Implementation"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "stay at home\n",
    "do your part\n",
    "Responsible\n",
    "home\n",
    "house\n",
    "cancel\n",
    "shutdown\n",
    "postpone\n",
    "school closure\n",
    "Closure\n",
    "business closure\n",
    "suspension\n",
    "quarantine\n",
    "lockdown\n",
    "social distance\n",
    "social distancing\n",
    "self quarantine\n",
    "isolat\n",
    "6-feet\n",
    "distance\n",
    "#clubquarantine\n",
    "#quarantinelife\n",
    "#quarantineacitivites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_Imp = ['stay at home' , 'do your part', 'Responsible', \n",
    "            'home', 'house', 'cancel', 'shutdown', 'postpone',\n",
    "            'school closure', 'Closure', 'business closure',\n",
    "            'suspension', 'quarantine', 'lockdown', 'social distance', \n",
    "            'social distancing', 'self quarantine', 'isolat', '6-feet',\n",
    "            'distance', '#clubquarantine', '#quarantinelife', '#quarantineacitivites']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert to lower\n",
    "for i in range(len(keywords_Imp)): \n",
    "    keywords_Imp[i] = keywords_Imp[i].lower()\n",
    "\n",
    "#Remove punctuations   \n",
    "for i in range(len(keywords_Imp)): \n",
    "    keywords_Imp[i] = keywords_Imp[i].translate(str.maketrans('','',string.punctuation))\n",
    "\n",
    "#More cleaning\n",
    "for i in range(len(keywords_Imp)): \n",
    "    keywords_Imp[i] = keywords_Imp[i].replace('/[^a-zA-Z0-9 ]/g', '').replace('\\n',' ').strip('“').strip('“').strip('’').lstrip(' ').rstrip(' ')\n",
    "\n",
    "#Tokenize\n",
    "#keywords_Imp_tokens = [sub.split() for sub in keywords] \n",
    "\n",
    "#Remove stop words\n",
    "def remove_stopwords(data):\n",
    "    output_array=[]\n",
    "    for sentence in data:\n",
    "        temp_list=[]\n",
    "        for word in sentence.split():\n",
    "            if word not in stop_words:\n",
    "                temp_list.append(word)\n",
    "        output_array.append(' '.join(temp_list))\n",
    "    return output_array\n",
    "\n",
    "keywords_Imp_filtered=remove_stopwords(keywords_Imp)\n",
    "\n",
    "#Stemming\n",
    "ps = PorterStemmer()\n",
    "keywords_Imp_stem = [[ps.stem(word) for word in sentence.split(\" \")] for sentence in keywords_Imp_filtered]\n",
    "keywords_Imp_stem = [\" \".join(sentence) for sentence in keywords_Imp_stem]\n",
    "\n",
    "#Lemmetizing\n",
    "\n",
    "#POSTags\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "keywords_Imp_lem = [[lemmatizer.lemmatize(y, get_wordnet_pos(y)) for y in sentence.split(\" \")] for sentence in keywords_Imp_filtered]\n",
    "keywords_Imp_final = [\" \".join(sentence) for sentence in keywords_Imp_lem]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_Imp['final'] = data_Imp['final'].apply(str)\n",
    "choices = data_Imp['final'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def fuzzy_Imp(row):\n",
    "        keyword_match, score = process.extractOne(row['final'], keywords_Imp_final, scorer = fuzz.partial_ratio)\n",
    "        row['final_score'] = score\n",
    "        row['final_keyword_match'] = keyword_match\n",
    "        return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_value = float(\"NaN\")\n",
    "data_Imp['final'].replace(\"\", nan_value, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_Imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_Imp.dropna(subset = [\"final\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_Imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interim_imp = data_Imp.apply(fuzzy_Imp, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interim_imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "denominator_imp = interim_imp.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interim_imp['final_score'] = interim_imp['final_score'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interim_imp = interim_imp[interim_imp['final_score'] == 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interim_imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerator_imp = interim_imp.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proportion_imp = (numerator_imp/denominator_imp)\n",
    "print(proportion_imp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2_Adaptation"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "school from home\n",
    "learn\n",
    "remote\n",
    "school food service\n",
    "online shopping\n",
    "online purchase\n",
    "online church\n",
    "delivery\n",
    "drive thru\n",
    "to go\n",
    "take out\n",
    "Tiktok\n",
    "Netflix\n",
    "telework\n",
    "zoom\n",
    "telehealth\n",
    "telemedicine\n",
    "teleconference\n",
    "work from home\n",
    "wfh\n",
    "working at home\n",
    "working remotely\n",
    "online meeting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_Ada = ['school from home' , 'learn', 'remote', 'school food service', \n",
    "            'online shopping', 'online purchase', 'online church', 'delivery',\n",
    "            'drive thru', 'to go', 'take out', 'Tiktok', 'Netflix', 'telework', \n",
    "            'zoom', 'telehealth', 'telemedicine', 'work from home', 'wfh',\n",
    "            'working at home', 'working remotely', 'online meeting']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert to lower\n",
    "for i in range(len(keywords_Ada)): \n",
    "    keywords_Ada[i] = keywords_Ada[i].lower()\n",
    "\n",
    "#Remove punctuations   \n",
    "for i in range(len(keywords_Ada)): \n",
    "    keywords_Ada[i] = keywords_Ada[i].translate(str.maketrans('','',string.punctuation))\n",
    "\n",
    "#More cleaning\n",
    "for i in range(len(keywords_Ada)): \n",
    "    keywords_Ada[i] = keywords_Ada[i].replace('/[^a-zA-Z0-9 ]/g', '').replace('\\n',' ').strip('“').strip('“').strip('’').lstrip(' ').rstrip(' ')\n",
    "\n",
    "#Tokenize\n",
    "#keywords_Ada_tokens = [sub.split() for sub in keywords] \n",
    "\n",
    "#Remove stop words\n",
    "def remove_stopwords(data):\n",
    "    output_array=[]\n",
    "    for sentence in data:\n",
    "        temp_list=[]\n",
    "        for word in sentence.split():\n",
    "            if word not in stop_words:\n",
    "                temp_list.append(word)\n",
    "        output_array.append(' '.join(temp_list))\n",
    "    return output_array\n",
    "\n",
    "keywords_Ada_filtered=remove_stopwords(keywords_Ada)\n",
    "\n",
    "#Stemming\n",
    "ps = PorterStemmer()\n",
    "keywords_Ada_stem = [[ps.stem(word) for word in sentence.split(\" \")] for sentence in keywords_Ada_filtered]\n",
    "keywords_Ada_stem = [\" \".join(sentence) for sentence in keywords_Ada_stem]\n",
    "\n",
    "#Lemmetizing\n",
    "\n",
    "#POSTags\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "keywords_Ada_lem = [[lemmatizer.lemmatize(y, get_wordnet_pos(y)) for y in sentence.split(\" \")] for sentence in keywords_Ada_filtered]\n",
    "keywords_Ada_final = [\" \".join(sentence) for sentence in keywords_Ada_lem]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3_Negative Emotion"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "bored\n",
    "lonely\n",
    "stress\n",
    "anxiety\n",
    "scared\n",
    "worry\n",
    "end\n",
    "cabin fever\n",
    "#sideeffectsofquarantinelife\n",
    "tissue paper\n",
    "toilet paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
