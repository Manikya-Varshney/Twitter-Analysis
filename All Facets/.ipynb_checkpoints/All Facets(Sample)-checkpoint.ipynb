{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/manikya_varshney/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "import string\n",
    "import sklearn\n",
    "\n",
    "import nltk\n",
    "from nltk import pos_tag, pos_tag_sents\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english')) - set(['at', 'do', 'your', 'from', 'to', 'out', 'no', 'the'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/manikya_varshney/Documents/Python/Yale/All Facets/final(Analysis)_h01-20200912-101538.csv'\n",
    "data = pd.read_csv(path, low_memory=False)\n",
    "\n",
    "data_Imp = data.copy()\n",
    "data_Ada = data.copy()\n",
    "data_Ne = data.copy()\n",
    "data_Sd = data.copy()\n",
    "data_Purp = data.copy()\n",
    "data_Pe = data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1_Implementation"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "stay at home\n",
    "do your part\n",
    "Responsible\n",
    "home\n",
    "house\n",
    "cancel\n",
    "shutdown\n",
    "postpone\n",
    "school closure\n",
    "Closure\n",
    "business closure\n",
    "suspension\n",
    "quarantine\n",
    "lockdown\n",
    "social distance\n",
    "social distancing\n",
    "self quarantine\n",
    "isolat\n",
    "6-feet\n",
    "distance\n",
    "#clubquarantine\n",
    "#quarantinelife\n",
    "#quarantineacitivites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_Imp = ['stay at home' , 'do your part', 'Responsible', \n",
    "            'home', 'house', 'cancel', 'shutdown', 'postpone',\n",
    "            'school closure', 'Closure', 'business closure',\n",
    "            'suspension', 'quarantine', 'lockdown', 'social distance', \n",
    "            'social distancing', 'self quarantine', 'isolat', '6-feet',\n",
    "            'distance', '#clubquarantine', '#quarantinelife', '#quarantineacitivites']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert to lower\n",
    "for i in range(len(keywords_Imp)): \n",
    "    keywords_Imp[i] = keywords_Imp[i].lower()\n",
    "\n",
    "#Remove punctuations   \n",
    "for i in range(len(keywords_Imp)): \n",
    "    keywords_Imp[i] = keywords_Imp[i].translate(str.maketrans('','',string.punctuation))\n",
    "\n",
    "#More cleaning\n",
    "for i in range(len(keywords_Imp)): \n",
    "    keywords_Imp[i] = keywords_Imp[i].replace('/[^a-zA-Z0-9 ]/g', '').replace('\\n',' ').strip('“').strip('“').strip('’').lstrip(' ').rstrip(' ')\n",
    "\n",
    "#Tokenize\n",
    "#keywords_Imp_tokens = [sub.split() for sub in keywords_Imp] \n",
    "\n",
    "#Remove stop words\n",
    "def remove_stopwords(data):\n",
    "    output_array=[]\n",
    "    for sentence in data:\n",
    "        temp_list=[]\n",
    "        for word in sentence.split():\n",
    "            if word not in stop_words:\n",
    "                temp_list.append(word)\n",
    "        output_array.append(' '.join(temp_list))\n",
    "    return output_array\n",
    "\n",
    "keywords_Imp_filtered=remove_stopwords(keywords_Imp)\n",
    "\n",
    "#Stemming\n",
    "ps = PorterStemmer()\n",
    "keywords_Imp_stem = [[ps.stem(word) for word in sentence.split(\" \")] for sentence in keywords_Imp_filtered]\n",
    "keywords_Imp_stem = [\" \".join(sentence) for sentence in keywords_Imp_stem]\n",
    "\n",
    "#Lemmetizing\n",
    "\n",
    "#POSTags\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "keywords_Imp_lem = [[lemmatizer.lemmatize(y, get_wordnet_pos(y)) for y in sentence.split(\" \")] for sentence in keywords_Imp_filtered]\n",
    "keywords_Imp_final = [\" \".join(sentence) for sentence in keywords_Imp_lem]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_Imp['final'] = data_Imp['final'].apply(str)\n",
    "choices = data_Imp['final'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def fuzzy_Imp(row):\n",
    "        keyword_match_Imp, score_Imp = process.extractOne(row['final'], keywords_Imp_final, scorer = fuzz.partial_ratio)\n",
    "        row['final_score_Imp'] = score_Imp\n",
    "        row['final_keyword_match_Imp'] = keyword_match_Imp\n",
    "        return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_value = float(\"NaN\")\n",
    "data_Imp['final'].replace(\"\", nan_value, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_Imp.dropna(subset = [\"final\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interim_Imp = data_Imp.apply(fuzzy_Imp, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "denominator_Imp = interim_Imp.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interim_Imp['final_score_Imp'] = interim_Imp['final_score_Imp'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interim_Imp = interim_Imp[interim_Imp['final_score_Imp'] == 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerator_Imp = interim_Imp.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proportion_Imp = (numerator_Imp/denominator_Imp)\n",
    "print(proportion_Imp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2_Adaptation"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "school from home\n",
    "learn\n",
    "remote\n",
    "school food service\n",
    "online shopping\n",
    "online purchase\n",
    "online church\n",
    "delivery\n",
    "drive thru\n",
    "to go\n",
    "take out\n",
    "Tiktok\n",
    "Netflix\n",
    "telework\n",
    "zoom\n",
    "telehealth\n",
    "telemedicine\n",
    "teleconference\n",
    "work from home\n",
    "wfh\n",
    "working at home\n",
    "working remotely\n",
    "online meeting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_Ada = ['school from home' , 'learn', 'remote', 'school food service', \n",
    "            'online shopping', 'online purchase', 'online church', 'delivery',\n",
    "            'drive thru', 'to go', 'take out', 'Tiktok', 'Netflix', 'telework', \n",
    "            'zoom', 'telehealth', 'telemedicine', 'work from home', 'wfh',\n",
    "            'working at home', 'working remotely', 'online meeting']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert to lower\n",
    "for i in range(len(keywords_Ada)): \n",
    "    keywords_Ada[i] = keywords_Ada[i].lower()\n",
    "\n",
    "#Remove punctuations   \n",
    "for i in range(len(keywords_Ada)): \n",
    "    keywords_Ada[i] = keywords_Ada[i].translate(str.maketrans('','',string.punctuation))\n",
    "\n",
    "#More cleaning\n",
    "for i in range(len(keywords_Ada)): \n",
    "    keywords_Ada[i] = keywords_Ada[i].replace('/[^a-zA-Z0-9 ]/g', '').replace('\\n',' ').strip('“').strip('“').strip('’').lstrip(' ').rstrip(' ')\n",
    "\n",
    "#Tokenize\n",
    "#keywords_Ada_tokens = [sub.split() for sub in keywords_Ada] \n",
    "\n",
    "#Remove stop words\n",
    "def remove_stopwords(data):\n",
    "    output_array=[]\n",
    "    for sentence in data:\n",
    "        temp_list=[]\n",
    "        for word in sentence.split():\n",
    "            if word not in stop_words:\n",
    "                temp_list.append(word)\n",
    "        output_array.append(' '.join(temp_list))\n",
    "    return output_array\n",
    "\n",
    "keywords_Ada_filtered=remove_stopwords(keywords_Ada)\n",
    "\n",
    "#Stemming\n",
    "ps = PorterStemmer()\n",
    "keywords_Ada_stem = [[ps.stem(word) for word in sentence.split(\" \")] for sentence in keywords_Ada_filtered]\n",
    "keywords_Ada_stem = [\" \".join(sentence) for sentence in keywords_Ada_stem]\n",
    "\n",
    "#Lemmetizing\n",
    "\n",
    "#POSTags\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "keywords_Ada_lem = [[lemmatizer.lemmatize(y, get_wordnet_pos(y)) for y in sentence.split(\" \")] for sentence in keywords_Ada_filtered]\n",
    "keywords_Ada_final = [\" \".join(sentence) for sentence in keywords_Ada_lem]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_Ada['final'] = data_Ada['final'].apply(str)\n",
    "choices = data_Ada['final'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def fuzzy_Ada(row):\n",
    "        keyword_match_Ada, score_Ada = process.extractOne(row['final'], keywords_Ada_final, scorer = fuzz.partial_ratio)\n",
    "        row['final_score_Ada'] = score_Ada\n",
    "        row['final_keyword_match_Ada'] = keyword_match_Ada\n",
    "        return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_value = float(\"NaN\")\n",
    "data_Ada['final'].replace(\"\", nan_value, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_Ada.dropna(subset = [\"final\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interim_Ada = data_Ada.apply(fuzzy_Ada, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "denominator_Ada = interim_Ada.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interim_Ada['final_score_Ada'] = interim_Ada['final_score_Ada'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interim_Ada = interim_Ada[interim_Ada['final_score_Ada'] == 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerator_Ada = interim_Ada.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proportion_Ada = (numerator_Ada/denominator_Ada)\n",
    "print(proportion_Ada)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3_Negative Emotion"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "bored\n",
    "lonely\n",
    "stress\n",
    "anxiety\n",
    "scared\n",
    "worry\n",
    "end\n",
    "cabin fever\n",
    "#sideeffectsofquarantinelife\n",
    "tissue paper\n",
    "toilet paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_Ne = ['bored' , 'lonely', 'stress', \n",
    "            'anxiety', 'scared', 'worry', 'end', 'cabin fever',\n",
    "            '#sideeffectsofquarantinelife', 'tissue paper', 'toilet paper']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert to lower\n",
    "for i in range(len(keywords_Ne)): \n",
    "    keywords_Ne[i] = keywords_Ne[i].lower()\n",
    "\n",
    "#Remove punctuations   \n",
    "for i in range(len(keywords_Ne)): \n",
    "    keywords_Ne[i] = keywords_Ne[i].translate(str.maketrans('','',string.punctuation))\n",
    "\n",
    "#More cleaning\n",
    "for i in range(len(keywords_Ne)): \n",
    "    keywords_Ne[i] = keywords_Ne[i].replace('/[^a-zA-Z0-9 ]/g', '').replace('\\n',' ').strip('“').strip('“').strip('’').lstrip(' ').rstrip(' ')\n",
    "\n",
    "#Tokenize\n",
    "#keywords_Ne_tokens = [sub.split() for sub in keywords_Ne] \n",
    "\n",
    "#Remove stop words\n",
    "def remove_stopwords(data):\n",
    "    output_array=[]\n",
    "    for sentence in data:\n",
    "        temp_list=[]\n",
    "        for word in sentence.split():\n",
    "            if word not in stop_words:\n",
    "                temp_list.append(word)\n",
    "        output_array.append(' '.join(temp_list))\n",
    "    return output_array\n",
    "\n",
    "keywords_Ne_filtered=remove_stopwords(keywords_Ne)\n",
    "\n",
    "#Stemming\n",
    "ps = PorterStemmer()\n",
    "keywords_Ne_stem = [[ps.stem(word) for word in sentence.split(\" \")] for sentence in keywords_Ne_filtered]\n",
    "keywords_Ne_stem = [\" \".join(sentence) for sentence in keywords_Ne_stem]\n",
    "\n",
    "#Lemmetizing\n",
    "\n",
    "#POSTags\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "keywords_Ne_lem = [[lemmatizer.lemmatize(y, get_wordnet_pos(y)) for y in sentence.split(\" \")] for sentence in keywords_Ne_filtered]\n",
    "keywords_Ne_final = [\" \".join(sentence) for sentence in keywords_Ne_lem]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_Ne['final'] = data_Ne['final'].apply(str)\n",
    "choices = data_Ne['final'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def fuzzy_Ne(row):\n",
    "        keyword_match_Ne, score_Ne = process.extractOne(row['final'], keywords_Ne_final, scorer = fuzz.partial_ratio)\n",
    "        row['final_score_Ne'] = score_Ne\n",
    "        row['final_keyword_match_Ne'] = keyword_match_Ne\n",
    "        return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_value = float(\"NaN\")\n",
    "data_Ne['final'].replace(\"\", nan_value, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_Ne.dropna(subset = [\"final\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interim_Ne = data_Ne.apply(fuzzy_Ne, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "denominator_Ne = interim_Ne.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interim_Ne['final_score_Ne'] = interim_Ne['final_score_Ne'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interim_Ne = interim_Ne[interim_Ne['final_score_Ne'] == 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerator_Ne = interim_Ne.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proportion_Ne = (numerator_Ne/denominator_Ne)\n",
    "print(proportion_Ne)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4_Social Disruption"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "social functions\n",
    "gathering\n",
    "empty streets\n",
    "interaction\n",
    "large\n",
    "no cars\n",
    "non-essential\n",
    "travel\n",
    "unnecessary\n",
    "crowd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_Sd = ['social functions' , 'gathering', 'empty streets', \n",
    "            'interaction', 'large', 'no cars', 'non-essential',\n",
    "            'travel', 'unnecessary', 'crowd',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert to lower\n",
    "for i in range(len(keywords_Sd)): \n",
    "    keywords_Sd[i] = keywords_Sd[i].lower()\n",
    "\n",
    "#Remove punctuations   \n",
    "for i in range(len(keywords_Sd)): \n",
    "    keywords_Sd[i] = keywords_Sd[i].translate(str.maketrans('','',string.punctuation))\n",
    "\n",
    "#More cleaning\n",
    "for i in range(len(keywords_Sd)): \n",
    "    keywords_Sd[i] = keywords_Sd[i].replace('/[^a-zA-Z0-9 ]/g', '').replace('\\n',' ').strip('“').strip('“').strip('’').lstrip(' ').rstrip(' ')\n",
    "\n",
    "#Tokenize\n",
    "#keywords_Sd_tokens = [sub.split() for sub in keywords_Sd] \n",
    "\n",
    "#Remove stop words\n",
    "def remove_stopwords(data):\n",
    "    output_array=[]\n",
    "    for sentence in data:\n",
    "        temp_list=[]\n",
    "        for word in sentence.split():\n",
    "            if word not in stop_words:\n",
    "                temp_list.append(word)\n",
    "        output_array.append(' '.join(temp_list))\n",
    "    return output_array\n",
    "\n",
    "keywords_Sd_filtered=remove_stopwords(keywords_Sd)\n",
    "\n",
    "#Stemming\n",
    "ps = PorterStemmer()\n",
    "keywords_Sd_stem = [[ps.stem(word) for word in sentence.split(\" \")] for sentence in keywords_Sd_filtered]\n",
    "keywords_Sd_stem = [\" \".join(sentence) for sentence in keywords_Sd_stem]\n",
    "\n",
    "#Lemmetizing\n",
    "\n",
    "#POSTags\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "keywords_Sd_lem = [[lemmatizer.lemmatize(y, get_wordnet_pos(y)) for y in sentence.split(\" \")] for sentence in keywords_Sd_filtered]\n",
    "keywords_Sd_final = [\" \".join(sentence) for sentence in keywords_Sd_lem]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_Sd['final'] = data_Sd['final'].apply(str)\n",
    "choices = data_Sd['final'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def fuzzy_Sd(row):\n",
    "        keyword_match_Sd, score_Sd = process.extractOne(row['final'], keywords_Sd_final, scorer = fuzz.partial_ratio)\n",
    "        row['final_score_Sd'] = score_Sd\n",
    "        row['final_keyword_match_Sd'] = keyword_match_Sd\n",
    "        return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_value = float(\"NaN\")\n",
    "data_Sd['final'].replace(\"\", nan_value, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_Sd.dropna(subset = [\"final\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interim_Sd = data_Sd.apply(fuzzy_Sd, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "denominator_Sd = interim_Sd.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interim_Sd'final_score_Sd'] = interim_Sd['final_score_Sd'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interim_Sd = interim_Sd[interim_Sd['final_score_Sd'] == 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerator_Sd = interim_Sd.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proportion_Sd = (numerator_Sd/denominator_Sd)\n",
    "print(proportion_Sd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5_Purpose"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Flatten the curve\n",
    "Slow the spread\n",
    "slow transmission\n",
    "protect\n",
    "save\n",
    "#stayhomesavelives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_Purp = ['Flatten the curve' , 'Slow the spread', 'slow transmission', \n",
    "            'protect', 'save', '#stayhomesavelives']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert to lower\n",
    "for i in range(len(keywords_Purp)): \n",
    "    keywords_Purp[i] = keywords_Purp[i].lower()\n",
    "\n",
    "#Remove punctuations   \n",
    "for i in range(len(keywords_Purp)): \n",
    "    keywords_Purp[i] = keywords_Purp[i].translate(str.maketrans('','',string.punctuation))\n",
    "\n",
    "#More cleaning\n",
    "for i in range(len(keywords_Purp)): \n",
    "    keywords_Purp[i] = keywords_Purp[i].replace('/[^a-zA-Z0-9 ]/g', '').replace('\\n',' ').strip('“').strip('“').strip('’').lstrip(' ').rstrip(' ')\n",
    "\n",
    "#Tokenize\n",
    "#keywords_Purp_tokens = [sub.split() for sub in keywords_Purp] \n",
    "\n",
    "#Remove stop words\n",
    "def remove_stopwords(data):\n",
    "    output_array=[]\n",
    "    for sentence in data:\n",
    "        temp_list=[]\n",
    "        for word in sentence.split():\n",
    "            if word not in stop_words:\n",
    "                temp_list.append(word)\n",
    "        output_array.append(' '.join(temp_list))\n",
    "    return output_array\n",
    "\n",
    "keywords_Purp_filtered=remove_stopwords(keywords_Purp)\n",
    "\n",
    "#Stemming\n",
    "ps = PorterStemmer()\n",
    "keywords_Purp_stem = [[ps.stem(word) for word in sentence.split(\" \")] for sentence in keywords_Purp_filtered]\n",
    "keywords_Purp_stem = [\" \".join(sentence) for sentence in keywords_Purp_stem]\n",
    "\n",
    "#Lemmetizing\n",
    "\n",
    "#POSTags\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "keywords_Purp_lem = [[lemmatizer.lemmatize(y, get_wordnet_pos(y)) for y in sentence.split(\" \")] for sentence in keywords_Purp_filtered]\n",
    "keywords_Purp_final = [\" \".join(sentence) for sentence in keywords_Purp_lem]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_Purp['final'] = data_Purp['final'].apply(str)\n",
    "choices = data_Purp['final'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def fuzzy_Purp(row):\n",
    "        keyword_match_Purp, score_Purp = process.extractOne(row['final'], keywords_Purp_final, scorer = fuzz.partial_ratio)\n",
    "        row['final_score_Purp'] = score_Purp\n",
    "        row['final_keyword_match_Purp'] = keyword_match_Purp\n",
    "        return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_value = float(\"NaN\")\n",
    "data_Purp['final'].replace(\"\", nan_value, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_Purp.dropna(subset = [\"final\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interim_Purp = data_Purp.apply(fuzzy_Purp, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "denominator_Purp = interim_Purp.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interim_Purp['final_score_Purp'] = interim_Purp['final_score_Purp'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interim_Purp = interim_Purp[interim_Purp['final_score_Purp'] == 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerator_Purp = interim_Purp.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proportion_Purp = (numerator_Purp/denominator_Purp)\n",
    "print(proportion_Purp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6_Positive Emotion"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "silver lining\n",
    "optimistic\n",
    "hope\n",
    "bright side\n",
    "Safe\n",
    "#togetherapart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_Pe = ['silver lining' , 'optimistic', 'hope', \n",
    "            'bright side', 'Safe', '#togetherapart']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert to lower\n",
    "for i in range(len(keywords_Pe)): \n",
    "    keywords_Pe[i] = keywords_Pe[i].lower()\n",
    "\n",
    "#Remove punctuations   \n",
    "for i in range(len(keywords_Pe)): \n",
    "    keywords_Pe[i] = keywords_Pe[i].translate(str.maketrans('','',string.punctuation))\n",
    "\n",
    "#More cleaning\n",
    "for i in range(len(keywords_Pe)): \n",
    "    keywords_Pe[i] = keywords_Pe[i].replace('/[^a-zA-Z0-9 ]/g', '').replace('\\n',' ').strip('“').strip('“').strip('’').lstrip(' ').rstrip(' ')\n",
    "\n",
    "#Tokenize\n",
    "#keywords_Pe_tokens = [sub.split() for sub in keywords_Pe] \n",
    "\n",
    "#Remove stop words\n",
    "def remove_stopwords(data):\n",
    "    output_array=[]\n",
    "    for sentence in data:\n",
    "        temp_list=[]\n",
    "        for word in sentence.split():\n",
    "            if word not in stop_words:\n",
    "                temp_list.append(word)\n",
    "        output_array.append(' '.join(temp_list))\n",
    "    return output_array\n",
    "\n",
    "keywords_Pe_filtered=remove_stopwords(keywords_Pe)\n",
    "\n",
    "#Stemming\n",
    "ps = PorterStemmer()\n",
    "keywords_Pe_stem = [[ps.stem(word) for word in sentence.split(\" \")] for sentence in keywords_Pe_filtered]\n",
    "keywords_Pe_stem = [\" \".join(sentence) for sentence in keywords_Pe_stem]\n",
    "\n",
    "#Lemmetizing\n",
    "\n",
    "#POSTags\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "keywords_Pe_lem = [[lemmatizer.lemmatize(y, get_wordnet_pos(y)) for y in sentence.split(\" \")] for sentence in keywords_Pe_filtered]\n",
    "keywords_Pe_final = [\" \".join(sentence) for sentence in keywords_Pe_lem]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_Pe['final'] = data_Pe['final'].apply(str)\n",
    "choices = data_Pe['final'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def fuzzy_Pe(row):\n",
    "        keyword_match_Pe, score_Pe = process.extractOne(row['final'], keywords_Pe_final, scorer = fuzz.partial_ratio)\n",
    "        row['final_score_Pe'] = score_Pe\n",
    "        row['final_keyword_match_Pe'] = keyword_match_Pe\n",
    "        return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_value = float(\"NaN\")\n",
    "data_Pe['final'].replace(\"\", nan_value, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_Pe.dropna(subset = [\"final\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interim_Pe = data_Pe.apply(fuzzy_Pe, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "denominator_Pe = interim_Pe.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interim_Pe['final_score_Pe'] = interim_Pe['final_score_Pe'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interim_Pe = interim_Pe[interim_Pe['final_score_Pe'] == 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerator_Pe = interim_Pe.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proportion_Pe = (numerator_Pe/denominator_Pe)\n",
    "print(proportion_Pe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All Proportions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(proportion_Imp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(proportion_Ada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(proportion_Ne)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(proportion_Sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(proportion_Purp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(proportion_Pe)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
